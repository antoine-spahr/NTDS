{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the similarity matrices and the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading keywords similaryty matrix of shape (4803, 4803)\n",
      ">>> Loading genre similaryty matrix of shape (4803, 4803)\n",
      ">>> Loading crew similaryty matrix of shape (4803, 4803)\n",
      ">>> Loading cast similaryty matrix of shape (4803, 4803)\n"
     ]
    }
   ],
   "source": [
    "Data_path = 'Data/'\n",
    "\n",
    "sim_mat = {}\n",
    "names = ['keywords', 'genre', 'crew', 'cast']\n",
    "for name in names:\n",
    "    with open(Data_path+'csim_'+name, 'rb') as src:\n",
    "        sim_mat[name] = pickle.load(src)\n",
    "        print(f'>>> Loading {name} similaryty matrix of shape {sim_mat[name].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the 4 similarity matrices equitably (**to be optimized**) and pruned those with a similarity below 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple way\n",
    "adj_mat = np.zeros(sim_mat['keywords'].shape)\n",
    "for wi, name in zip([0.25, 0.25, 0.25, 0.25], names):\n",
    "     adj_mat += wi*sim_mat[name].values\n",
    "final_adjacency_mat = np.where(adj_mat < 0.25, 0, adj_mat)\n",
    "weight_dict = {'names':names, 'weights':[0.25, 0.25, 0.25, 0.25]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridsearch way\n",
    "names = list(sim_mat.keys()) # name of similarity matrix\n",
    "mat_dim = sim_mat['keywords'].shape # shape of them\n",
    "prune_threshold = 0.25 # threhold to prune the summed adjacency matrix\n",
    "\n",
    "weight_range = [np.arange(0, 1.01, 0.25), \\\n",
    "                          np.arange(0, 1.01, 0.25), \\\n",
    "                          np.arange(0, 1.01, 0.25), \\\n",
    "                          np.arange(0, 1.01, 0.25)] # Range of weight to test\n",
    "weights_list = [] # to store the output\n",
    "clustering_coef = [] # to store output\n",
    "eigenvals = [] # to store the eigenvalues\n",
    "\n",
    "j=0\n",
    "for w in itertools.product(*weight_range): # iterate over all combination of weights\n",
    "    if sum(w) == 1: # if the weight sum up to 1 \n",
    "        j+=1\n",
    "        print(f'>>> Combination nÂ°{j} with weights {w}')\n",
    "        weights_list.append(w)\n",
    "        adj_mat = np.zeros(mat_dim)\n",
    "        for wi, name in zip(w, names): # sum the similarity matrices\n",
    "            adj_mat += wi*sim_mat[name].values\n",
    "\n",
    "        G=nx.from_numpy_matrix(np.where(adj_mat < prune_threshold, 0, adj_mat)) # prune adjacency matrix\n",
    "        C = nx.average_clustering(G) # compute the average clustering coefficient\n",
    "        eigenvals.append(nx.linalg.spectrum.normalized_laplacian_spectrum(G))\n",
    "        \n",
    "        print(f'\\t--> Clustering coefficient : {C}')\n",
    "        clustering_coef.append(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the best combination of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmin(np.array(eigenvals).sum(axis=1), axis=1) # where the sum of eigenvalues is minimal\n",
    "weights = weights_list[idx]\n",
    "\n",
    "final_adjacency_mat= np.zeros(mat_dim)\n",
    "for wi, name in zip(weights, names): # sum the similarity matrices\n",
    "    final_adjacency_mat+= wi*sim_mat[name].values\n",
    "    \n",
    "final_adjacency_mat = np.where(final_adjacency_mat < prune_threshold, 0, final_adjacency_mat)\n",
    "weight_dict = {'names':names, 'weights':weights}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the adjacency matrix and the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix saved at Data/Adjacency_matrix.pickle\n",
      "Adjacency weights saved at Data/Adjacency_weights.pickle\n"
     ]
    }
   ],
   "source": [
    "with open(Data_path+'Adjacency_matrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(final_adjacency_mat, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('Adjacency matrix saved at '+Data_path+'Adjacency_matrix.pickle')\n",
    "    \n",
    "with open(Data_path+'Adjacency_weights.pickle', 'wb') as handle:\n",
    "    pickle.dump(weight_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('Adjacency weights saved at '+Data_path+'Adjacency_weights.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
