{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "\n",
    "import random\n",
    "\n",
    "import dgl.function as fn\n",
    "from dgl import DGLGraph\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the similarity matrices and generate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading keywords similarity matrix with shape (4802, 4802)\n",
      ">>> Loading genre similarity matrix with shape (4802, 4802)\n",
      ">>> Loading crew similarity matrix with shape (4802, 4802)\n",
      ">>> Loading cast similarity matrix with shape (4802, 4802)\n"
     ]
    }
   ],
   "source": [
    "Data_path = 'Data/'\n",
    "\n",
    "sim_mat = {}\n",
    "names = ['keywords', 'genre', 'crew', 'cast']\n",
    "for name in names:\n",
    "    with open(Data_path+'csim_'+name, 'rb') as src:\n",
    "        sim_mat[name] = pickle.load(src)\n",
    "        print(f'>>> Loading {name} similarity matrix with shape {sim_mat[name].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative 1** Combine the 4 similarity matrices equitably and pruned those with a similarity below 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple way\n",
    "adj_mat = sim_mat['keywords'].copy()\n",
    "for name in names[1:]:\n",
    "    adj_mat = adj_mat.add(sim_mat[name], fill_value=0)\n",
    "adj_mat = adj_mat/4    \n",
    "\n",
    "threshold = 0.25\n",
    "adj_mat[adj_mat < threshold] = 0\n",
    "\n",
    "# Generate graph\n",
    "G = DGLGraph(graph_data=adj_mat.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['budget','popularity','revenue','runtime','vote_average','vote_count','Nominations_GoldenGlobes']\n",
    "\n",
    "IMDB_path = Data_path + 'nodes_attributes.csv'\n",
    "IMDB = pd.read_csv(IMDB_path)\n",
    "IMDB.drop(columns = ['Unnamed: 0','id'], inplace=True)\n",
    "IMDB.set_index('title',inplace=True)\n",
    "\n",
    "# features\n",
    "IMDB_feat = IMDB[features]\n",
    "tensor_feat = torch.FloatTensor(IMDB_feat.values)\n",
    "\n",
    "# labels\n",
    "IMDB_nom = IMDB['Nominations_Oscars'].copy()\n",
    "IMDB_nom.loc[IMDB_nom > 0] = 1\n",
    "tensor_labels = torch.LongTensor(IMDB_nom.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, train_size=0.8, random_state=0)\n",
    "\n",
    "for prov_index, test_index in sss.split(tensor_feat, tensor_nom):\n",
    "    prov_mask = prov_index\n",
    "    test_mask = test_index\n",
    "\n",
    "for train_index, val_index in sss.split(tensor_feat[prov_mask], tensor_nom[prov_mask]):\n",
    "    train_mask = train_index\n",
    "    val_mask = val_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplacianPolynomial(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats: int,\n",
    "                 out_feats: int,\n",
    "                 k: int,\n",
    "                 dropout_prob: float,\n",
    "                 norm=True):\n",
    "        super().__init__()\n",
    "        self._in_feats = in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self._k = k\n",
    "        self._norm = norm\n",
    "        # Contains the weights learned by the Laplacian polynomial\n",
    "        self.pol_weights = nn.Parameter(torch.Tensor(self._k + 1))\n",
    "        # Contains the weights learned by the logistic regression (without bias)\n",
    "        self.logr_weights = nn.Parameter(torch.Tensor(in_feats, out_feats))\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        torch.manual_seed(0)\n",
    "        torch.nn.init.xavier_uniform_(self.logr_weights, gain=0.01)\n",
    "        torch.nn.init.normal_(self.pol_weights, mean=0.0, std=1e-3)\n",
    "\n",
    "    def forward(self, graph, feat):\n",
    "        r\"\"\"Compute graph convolution.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        * Input shape: :math:`(N, *, \\text{in_feats})` where * means any number of additional\n",
    "          dimensions, :math:`N` is the number of nodes.\n",
    "        * Output shape: :math:`(N, *, \\text{out_feats})` where all but the last dimension are\n",
    "          the same shape as the input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph (DGLGraph) : The graph.\n",
    "        feat (torch.Tensor): The input feature\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (torch.Tensor) The output feature\n",
    "        \"\"\"\n",
    "        #feat = self.dropout(feat)\n",
    "        graph = graph.local_var()\n",
    "        \n",
    "        # D^(-1/2)\n",
    "        norm = torch.pow(graph.in_degrees().float().clamp(min=1), -0.5)\n",
    "        shp = norm.shape + (1,) * (feat.dim() - 1)\n",
    "        norm = torch.reshape(norm, shp)\n",
    "        \n",
    "        # mult W first to reduce the feature size for aggregation.\n",
    "        feat = torch.matmul(feat, self.logr_weights) # X*Teta\n",
    "\n",
    "        result = self.pol_weights[0] * feat.clone() # a0*L^0*X*Teta <-- fisrt polynomial weight a0 * L^0 * x\n",
    "\n",
    "        for i in range(1, self._k + 1): # get the next polynomial coefficient (a1*L^1, a2*L^2, ..... ak*L^k) \n",
    "            old_feat = feat.clone()\n",
    "            if self._norm:\n",
    "                feat = feat * norm\n",
    "            graph.ndata['h'] = feat\n",
    "            # Feat is not modified in place\n",
    "            graph.update_all(fn.copy_src(src='h', out='m'),\n",
    "                             fn.sum(msg='m', out='h')) # update all nodes with msg function copy_src (get data from source node) and reduce function sum\n",
    "            if self._norm:\n",
    "                graph.ndata['h'] = graph.ndata['h'] * norm\n",
    "\n",
    "            feat = old_feat - graph.ndata['h']\n",
    "            result += self.pol_weights[i] * feat\n",
    "\n",
    "        return result\n",
    "\n",
    "    def extra_repr(self):\n",
    "        \"\"\"Set the extra representation of the module,\n",
    "        which will come into effect when printing the model.\n",
    "        \"\"\"\n",
    "        summary = 'in={_in_feats}, out={_out_feats}'\n",
    "        summary += ', normalization={_norm}'\n",
    "        return summary.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.nn.pytorch as dgl_nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph_NN(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_feats: int,\n",
    "                 out_feats: int,\n",
    "                 k: int,\n",
    "                 dropout_prob: float,\n",
    "                 hidden_size: int):\n",
    "        super().__init__()\n",
    "        self._in_feats = in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self._k = k\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        self.appnpconv = dgl_nn.conv.APPNPConv(self._k,0.1,0) #alpha teleport proba, TA a dit 0.1\n",
    "        self.linear1 = nn.Linear(self._in_feats,self.hidden_size)\n",
    "        self.linear2 = nn.Linear(self.hidden_size,self._out_feats)\n",
    "        \n",
    "    def forward(self, graph, feat):\n",
    "        \n",
    "        h = self.linear1(feat)\n",
    "        h = self.appnpconv(graph,h)\n",
    "        h = F.relu(h)\n",
    "        h = self.linear2(h)\n",
    "        \n",
    "        return h \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have are model ready we just need to create a function that performs one step of our training loop, and another one that evaluates our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, g, features, labels, loss_fcn, train_mask, optimizer):\n",
    "    model.train()  # Activate dropout\n",
    "    \n",
    "    logits = model(g, features) # prediction\n",
    "    logp = F.softmax(logits[train_mask],dim=1)\n",
    "    loss = loss_fcn(logp, labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def evaluate(model, g, features, mask, labels):\n",
    "    model.eval()  # Deactivate dropout\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)[mask]  # only compute the evaluation set\n",
    "        labels = labels[mask]\n",
    "        logp = F.softmax(logits)\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        acc = correct.item() * 1.0 / len(labels)\n",
    "        f1 = f1_score(labels, indices)\n",
    "        return f1,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_feats = len(features)\n",
    "output = 2\n",
    "pol_order = 3\n",
    "lr = 0.005\n",
    "weight_decay = 5e-6\n",
    "n_epochs = 1000\n",
    "p_dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train the classifier end to end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alicebizeul/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00001 | Time(s) nan | Train Loss 0.6619 | Val f1 29.5688% | Val Accuracy 55.3966%\n",
      "Epoch 00002 | Time(s) nan | Train Loss 0.7321 | Val f1 6.1350% | Val Accuracy 80.1040%\n",
      "Epoch 00003 | Time(s) nan | Train Loss 0.9914 | Val f1 0.0000% | Val Accuracy 80.7542%\n",
      "Epoch 00004 | Time(s) 1.3632 | Train Loss 1.0066 | Val f1 0.0000% | Val Accuracy 80.7542%\n",
      "Epoch 00005 | Time(s) 1.3681 | Train Loss 1.0066 | Val f1 0.0000% | Val Accuracy 80.7542%\n",
      "Epoch 00006 | Time(s) 1.3462 | Train Loss 1.0066 | Val f1 0.0000% | Val Accuracy 80.7542%\n",
      "Epoch 00007 | Time(s) 1.3276 | Train Loss 1.0065 | Val f1 0.0000% | Val Accuracy 80.7542%\n",
      "Epoch 00008 | Time(s) 1.3176 | Train Loss 1.0065 | Val f1 0.0000% | Val Accuracy 80.7542%\n",
      "Epoch 00009 | Time(s) 1.3116 | Train Loss 1.0065 | Val f1 0.0000% | Val Accuracy 80.7542%\n",
      "Epoch 00010 | Time(s) 1.3072 | Train Loss 1.0064 | Val f1 0.0000% | Val Accuracy 80.6242%\n",
      "Epoch 00011 | Time(s) 1.3299 | Train Loss 1.0064 | Val f1 0.0000% | Val Accuracy 80.4941%\n",
      "Epoch 00012 | Time(s) 1.3397 | Train Loss 1.0063 | Val f1 0.0000% | Val Accuracy 80.4941%\n",
      "Epoch 00013 | Time(s) 1.3469 | Train Loss 1.0064 | Val f1 0.0000% | Val Accuracy 80.4941%\n",
      "Epoch 00014 | Time(s) 1.3431 | Train Loss 1.0064 | Val f1 0.0000% | Val Accuracy 80.4941%\n",
      "Epoch 00015 | Time(s) 1.3400 | Train Loss 1.0064 | Val f1 0.0000% | Val Accuracy 80.4941%\n",
      "Epoch 00016 | Time(s) 1.3374 | Train Loss 1.0064 | Val f1 0.0000% | Val Accuracy 80.4941%\n",
      "Epoch 00017 | Time(s) 1.3341 | Train Loss 1.0063 | Val f1 0.0000% | Val Accuracy 80.4941%\n",
      "Epoch 00018 | Time(s) 1.3374 | Train Loss 1.0063 | Val f1 0.0000% | Val Accuracy 80.4941%\n",
      "Epoch 00019 | Time(s) 1.3332 | Train Loss 1.0063 | Val f1 0.0000% | Val Accuracy 80.6242%\n",
      "Epoch 00020 | Time(s) 1.3430 | Train Loss 1.0063 | Val f1 0.0000% | Val Accuracy 80.6242%\n",
      "Epoch 00021 | Time(s) 1.3485 | Train Loss 1.0063 | Val f1 0.0000% | Val Accuracy 80.7542%\n",
      "Epoch 00022 | Time(s) 1.3472 | Train Loss 1.0063 | Val f1 0.0000% | Val Accuracy 80.7542%\n",
      "Epoch 00023 | Time(s) 1.3633 | Train Loss 1.0063 | Val f1 0.0000% | Val Accuracy 80.7542%\n",
      "Epoch 00024 | Time(s) 1.3606 | Train Loss 1.0063 | Val f1 0.0000% | Val Accuracy 80.7542%\n",
      "Epoch 00025 | Time(s) 1.3577 | Train Loss 1.0063 | Val f1 0.0000% | Val Accuracy 80.7542%\n",
      "Epoch 00026 | Time(s) 1.3544 | Train Loss 1.0063 | Val f1 0.0000% | Val Accuracy 80.6242%\n",
      "Epoch 00027 | Time(s) 1.3583 | Train Loss 1.0062 | Val f1 0.0000% | Val Accuracy 80.6242%\n",
      "Epoch 00028 | Time(s) 1.3560 | Train Loss 1.0062 | Val f1 0.0000% | Val Accuracy 80.6242%\n",
      "Epoch 00029 | Time(s) 1.3532 | Train Loss 1.0062 | Val f1 0.0000% | Val Accuracy 80.4941%\n",
      "Epoch 00030 | Time(s) 1.3505 | Train Loss 1.0062 | Val f1 0.0000% | Val Accuracy 80.4941%\n",
      "Epoch 00031 | Time(s) 1.3477 | Train Loss 1.0062 | Val f1 0.0000% | Val Accuracy 80.4941%\n",
      "Epoch 00032 | Time(s) 1.3468 | Train Loss 1.0062 | Val f1 0.0000% | Val Accuracy 80.4941%\n",
      "Epoch 00033 | Time(s) 1.3520 | Train Loss 1.0062 | Val f1 0.0000% | Val Accuracy 80.4941%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-ee4f73b6b514>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fcn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-cd8888afd723>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, g, features, labels, loss_fcn, train_mask, optimizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Activate dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlogp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-5a5705ef58a1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, graph, feat)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappnpconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/dgl/nn/pytorch/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, graph, feat)\u001b[0m\n\u001b[1;32m   1426\u001b[0m                 th.ones(graph.number_of_edges(), 1).to(feat.device))\n\u001b[1;32m   1427\u001b[0m             graph.update_all(fn.u_mul_e('h', 'w', 'm'),\n\u001b[0;32m-> 1428\u001b[0;31m                              fn.sum('m', 'h'))\n\u001b[0m\u001b[1;32m   1429\u001b[0m             \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m             \u001b[0;31m# normalization by dst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/dgl/graph.py\u001b[0m in \u001b[0;36mupdate_all\u001b[0;34m(self, message_func, reduce_func, apply_node_func)\u001b[0m\n\u001b[1;32m   2767\u001b[0m                                           \u001b[0mreduce_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2768\u001b[0m                                           apply_func=apply_node_func)\n\u001b[0;32m-> 2769\u001b[0;31m             \u001b[0mRuntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2771\u001b[0m     def prop_nodes(self,\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/dgl/runtime/runtime.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(prog)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mexe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m# prog.pprint_exe(exe)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mexe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/dgl/runtime/ir/executor.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1076\u001b[0m         self.ret.data = F.binary_reduce(\n\u001b[1;32m   1077\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreducer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m             lhs_data, rhs_data, self.out_size, lhs_map, rhs_map, out_map)\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/dgl/backend/pytorch/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, reducer, binary_op, graph, lhs, rhs, lhs_data, rhs_data, out_size, lhs_map, rhs_map, out_map)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mreducer\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreducer\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mbinary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlhs_data_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs_data_nd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             out_data_nd, lhs_map[0], rhs_map[0], out_map[0])\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0;31m# normalize if mean reducer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;31m# NOTE(zihao): this is a temporary hack and we should have better solution in the future.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ntds_2019/lib/python3.7/site-packages/dgl/kernel.py\u001b[0m in \u001b[0;36mbinary_op_reduce\u001b[0;34m(reducer, op, G, A_target, B_target, A, B, out, A_rows, B_rows, out_rows)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         A_rows, B_rows, out_rows)\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;31m# pylint: disable=invalid-name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Graph_NN(in_feats, output, pol_order, p_dropout, hidden_size = 100)\n",
    "\n",
    "loss_fcn = torch.nn.CrossEntropyLoss(weight = torch.FloatTensor([0.1,0.9]))\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=lr,\n",
    "                             weight_decay=weight_decay)\n",
    "\n",
    "dur = []\n",
    "for epoch in range(n_epochs):\n",
    "    if epoch >= 3:\n",
    "        t0 = time.time()\n",
    "    loss = train(model, G, tensor_feat, tensor_labels, loss_fcn, train_mask, optimizer)\n",
    "    if epoch >= 3:\n",
    "        dur.append(time.time() - t0)\n",
    "        \n",
    "    f1, acc = evaluate(model, G, tensor_feat, val_mask, tensor_labels)\n",
    "    \n",
    "    if epoch%1 == 0:\n",
    "        print(\"Epoch {:05d} | Time(s) {:.4f} | Train Loss {:.4f} | Val f1 {:.4%} | Val Accuracy {:.4%}\". format(\n",
    "                epoch+1, np.mean(dur), loss.item(), f1, acc))\n",
    "\n",
    "print()\n",
    "f1, acc = evaluate(model, G, tensor_feat, test_mask, tensor_labels)\n",
    "print(\"Test Accuracy {:.4%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(G, tensor_feat)[val_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rajouter des features --> e.g. summary (bag of words), production company\n",
    "* Batch? \n",
    "* Comprendre et améilorer l'architecture\n",
    "* fine tuning des params --> pas optimisé pour le moment\n",
    "* comprendre si les labels imbalance est problematique\n",
    "* use only cast crew for graph construction and use keywods(2000 best)/genre/production companies as features \n",
    "* \n",
    "\n",
    "\n",
    "Question : \n",
    "* loss jump ?!\n",
    "* stability of scores ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
