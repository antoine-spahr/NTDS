{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HAS BEEN DONE**\n",
    "* Jakart vs Cosine vs Euclidian for features\n",
    "* Logistic regression simple et tuné + filtres --> ça marche assez bien!!\n",
    "* try different architecture -> testé combinaison de linear et GCNN / APPNP => GCNN avec une linear avant et deux linear apres = best\n",
    "\n",
    "\n",
    "**DONE 28.12.2019**\n",
    "\n",
    "* Trasformer some features because skewed --> OK, seems better\n",
    "* Use genre as feature as well --> OK are we sure we can do it? \n",
    "* Grid search for the Laplacian Polynomial: learning rate, p_order, dropout, weight_decay --> maximiser en fonction du F1 sur le val OK \n",
    "* Grid search for the Linear_GNN: learning rate, first_layer_size, hidden_size, weight_decay --> maximiser en fonction du F1 sur le val set  OK\n",
    "* See/tune the impact of weight decay --> OK (added in the Grid search)\n",
    "\n",
    "\n",
    "\n",
    "**TO DO**\n",
    "* Finer Grid Search?\n",
    "* Does it make sense to tune the first_layer_size? And the middle one?\n",
    "* Skip Connections ? \n",
    "* 2 Grid Search again to recheck for standardization effect\n",
    "* Try to compute a Random classifier, do it 20 times and see what is the F1 score in this case, or is the baseline we have enough\n",
    "* Explore/tune different NN architectures: e.g. Pure_GNN and Simple_APPNP --> to compare with the tuned Linear_GNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO** --> Ant\n",
    "* add building of model with best parameters after grid search\n",
    "* Then add testing\n",
    "* do LP and GNN in grid search for both Standardize and Non-standardize data\n",
    "* 20 prediction of a random classifier to check F1 score at random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, precision_score, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import random\n",
    "\n",
    "import dgl.function as fn\n",
    "from dgl import DGLGraph\n",
    "import dgl.nn.pytorch as dgl_nn\n",
    "import dgl.transform as dgl_transform\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Data_path = 'Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the similarity matrix and generate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load similarity matrix\n",
    "file = open(Data_path + 'Adjacency_matrix_all025.pickle', 'rb')\n",
    "adj_mat =  pickle.load(file)\n",
    "\n",
    "# Generate graph\n",
    "G = DGLGraph(graph_data=adj_mat)\n",
    "G = dgl_transform.add_self_loop(G) # we are sure of doing this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    3728\n",
       "1.0    1074\n",
       "Name: Nominations, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Features\n",
    "features_df = pd.read_csv(Data_path + 'features.csv',index_col=0).drop(columns = ['title'])\n",
    "\n",
    "# To consider also genres in the features comment this:\n",
    "#features_df = features_df[['budget', 'popularity', 'revenue', 'runtime', 'vote_average', 'vote_count']]\n",
    "\n",
    "\n",
    "# Labels\n",
    "labels_df = pd.read_csv(Data_path + 'labels.csv',index_col=0).drop(columns = ['title', ])\n",
    "IMDB_nom = labels_df['Nominations'].copy()\n",
    "IMDB_nom.loc[IMDB_nom > 0] = 1\n",
    "# Checking class imbalance\n",
    "IMDB_nom.value_counts() # 18.263 % of CLASS 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't want to transform the features don't run the following section and uncomment this:\n",
    "\n",
    "#transformed_features = features_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features engineering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# To visualize features before the transformation\n",
    "features_to_plot = features_df[['budget', 'popularity', 'revenue', 'runtime', 'vote_average', 'vote_count']]\n",
    "feat_names = ['budget', 'popularity', 'revenue', 'runtime', 'vote_average', 'vote_count']\n",
    "fig, axs = plt.subplots(3, 2,figsize=(15, 15))\n",
    "feat_mat = features_to_plot\n",
    "\n",
    "for name, ax in zip(feat_names, axs.reshape(-1)):\n",
    "    ax.hist(features_to_plot[name].values, bins=70,color='royalblue')\n",
    "    ax.set_title(name,size=15)\n",
    "    ax.set_ylim([0, 4810])\n",
    "    ax.set_xlabel('specific feat value',size=12)\n",
    "    ax.set_ylabel('number of samples',size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_transform(x):\n",
    "    trasformed_x = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        if x[i]>0:\n",
    "            trasformed_x[i] = np.log(x[i]) # could try also sqrt\n",
    "        else:\n",
    "            trasformed_x[i] = x[i]\n",
    "    return trasformed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only budget, popularity, revenue and vote_count are very skewed\n",
    "features_to_transform = features_df[['budget', 'popularity', 'revenue', 'vote_count']]\n",
    "feat_names = ['budget', 'popularity', 'revenue', 'vote_count']\n",
    "transformed_feat = features_df.copy()\n",
    "for name in feat_names:\n",
    "    transformed_feat[name] = feat_transform(features_to_transform[name])\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# To visualize features after the transformation\n",
    "trasformed_features_to_plot = transformed_feat[['budget', 'popularity', 'revenue', 'runtime', 'vote_average', 'vote_count']]\n",
    "feat_names = ['budget', 'popularity', 'revenue', 'runtime', 'vote_average', 'vote_count']\n",
    "fig, axs = plt.subplots(3, 2,figsize=(15, 15))\n",
    "feat_mat = features_to_plot\n",
    "\n",
    "for name, ax in zip(feat_names, axs.reshape(-1)):\n",
    "    ax.hist(trasformed_features_to_plot[name].values, bins=70,color='royalblue')\n",
    "    ax.set_title(name,size=15)\n",
    "    ax.set_ylim([0, 4810])\n",
    "    ax.set_xlabel('specific feat value',size=12)\n",
    "    ax.set_ylabel('number of samples',size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate masks and split train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss1 = StratifiedShuffleSplit(n_splits=1, train_size=0.8) #random_state=0\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, train_size=0.8) #random_state=0\n",
    "\n",
    "for prov_index, test_index in sss1.split(transformed_feat, IMDB_nom.values):\n",
    "    prov_mask = prov_index\n",
    "    test_mask = test_index\n",
    "\n",
    "for train_index, val_index in sss2.split(transformed_feat.iloc[prov_mask], IMDB_nom.values[prov_mask]):\n",
    "    train_mask = train_index\n",
    "    val_mask = val_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "tensor_data = torch.FloatTensor(transformed_feat.values)\n",
    "\n",
    "# labels\n",
    "tensor_labels = torch.LongTensor(IMDB_nom.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the data increases performances (73 -> 82), in terms of repartition bit more imbalanced\n",
    "# but still increase of performance for class 1 samples\n",
    "\n",
    "# To recheck this in terms of F1 score!!!!\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#transformed_feat.iloc[train_mask] = scaler.fit_transform(transformed_feat.iloc[train_mask].to_numpy())\n",
    "#transformed_feat.iloc[val_mask] = scaler.transform(transformed_feat.iloc[val_mask].to_numpy())\n",
    "#transformed_feat.iloc[test_mask] = scaler.transform(transformed_feat.iloc[test_mask].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Classifier \n",
    "as baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_classifier(X):\n",
    "    \"\"\"\n",
    "    Binary classification at random\n",
    "    INPUT\n",
    "        |---- X (2D numpy array) the input data (the sample dimension is the first one)\n",
    "    OUTPUT \n",
    "        |---- y (1D numpy array) the random prediction for each sample\n",
    "    \"\"\"\n",
    "    return np.random.randint(0, 2, X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_classification(data, labels, mask, name, N):\n",
    "    precision, recall, f1score, accuracy, CM = [], [], [], [], []\n",
    "    for _ in range(N):\n",
    "        rand_pred = random_classifier(data[mask,:])\n",
    "        pre,rec,f1,sup = precision_recall_fscore_support(labels[mask], rand_pred)\n",
    "        acc = accuracy_score(labels[mask], rand_pred)\n",
    "        precision.append(pre[1])\n",
    "        recall.append(rec[1])\n",
    "        f1score.append(f1[1])\n",
    "        accuracy.append(acc)\n",
    "        CM.append(confusion_matrix(labels[mask], rand_pred))\n",
    "\n",
    "    print(f'Random Classifier on the {name} set \\n'+'-'*80)\n",
    "    print(f'\\t |---- Precision {np.mean(precision):.2%} +/- {np.std(precision):.2%} ')\n",
    "    print(f'\\t |---- Recall {np.mean(recall):.2%} +/- {np.std(recall):.2%}')\n",
    "    print(f'\\t |---- F1-score {np.mean(f1score):.2%} +/- {np.std(f1score):.2%}')\n",
    "    print(f'\\t |---- Accuracy {np.mean(accuracy):.2%} +/- {np.std(accuracy):.2%}')\n",
    "    print(f'\\t \\n{np.mean(CM, axis=0)} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Classifier on the Train set \n",
      "--------------------------------------------------------------------------------\n",
      "\t |---- Precision 24.23% +/- 0.82% \n",
      "\t |---- Recall 49.86% +/- 2.10%\n",
      "\t |---- F1-score 32.61% +/- 1.17%\n",
      "\t |---- Accuracy 49.98% +/- 0.79%\n",
      "\t \n",
      "[[1163.4  1162.6 ]\n",
      " [ 374.04  371.96]] \n",
      "\n",
      "Random Classifier on the Validation set \n",
      "--------------------------------------------------------------------------------\n",
      "\t |---- Precision 25.87% +/- 1.19% \n",
      "\t |---- Recall 50.09% +/- 2.83%\n",
      "\t |---- F1-score 34.11% +/- 1.61%\n",
      "\t |---- Accuracy 49.95% +/- 1.49%\n",
      "\t \n",
      "[[284.42 285.58]\n",
      " [ 99.32  99.68]] \n",
      "\n",
      "Random Classifier on the Test set \n",
      "--------------------------------------------------------------------------------\n",
      "\t |---- Precision 22.47% +/- 1.36% \n",
      "\t |---- Recall 50.11% +/- 3.46%\n",
      "\t |---- F1-score 31.02% +/- 1.91%\n",
      "\t |---- Accuracy 50.15% +/- 1.58%\n",
      "\t \n",
      "[[374.2  371.8 ]\n",
      " [107.26 107.74]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 50\n",
    "random_classification(tensor_data, tensor_labels, train_mask, 'Train', N)\n",
    "random_classification(tensor_data, tensor_labels, val_mask, 'Validation', N)\n",
    "random_classification(tensor_data, tensor_labels, test_mask, 'Test', N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      ">>> Precision: 0.3995\n",
      ">>> Recall: 0.6932\n",
      ">>> F1: 0.5068\n",
      ">>> Support: 427\n",
      "\n",
      "Test set:\n",
      ">>> Precision: 0.3907\n",
      ">>> Recall: 0.6829\n",
      ">>> F1: 0.497\n",
      ">>> Support: 123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C= 1,random_state = 0,solver = 'lbfgs').fit(transformed_feat.iloc[train_mask].to_numpy(),IMDB_nom.values[train_mask])\n",
    "train_pred = clf.predict(transformed_feat.iloc[train_mask].to_numpy())\n",
    "test_pred = clf.predict(transformed_feat.iloc[test_mask].to_numpy())\n",
    "\n",
    "tr_pre,tr_rec,tr_f1,tr_sup = precision_recall_fscore_support(train_pred,IMDB_nom.values[train_mask])\n",
    "print('Training set:')\n",
    "print('>>> Precision: {:0.4}'.format(tr_pre[1]))\n",
    "print('>>> Recall: {:0.4}'.format(tr_rec[1]))\n",
    "print('>>> F1: {:0.4}'.format(tr_f1[1]))\n",
    "print('>>> Support: {:}'.format(tr_sup[1]))\n",
    "print('')\n",
    "\n",
    "test_pre,test_rec,test_f1,test_sup = precision_recall_fscore_support(test_pred,IMDB_nom.values[test_mask])\n",
    "print('Test set:')\n",
    "print('>>> Precision: {:0.4}'.format(test_pre[1]))\n",
    "print('>>> Recall: {:0.4}'.format(test_rec[1]))\n",
    "print('>>> F1: {:0.4}'.format(test_f1[1]))\n",
    "print('>>> Support: {:}'.format(test_sup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix on validation classification\n",
    "#disp = sklearn.metrics.plot_confusion_matrix(clf, transformed_feat.iloc[val_mask],IMDB_nom.values[val_mask],cmap=plt.cm.Blues,display_labels = ['Not Nominated','Nominated'],normalize='true')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Logistic Regression & Graph Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix when classifier not available only confusion matrix\n",
    "def confusion_matrix(matrix):\n",
    "    figure = plt.figure()\n",
    "    axes = figure.add_subplot(111)\n",
    "    test = axes.matshow(matrix, cmap = plt.cm.get_cmap('Blues'));\n",
    "    axes.set_yticklabels(['','Not Nominated','Nominated'],style='italic')\n",
    "    axes.set_xticklabels(['','Not Nominated','Nominated'],style='italic')\n",
    "    axes.set_ylabel('True Label')\n",
    "    axes.set_xlabel('Predicted Label')\n",
    "    figure.colorbar(test)\n",
    "    for (j,i),label in np.ndenumerate(matrix):\n",
    "        axes.text(i,j,np.round(label,3),ha='center',va='center',color = 'grey')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplacianPolynomial(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats: int,\n",
    "                 out_feats: int,\n",
    "                 k: int,\n",
    "                 dropout_prob: float,\n",
    "                 norm=True):\n",
    "        super().__init__()\n",
    "        self._in_feats = in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self._k = k\n",
    "        self._norm = norm\n",
    "        # Contains the weights learned by the Laplacian polynomial\n",
    "        self.pol_weights = nn.Parameter(torch.Tensor(self._k + 1))\n",
    "        # Contains the weights learned by the logistic regression (without bias)\n",
    "        self.logr_weights = nn.Parameter(torch.Tensor(in_feats, out_feats))\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        torch.manual_seed(0)\n",
    "        torch.nn.init.xavier_uniform_(self.logr_weights, gain=0.01)\n",
    "        torch.nn.init.normal_(self.pol_weights, mean=0.0, std=1e-3)\n",
    "\n",
    "    def forward(self, graph, feat):\n",
    "        r\"\"\"Compute graph convolution.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        * Input shape: :math:`(N, *, \\text{in_feats})` where * means any number of additional\n",
    "          dimensions, :math:`N` is the number of nodes.\n",
    "        * Output shape: :math:`(N, *, \\text{out_feats})` where all but the last dimension are\n",
    "          the same shape as the input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph (DGLGraph) : The graph.\n",
    "        feat (torch.Tensor): The input feature\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (torch.Tensor) The output feature\n",
    "        \"\"\"\n",
    "        feat = self.dropout(feat)\n",
    "        graph = graph.local_var()\n",
    "        \n",
    "        # D^(-1/2)\n",
    "        norm = torch.pow(graph.in_degrees().float().clamp(min=1), -0.5)\n",
    "        shp = norm.shape + (1,) * (feat.dim() - 1)\n",
    "        norm = torch.reshape(norm, shp)\n",
    "\n",
    "        # mult W first to reduce the feature size for aggregation.\n",
    "        feat = torch.matmul(feat, self.logr_weights) # X*Teta\n",
    "\n",
    "        result = self.pol_weights[0] * feat.clone() # a0*L^0*X*Teta <-- fisrt polynomial weight a0 * L^0 * x\n",
    "\n",
    "        for i in range(1, self._k + 1): # get the next polynomial coefficient (a1*L^1, a2*L^2, ..... ak*L^k) \n",
    "            old_feat = feat.clone()\n",
    "            if self._norm:\n",
    "                feat = feat * norm\n",
    "            graph.ndata['h'] = feat\n",
    "            # Feat is not modified in place\n",
    "            graph.update_all(fn.copy_src(src='h', out='m'),\n",
    "                             fn.sum(msg='m', out='h')) # update all nodes with msg function copy_src (get data from source node) and reduce function sum\n",
    "            if self._norm:\n",
    "                graph.ndata['h'] = graph.ndata['h'] * norm\n",
    "\n",
    "            feat = old_feat - graph.ndata['h']\n",
    "            result += self.pol_weights[i] * feat\n",
    "\n",
    "        return result\n",
    "\n",
    "    def extra_repr(self):\n",
    "        \"\"\"Set the extra representation of the module,\n",
    "        which will come into effect when printing the model.\n",
    "        \"\"\"\n",
    "        summary = 'in={_in_feats}, out={_out_feats}'\n",
    "        summary += ', normalization={_norm}'\n",
    "        return summary.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, g, features, labels, train_mask, loss_fcn, optimizer):\n",
    "    \"\"\" \n",
    "    DESCRIPTION : Train and update model classification performances with training set\n",
    "    INPUT:\n",
    "        |--- model: [] classification model to train\n",
    "        |--- g: [DGLgraph] DeepGraphLearning graph object\n",
    "        |--- features: [FloatTensor] 2D tensor containing samples' features\n",
    "        |--- labels: [LongTensor] 1D tensor containing samples' labels (0-1)\n",
    "        |--- train_mask: [np.array] indices of training set\n",
    "        |--- loss_fcn: pytorch loss function chosen for model training\n",
    "        |--- optimizer: pytorch model optimizer \n",
    "    OUTPUT:\n",
    "        |--- loss: [float] value of loss function for the model at current state\n",
    "    \"\"\"\n",
    "    model.train()  \n",
    "    \n",
    "    pred = model(g, features)[train_mask] # prediction\n",
    "    loss = loss_fcn(pred, labels[train_mask])\n",
    "    optimizer.zero_grad()    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    #_, indices = torch.max(pred, dim=1)\n",
    "    #correct = torch.sum(indices == labels[train_mask])\n",
    "    #acc = correct.item() * 1.0 / len(labels[train_mask]) #not the best metric\n",
    "    \n",
    "    #C = sklearn.metrics.confusion_matrix(tensor_labels[train_mask], indices.numpy(), labels=[0,1], sample_weight=None, normalize='true')\n",
    "\n",
    "    #return loss, acc, C\n",
    "    return loss\n",
    "    \n",
    "def evaluate(model, g, features, mask, labels):\n",
    "    \"\"\" \n",
    "    DESCRIPTION : Evaluate model classification performance on validation set \n",
    "    INPUT:\n",
    "        |--- model: [] classification model to evaluate\n",
    "        |--- g: [DGLgraph] DeepGraphLearning graph object\n",
    "        |--- features: [FloatTensor] 2D tensor containing samples' features\n",
    "        |--- labels: [LongTensor] 1D tensor containing samples' labels (0-1)\n",
    "        |--- mask: [np.array] indices of validation set\n",
    "    OUTPUT:\n",
    "        |--- acc: [float] classification accuracy\n",
    "        |--- recall: [float] classification recall\n",
    "        |--- precision: [float] classification precision\n",
    "        |--- f1: [float] classification f1 score\n",
    "    \"\"\"\n",
    "    model.eval() \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(g, features)[mask]  # only compute the evaluation set\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(pred, dim=1)\n",
    "        \n",
    "        pre,rec,f1,sup = precision_recall_fscore_support(labels,indices.numpy())\n",
    "        #correct = torch.sum(indices == labels)\n",
    "        #acc = correct.item() * 1.0 / len(labels) #not the best metric\n",
    "        #f1 = f1_score(labels, indices)\n",
    "        #recall = recall_score(labels, indices)\n",
    "        #precision = precision_score(labels, indices)\n",
    "        \n",
    "        C = sklearn.metrics.confusion_matrix(labels, indices.numpy())\n",
    "        \n",
    "        return pre[1], rec[1], f1[1], sup[1], C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results of optimisation : \n",
    "\n",
    "Optimisation performed by looking at validation accuracy and distribution of errors across classes using confusion matrix\n",
    "- polynomial order : increase of the order tends to increase the instability of performances accross epochs, no strong impact on filter final shape -> tradeoff complexity/stability at 3\n",
    "- learning rate: small shift towards very unbalanced error, higher learning rate enabled to get a better trade-off between accuracy and distribution of error -> 0.2\n",
    "- number of epochs : event of strong instabilities across trials whatever parameters; strong instabilities allows better balance of errors but weaker accuracies; around 1500 period of stable learning\n",
    "- dropout : increase generates instabilities, tradeoff between accuracy and distribution of errors at 0.\n",
    "=> Final filter is basically always the same sort of shape as shown below"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pol_order = 3  # seems like a good trade off between performance and instability, increasing does give more\n",
    "lr = 0.3 # allows to get a good tradeoff between error distribution and accuracy\n",
    "weight_decay = 5e-6\n",
    "n_epochs = 500 \n",
    "p_dropout = 0.32  # 0.32 very good balancing, 75%, 0.3 balacing a bit less, 77%\n",
    "n_classes = 2\n",
    "in_feats=tensor_data.shape[1]\n",
    "\n",
    "true_ratio = 1074/4802 # <-- fraction of Nominations\n",
    "weights_loss = torch.FloatTensor([true_ratio, 1-true_ratio]) # to rebalance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model = LaplacianPolynomial(in_feats, n_classes, pol_order, p_dropout)\n",
    "\n",
    "loss_fcn = torch.nn.CrossEntropyLoss(weight=weights_loss)\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=lr,\n",
    "                             weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "accuracies_val = []\n",
    "accuracies_tr = []\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    loss = train(model, G, tensor_data, tensor_labels, train_mask, loss_fcn, optimizer)\n",
    "    losses.append(loss)\n",
    "    #accuracies_tr.append(acc)\n",
    "\n",
    "    pre, rec, f1, sup, C = evaluate(model, G, tensor_data, val_mask, tensor_labels)\n",
    "    #accuracies_val.append(acc)\n",
    "    if (epoch+1)%50 == 0:\n",
    "        print(\"Epoch {:05d} | Train Loss {:.4f} | Val precision {:.4%} | Val recall {:.4%} | Val F1 {:.4%}\". format(epoch+1, loss.item(), pre, rec, f1))\n",
    "\n",
    "print()\n",
    "print('Test:')\n",
    "pre, rec, f1, sup, C = evaluate(model, G, tensor_data, test_mask, tensor_labels)\n",
    "confusion_matrix(C)\n",
    "print(\"Precision {:.4%} | Recall {:.4%} | F1 {:.4%}\". format(pre, rec, f1))\n",
    "\n",
    "# instabilite au file des epochs -> more balanced, less performant"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Plot the loss\n",
    "fig, axs = plt.subplots(4,1,figsize=(6,8))\n",
    "ep = np.arange(1,n_epochs+1,1)\n",
    "axs[0].plot(ep[0:100], losses[0:100], color='Goldenrod', linewidth=2)\n",
    "axs[0].plot(ep, np.zeros(ep.shape), color='black', linewidth=0.5)\n",
    "axs[0].set_xlim([0,100])\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].set_ylabel('Loss')\n",
    "\n",
    "axs[1].plot(ep[0:100], accuracies_val[0:100], color='blue', linewidth=0.75)\n",
    "axs[1].plot(ep[0:100], accuracies_tr[0:100], color='red', linewidth=0.75)\n",
    "axs[1].plot(ep, np.zeros(ep.shape), color='black', linewidth=0.5)\n",
    "axs[1].set_xlim([0,100])\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].set_ylabel('Validation Accuracy')\n",
    "\n",
    "axs[2].plot(ep[100:], losses[100:], color='Goldenrod', linewidth=2)\n",
    "axs[2].plot(ep, np.zeros(ep.shape), color='black', linewidth=0.5)\n",
    "axs[2].set_xlim([100,n_epochs+1])\n",
    "axs[2].set_xlabel('epoch')\n",
    "axs[2].set_ylabel('Loss')\n",
    "\n",
    "axs[3].plot(ep[100:], accuracies_val[100:], color='blue', linewidth=0.75)\n",
    "axs[1].plot(ep[0:100], accuracies_tr[0:100], color='red', linewidth=0.75)\n",
    "axs[3].plot(ep, np.zeros(ep.shape), color='black', linewidth=0.5)\n",
    "axs[3].set_xlim([100,n_epochs+1])\n",
    "axs[3].set_xlabel('epoch')\n",
    "axs[3].set_ylabel('Validation Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning of Laplacian Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "\n",
    "# To tune\n",
    "learning_rate = [3e-1,5e-1,5e-2,1e-2]\n",
    "pol_order = [2,3,4] \n",
    "p_dropout = [0.2,0.3,0.4] \n",
    "weight_decay = [0,5e-5,5e-6] # by default = 0\n",
    "\n",
    "\n",
    "n_classes = 2\n",
    "in_feats=tensor_data.shape[1]\n",
    "\n",
    "true_ratio = 1074/4802 # <-- fraction of Nominations\n",
    "weights_loss = torch.FloatTensor([true_ratio, 1-true_ratio]) # to rebalance\n",
    "\n",
    "# results with features transformed and  without standardization : \n",
    "# best_F1 = 0.5662, learning_rate = 0.3, pol_order = 3, p_dropout = 0.2, weight_decay = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_LP(learning_rate, pol_order,p_dropout,weight_decay):\n",
    "    \n",
    "\n",
    "    performances = torch.zeros(len(learning_rate),len(pol_order),len(p_dropout),len(weight_decay))\n",
    "    for l, lr_ in enumerate(learning_rate):\n",
    "        for p, p_order in enumerate(pol_order):\n",
    "            for d, dropout in enumerate(p_dropout):\n",
    "                for w, weight in enumerate(weight_decay):\n",
    "                        \n",
    "                    model = LaplacianPolynomial(in_feats, n_classes, p_order, dropout)\n",
    "\n",
    "                    loss_fcn = torch.nn.CrossEntropyLoss(weight=weights_loss)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(),lr=lr_, weight_decay = weight)\n",
    "                    losses_tr = []\n",
    "                        \n",
    "                    for epoch in range(n_epochs):\n",
    "                        loss = train(model, G, tensor_data, tensor_labels, train_mask, loss_fcn, optimizer)\n",
    "                        losses_tr.append(loss.item())\n",
    "                        pre, rec, f1, sup, C = evaluate(model, G, tensor_data, val_mask, tensor_labels) \n",
    "                        performances[l,p,d,w] = f1\n",
    "                        if (epoch+1)%100 == 0:\n",
    "                            print(\"Epoch {:05d} | Train Loss {:.4f} | Val precision {:.4%} | Val recall {:.4%} | Val F1 {:.4%}\". format(epoch+1, loss.item(), pre, rec, f1))\n",
    "                            \n",
    "\n",
    "        best_performance = torch.max(performances)\n",
    "        best_idx = (performances == best_performance).nonzero();\n",
    "            \n",
    "        best_lr = learning_rate[best_idx[0,0]]\n",
    "        best_p_order = pol_order[best_idx[0,1]]\n",
    "        best_dropout = p_dropout[best_idx[0,2]]\n",
    "        best_weight = weight_decay[best_idx[0,3]]\n",
    "                \n",
    "        results = [best_performance, best_lr,  best_p_order, best_dropout,best_weight]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00100 | Train Loss 0.6797 | Val precision 31.3894% | Val recall 89.7059% | Val F1 46.5057%\n",
      "Epoch 00200 | Train Loss 0.6467 | Val precision 32.5424% | Val recall 94.1176% | Val F1 48.3627%\n",
      "Epoch 00300 | Train Loss 0.6088 | Val precision 42.4403% | Val recall 78.4314% | Val F1 55.0775%\n",
      "Epoch 00400 | Train Loss 0.6006 | Val precision 44.5428% | Val recall 74.0196% | Val F1 55.6169%\n",
      "Epoch 00500 | Train Loss 0.5977 | Val precision 44.6483% | Val recall 71.5686% | Val F1 54.9906%\n",
      "Epoch 00600 | Train Loss 0.6015 | Val precision 42.4403% | Val recall 78.4314% | Val F1 55.0775%\n",
      "Epoch 00700 | Train Loss 0.5980 | Val precision 45.4829% | Val recall 71.5686% | Val F1 55.6190%\n",
      "Epoch 00800 | Train Loss 0.5968 | Val precision 44.3750% | Val recall 69.6078% | Val F1 54.1985%\n",
      "Epoch 00900 | Train Loss 0.5983 | Val precision 44.5455% | Val recall 72.0588% | Val F1 55.0562%\n",
      "Epoch 01000 | Train Loss 0.5988 | Val precision 45.4545% | Val recall 68.6275% | Val F1 54.6875%\n",
      "Epoch 00100 | Train Loss 0.6269 | Val precision 39.8990% | Val recall 77.4510% | Val F1 52.6667%\n",
      "Epoch 00200 | Train Loss 0.6014 | Val precision 44.6064% | Val recall 75.0000% | Val F1 55.9415%\n",
      "Epoch 00300 | Train Loss 0.6067 | Val precision 44.6927% | Val recall 78.4314% | Val F1 56.9395%\n",
      "Epoch 00400 | Train Loss 0.6016 | Val precision 44.3787% | Val recall 73.5294% | Val F1 55.3506%\n",
      "Epoch 00500 | Train Loss 0.6015 | Val precision 43.1169% | Val recall 81.3725% | Val F1 56.3667%\n",
      "Epoch 00600 | Train Loss 0.6056 | Val precision 45.9144% | Val recall 57.8431% | Val F1 51.1931%\n",
      "Epoch 00700 | Train Loss 0.6169 | Val precision 38.8764% | Val recall 84.8039% | Val F1 53.3128%\n",
      "Epoch 00800 | Train Loss 0.5991 | Val precision 42.8954% | Val recall 78.4314% | Val F1 55.4593%\n",
      "Epoch 00900 | Train Loss 0.6122 | Val precision 41.5190% | Val recall 80.3922% | Val F1 54.7579%\n",
      "Epoch 01000 | Train Loss 0.6166 | Val precision 33.0948% | Val recall 90.6863% | Val F1 48.4928%\n",
      "Epoch 00100 | Train Loss 0.6794 | Val precision 31.4136% | Val recall 88.2353% | Val F1 46.3320%\n",
      "Epoch 00200 | Train Loss 0.6437 | Val precision 33.0435% | Val recall 93.1373% | Val F1 48.7805%\n",
      "Epoch 00300 | Train Loss 0.6088 | Val precision 42.6273% | Val recall 77.9412% | Val F1 55.1127%\n",
      "Epoch 00400 | Train Loss 0.6010 | Val precision 45.0161% | Val recall 68.6275% | Val F1 54.3689%\n",
      "Epoch 00500 | Train Loss 0.5991 | Val precision 45.0000% | Val recall 75.0000% | Val F1 56.2500%\n",
      "Epoch 00600 | Train Loss 0.6015 | Val precision 45.6869% | Val recall 70.0980% | Val F1 55.3191%\n",
      "Epoch 00700 | Train Loss 0.5978 | Val precision 45.6522% | Val recall 72.0588% | Val F1 55.8935%\n",
      "Epoch 00800 | Train Loss 0.5978 | Val precision 42.8571% | Val recall 77.9412% | Val F1 55.3043%\n",
      "Epoch 00900 | Train Loss 0.6026 | Val precision 46.7391% | Val recall 63.2353% | Val F1 53.7500%\n",
      "Epoch 01000 | Train Loss 0.5984 | Val precision 45.8065% | Val recall 69.6078% | Val F1 55.2529%\n",
      "Epoch 00100 | Train Loss 0.6193 | Val precision 45.2308% | Val recall 72.0588% | Val F1 55.5766%\n",
      "Epoch 00200 | Train Loss 0.6057 | Val precision 44.4776% | Val recall 73.0392% | Val F1 55.2876%\n",
      "Epoch 00300 | Train Loss 0.6100 | Val precision 44.0514% | Val recall 67.1569% | Val F1 53.2039%\n",
      "Epoch 00400 | Train Loss 0.6077 | Val precision 43.1250% | Val recall 67.6471% | Val F1 52.6718%\n",
      "Epoch 00500 | Train Loss 0.6012 | Val precision 44.7130% | Val recall 72.5490% | Val F1 55.3271%\n",
      "Epoch 00600 | Train Loss 0.6053 | Val precision 44.4099% | Val recall 70.0980% | Val F1 54.3726%\n",
      "Epoch 00700 | Train Loss 0.6018 | Val precision 44.3262% | Val recall 61.2745% | Val F1 51.4403%\n",
      "Epoch 00800 | Train Loss 0.6013 | Val precision 42.7419% | Val recall 77.9412% | Val F1 55.2083%\n",
      "Epoch 00900 | Train Loss 0.6053 | Val precision 45.0161% | Val recall 68.6275% | Val F1 54.3689%\n",
      "Epoch 01000 | Train Loss 0.6041 | Val precision 44.9367% | Val recall 69.6078% | Val F1 54.6154%\n",
      "Epoch 00100 | Train Loss 0.6197 | Val precision 45.5128% | Val recall 69.6078% | Val F1 55.0388%\n",
      "Epoch 00200 | Train Loss 0.6073 | Val precision 44.7853% | Val recall 71.5686% | Val F1 55.0943%\n",
      "Epoch 00300 | Train Loss 0.6115 | Val precision 42.0103% | Val recall 79.9020% | Val F1 55.0676%\n",
      "Epoch 00400 | Train Loss 0.6082 | Val precision 42.5134% | Val recall 77.9412% | Val F1 55.0173%\n",
      "Epoch 00500 | Train Loss 0.6032 | Val precision 44.8763% | Val recall 62.2549% | Val F1 52.1561%\n",
      "Epoch 00600 | Train Loss 0.6057 | Val precision 45.2632% | Val recall 63.2353% | Val F1 52.7607%\n",
      "Epoch 00700 | Train Loss 0.6020 | Val precision 44.3804% | Val recall 75.4902% | Val F1 55.8984%\n",
      "Epoch 00800 | Train Loss 0.6286 | Val precision 42.5373% | Val recall 55.8824% | Val F1 48.3051%\n",
      "Epoch 00900 | Train Loss 0.6099 | Val precision 43.2584% | Val recall 75.4902% | Val F1 55.0000%\n",
      "Epoch 01000 | Train Loss 0.6077 | Val precision 42.3773% | Val recall 80.3922% | Val F1 55.4992%\n",
      "Epoch 00100 | Train Loss 0.6193 | Val precision 45.2012% | Val recall 71.5686% | Val F1 55.4080%\n",
      "Epoch 00200 | Train Loss 0.6060 | Val precision 45.2599% | Val recall 72.5490% | Val F1 55.7439%\n",
      "Epoch 00300 | Train Loss 0.6097 | Val precision 43.9490% | Val recall 67.6471% | Val F1 53.2819%\n",
      "Epoch 00400 | Train Loss 0.6062 | Val precision 44.3366% | Val recall 67.1569% | Val F1 53.4113%\n",
      "Epoch 00500 | Train Loss 0.6025 | Val precision 44.3730% | Val recall 67.6471% | Val F1 53.5922%\n",
      "Epoch 00600 | Train Loss 0.6056 | Val precision 44.6945% | Val recall 68.1373% | Val F1 53.9806%\n",
      "Epoch 00700 | Train Loss 0.6010 | Val precision 43.6975% | Val recall 76.4706% | Val F1 55.6150%\n",
      "Epoch 00800 | Train Loss 0.6079 | Val precision 38.4787% | Val recall 84.3137% | Val F1 52.8418%\n",
      "Epoch 00900 | Train Loss 0.6056 | Val precision 44.7130% | Val recall 72.5490% | Val F1 55.3271%\n",
      "Epoch 01000 | Train Loss 0.6055 | Val precision 42.7027% | Val recall 77.4510% | Val F1 55.0523%\n",
      "Epoch 00100 | Train Loss 0.6262 | Val precision 44.7284% | Val recall 68.6275% | Val F1 54.1586%\n",
      "Epoch 00200 | Train Loss 0.6082 | Val precision 44.7761% | Val recall 73.5294% | Val F1 55.6586%\n",
      "Epoch 00300 | Train Loss 0.6143 | Val precision 43.0868% | Val recall 65.6863% | Val F1 52.0388%\n",
      "Epoch 00400 | Train Loss 0.6081 | Val precision 42.5474% | Val recall 76.9608% | Val F1 54.7993%\n",
      "Epoch 00500 | Train Loss 0.6095 | Val precision 42.8571% | Val recall 76.4706% | Val F1 54.9296%\n",
      "Epoch 00600 | Train Loss 0.6105 | Val precision 42.5287% | Val recall 72.5490% | Val F1 53.6232%\n",
      "Epoch 00700 | Train Loss 0.6089 | Val precision 42.8105% | Val recall 64.2157% | Val F1 51.3725%\n",
      "Epoch 00800 | Train Loss 0.6045 | Val precision 41.0891% | Val recall 81.3725% | Val F1 54.6053%\n",
      "Epoch 00900 | Train Loss 0.6145 | Val precision 42.8969% | Val recall 75.4902% | Val F1 54.7069%\n",
      "Epoch 01000 | Train Loss 0.6087 | Val precision 42.9412% | Val recall 71.5686% | Val F1 53.6765%\n",
      "Epoch 00100 | Train Loss 0.6275 | Val precision 42.9395% | Val recall 73.0392% | Val F1 54.0835%\n",
      "Epoch 00200 | Train Loss 0.6109 | Val precision 44.7950% | Val recall 69.6078% | Val F1 54.5106%\n",
      "Epoch 00300 | Train Loss 0.6184 | Val precision 42.2977% | Val recall 79.4118% | Val F1 55.1959%\n",
      "Epoch 00400 | Train Loss 0.6070 | Val precision 42.2764% | Val recall 76.4706% | Val F1 54.4503%\n",
      "Epoch 00500 | Train Loss 0.6173 | Val precision 42.5926% | Val recall 78.9216% | Val F1 55.3265%\n",
      "Epoch 00600 | Train Loss 0.6142 | Val precision 43.3022% | Val recall 68.1373% | Val F1 52.9524%\n",
      "Epoch 00700 | Train Loss 0.6081 | Val precision 44.2724% | Val recall 70.0980% | Val F1 54.2694%\n",
      "Epoch 00800 | Train Loss 0.6233 | Val precision 42.5982% | Val recall 69.1176% | Val F1 52.7103%\n",
      "Epoch 00900 | Train Loss 0.6162 | Val precision 44.0729% | Val recall 71.0784% | Val F1 54.4090%\n",
      "Epoch 01000 | Train Loss 0.6133 | Val precision 43.2331% | Val recall 56.3725% | Val F1 48.9362%\n",
      "Epoch 00100 | Train Loss 0.6265 | Val precision 44.3425% | Val recall 71.0784% | Val F1 54.6139%\n",
      "Epoch 00200 | Train Loss 0.6087 | Val precision 43.9883% | Val recall 73.5294% | Val F1 55.0459%\n",
      "Epoch 00300 | Train Loss 0.6152 | Val precision 43.3962% | Val recall 67.6471% | Val F1 52.8736%\n",
      "Epoch 00400 | Train Loss 0.6092 | Val precision 42.2961% | Val recall 68.6275% | Val F1 52.3364%\n",
      "Epoch 00500 | Train Loss 0.6085 | Val precision 43.3022% | Val recall 68.1373% | Val F1 52.9524%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00600 | Train Loss 0.6163 | Val precision 43.7063% | Val recall 61.2745% | Val F1 51.0204%\n",
      "Epoch 00700 | Train Loss 0.6148 | Val precision 42.9775% | Val recall 75.0000% | Val F1 54.6429%\n",
      "Epoch 00800 | Train Loss 0.6066 | Val precision 42.4501% | Val recall 73.0392% | Val F1 53.6937%\n",
      "Epoch 00900 | Train Loss 0.6145 | Val precision 44.1696% | Val recall 61.2745% | Val F1 51.3347%\n",
      "Epoch 01000 | Train Loss 0.6124 | Val precision 42.7536% | Val recall 57.8431% | Val F1 49.1667%\n",
      "Epoch 00100 | Train Loss 0.6195 | Val precision 41.2256% | Val recall 72.5490% | Val F1 52.5755%\n",
      "Epoch 00200 | Train Loss 0.5986 | Val precision 45.0450% | Val recall 73.5294% | Val F1 55.8659%\n",
      "Epoch 00300 | Train Loss 0.6053 | Val precision 44.7531% | Val recall 71.0784% | Val F1 54.9242%\n",
      "Epoch 00400 | Train Loss 0.6007 | Val precision 43.8202% | Val recall 76.4706% | Val F1 55.7143%\n",
      "Epoch 00500 | Train Loss 0.5978 | Val precision 45.3731% | Val recall 74.5098% | Val F1 56.4007%\n",
      "Epoch 00600 | Train Loss 0.6005 | Val precision 44.7853% | Val recall 71.5686% | Val F1 55.0943%\n",
      "Epoch 00700 | Train Loss 0.5975 | Val precision 45.5128% | Val recall 69.6078% | Val F1 55.0388%\n",
      "Epoch 00800 | Train Loss 0.5970 | Val precision 45.1807% | Val recall 73.5294% | Val F1 55.9701%\n",
      "Epoch 00900 | Train Loss 0.5985 | Val precision 45.3125% | Val recall 71.0784% | Val F1 55.3435%\n",
      "Epoch 01000 | Train Loss 0.5988 | Val precision 44.8916% | Val recall 71.0784% | Val F1 55.0285%\n",
      "Epoch 00100 | Train Loss 0.6842 | Val precision 33.1418% | Val recall 84.8039% | Val F1 47.6584%\n",
      "Epoch 00200 | Train Loss 0.6592 | Val precision 32.9310% | Val recall 93.6275% | Val F1 48.7245%\n",
      "Epoch 00300 | Train Loss 0.6104 | Val precision 43.8547% | Val recall 76.9608% | Val F1 55.8719%\n",
      "Epoch 00400 | Train Loss 0.6016 | Val precision 43.4783% | Val recall 78.4314% | Val F1 55.9441%\n",
      "Epoch 00500 | Train Loss 0.5977 | Val precision 43.1635% | Val recall 78.9216% | Val F1 55.8059%\n",
      "Epoch 00600 | Train Loss 0.6057 | Val precision 46.7153% | Val recall 62.7451% | Val F1 53.5565%\n",
      "Epoch 00700 | Train Loss 0.6049 | Val precision 42.1189% | Val recall 79.9020% | Val F1 55.1607%\n",
      "Epoch 00800 | Train Loss 0.6000 | Val precision 45.2381% | Val recall 74.5098% | Val F1 56.2963%\n",
      "Epoch 00900 | Train Loss 0.6033 | Val precision 42.6667% | Val recall 78.4314% | Val F1 55.2677%\n",
      "Epoch 01000 | Train Loss 0.6006 | Val precision 44.9568% | Val recall 76.4706% | Val F1 56.6243%\n",
      "Epoch 00100 | Train Loss 0.6164 | Val precision 45.3704% | Val recall 72.0588% | Val F1 55.6818%\n",
      "Epoch 00200 | Train Loss 0.5993 | Val precision 45.0920% | Val recall 72.0588% | Val F1 55.4717%\n",
      "Epoch 00300 | Train Loss 0.6045 | Val precision 44.9704% | Val recall 74.5098% | Val F1 56.0886%\n",
      "Epoch 00400 | Train Loss 0.6018 | Val precision 44.9405% | Val recall 74.0196% | Val F1 55.9259%\n",
      "Epoch 00500 | Train Loss 0.5991 | Val precision 44.8276% | Val recall 76.4706% | Val F1 56.5217%\n",
      "Epoch 00600 | Train Loss 0.6009 | Val precision 44.8916% | Val recall 71.0784% | Val F1 55.0285%\n",
      "Epoch 00700 | Train Loss 0.5976 | Val precision 45.7944% | Val recall 72.0588% | Val F1 56.0000%\n",
      "Epoch 00800 | Train Loss 0.5977 | Val precision 42.5197% | Val recall 79.4118% | Val F1 55.3846%\n",
      "Epoch 00900 | Train Loss 0.6008 | Val precision 45.0617% | Val recall 71.5686% | Val F1 55.3030%\n",
      "Epoch 01000 | Train Loss 0.5994 | Val precision 45.2459% | Val recall 67.6471% | Val F1 54.2240%\n",
      "Epoch 00100 | Train Loss 0.6255 | Val precision 39.2857% | Val recall 75.4902% | Val F1 51.6779%\n",
      "Epoch 00200 | Train Loss 0.6053 | Val precision 44.8795% | Val recall 73.0392% | Val F1 55.5970%\n",
      "Epoch 00300 | Train Loss 0.6094 | Val precision 43.8095% | Val recall 67.6471% | Val F1 53.1792%\n",
      "Epoch 00400 | Train Loss 0.6065 | Val precision 43.7700% | Val recall 67.1569% | Val F1 52.9981%\n",
      "Epoch 00500 | Train Loss 0.6015 | Val precision 44.3750% | Val recall 69.6078% | Val F1 54.1985%\n",
      "Epoch 00600 | Train Loss 0.6054 | Val precision 43.3803% | Val recall 75.4902% | Val F1 55.0984%\n",
      "Epoch 00700 | Train Loss 0.6008 | Val precision 44.5230% | Val recall 61.7647% | Val F1 51.7454%\n",
      "Epoch 00800 | Train Loss 0.6002 | Val precision 44.1520% | Val recall 74.0196% | Val F1 55.3114%\n",
      "Epoch 00900 | Train Loss 0.6044 | Val precision 45.0311% | Val recall 71.0784% | Val F1 55.1331%\n",
      "Epoch 01000 | Train Loss 0.6046 | Val precision 44.6875% | Val recall 70.0980% | Val F1 54.5802%\n",
      "Epoch 00100 | Train Loss 0.6268 | Val precision 39.1858% | Val recall 75.4902% | Val F1 51.5913%\n",
      "Epoch 00200 | Train Loss 0.6071 | Val precision 44.6429% | Val recall 73.5294% | Val F1 55.5556%\n",
      "Epoch 00300 | Train Loss 0.6100 | Val precision 43.9883% | Val recall 73.5294% | Val F1 55.0459%\n",
      "Epoch 00400 | Train Loss 0.6073 | Val precision 43.8806% | Val recall 72.0588% | Val F1 54.5455%\n",
      "Epoch 00500 | Train Loss 0.6030 | Val precision 44.4099% | Val recall 70.0980% | Val F1 54.3726%\n",
      "Epoch 00600 | Train Loss 0.6060 | Val precision 45.2381% | Val recall 65.1961% | Val F1 53.4137%\n",
      "Epoch 00700 | Train Loss 0.6011 | Val precision 44.6429% | Val recall 61.2745% | Val F1 51.6529%\n",
      "Epoch 00800 | Train Loss 0.6035 | Val precision 35.7285% | Val recall 87.7451% | Val F1 50.7801%\n",
      "Epoch 00900 | Train Loss 0.6189 | Val precision 42.5876% | Val recall 77.4510% | Val F1 54.9565%\n",
      "Epoch 01000 | Train Loss 0.6148 | Val precision 37.2093% | Val recall 86.2745% | Val F1 51.9941%\n",
      "Epoch 00100 | Train Loss 0.6257 | Val precision 39.4402% | Val recall 75.9804% | Val F1 51.9263%\n",
      "Epoch 00200 | Train Loss 0.6056 | Val precision 44.7853% | Val recall 71.5686% | Val F1 55.0943%\n",
      "Epoch 00300 | Train Loss 0.6101 | Val precision 43.1677% | Val recall 68.1373% | Val F1 52.8517%\n",
      "Epoch 00400 | Train Loss 0.6064 | Val precision 43.6137% | Val recall 68.6275% | Val F1 53.3333%\n",
      "Epoch 00500 | Train Loss 0.6037 | Val precision 43.4911% | Val recall 72.0588% | Val F1 54.2435%\n",
      "Epoch 00600 | Train Loss 0.6053 | Val precision 44.6602% | Val recall 67.6471% | Val F1 53.8012%\n",
      "Epoch 00700 | Train Loss 0.6008 | Val precision 43.7326% | Val recall 76.9608% | Val F1 55.7726%\n",
      "Epoch 00800 | Train Loss 0.6040 | Val precision 44.2675% | Val recall 68.1373% | Val F1 53.6680%\n",
      "Epoch 00900 | Train Loss 0.6058 | Val precision 44.7368% | Val recall 66.6667% | Val F1 53.5433%\n",
      "Epoch 01000 | Train Loss 0.6052 | Val precision 44.8276% | Val recall 63.7255% | Val F1 52.6316%\n",
      "Epoch 00100 | Train Loss 0.6723 | Val precision 27.8689% | Val recall 100.0000% | Val F1 43.5897%\n",
      "Epoch 00200 | Train Loss 0.6093 | Val precision 44.1520% | Val recall 74.0196% | Val F1 55.3114%\n",
      "Epoch 00300 | Train Loss 0.6138 | Val precision 43.2602% | Val recall 67.6471% | Val F1 52.7725%\n",
      "Epoch 00400 | Train Loss 0.6090 | Val precision 42.7397% | Val recall 76.4706% | Val F1 54.8330%\n",
      "Epoch 00500 | Train Loss 0.6092 | Val precision 43.0137% | Val recall 76.9608% | Val F1 55.1845%\n",
      "Epoch 00600 | Train Loss 0.6115 | Val precision 42.7762% | Val recall 74.0196% | Val F1 54.2190%\n",
      "Epoch 00700 | Train Loss 0.6075 | Val precision 43.6066% | Val recall 65.1961% | Val F1 52.2593%\n",
      "Epoch 00800 | Train Loss 0.6074 | Val precision 42.5134% | Val recall 77.9412% | Val F1 55.0173%\n",
      "Epoch 00900 | Train Loss 0.6151 | Val precision 43.3140% | Val recall 73.0392% | Val F1 54.3796%\n",
      "Epoch 01000 | Train Loss 0.6085 | Val precision 43.3022% | Val recall 68.1373% | Val F1 52.9524%\n",
      "Epoch 00100 | Train Loss 0.6701 | Val precision 29.8643% | Val recall 97.0588% | Val F1 45.6747%\n",
      "Epoch 00200 | Train Loss 0.6109 | Val precision 43.1429% | Val recall 74.0196% | Val F1 54.5126%\n",
      "Epoch 00300 | Train Loss 0.6145 | Val precision 43.5530% | Val recall 74.5098% | Val F1 54.9729%\n",
      "Epoch 00400 | Train Loss 0.6076 | Val precision 44.7917% | Val recall 63.2353% | Val F1 52.4390%\n",
      "Epoch 00500 | Train Loss 0.6143 | Val precision 37.7729% | Val recall 84.8039% | Val F1 52.2659%\n",
      "Epoch 00600 | Train Loss 0.6108 | Val precision 43.0868% | Val recall 65.6863% | Val F1 52.0388%\n",
      "Epoch 00700 | Train Loss 0.6117 | Val precision 43.4286% | Val recall 74.5098% | Val F1 54.8736%\n",
      "Epoch 00800 | Train Loss 0.6088 | Val precision 34.9301% | Val recall 85.7843% | Val F1 49.6454%\n",
      "Epoch 00900 | Train Loss 0.6251 | Val precision 41.4322% | Val recall 79.4118% | Val F1 54.4538%\n",
      "Epoch 01000 | Train Loss 0.6178 | Val precision 41.7808% | Val recall 59.8039% | Val F1 49.1935%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00100 | Train Loss 0.6738 | Val precision 28.5112% | Val recall 99.5098% | Val F1 44.3231%\n",
      "Epoch 00200 | Train Loss 0.6118 | Val precision 43.4659% | Val recall 75.0000% | Val F1 55.0360%\n",
      "Epoch 00300 | Train Loss 0.6144 | Val precision 43.2177% | Val recall 67.1569% | Val F1 52.5912%\n",
      "Epoch 00400 | Train Loss 0.6089 | Val precision 42.3077% | Val recall 70.0980% | Val F1 52.7675%\n",
      "Epoch 00500 | Train Loss 0.6090 | Val precision 42.3881% | Val recall 69.6078% | Val F1 52.6902%\n",
      "Epoch 00600 | Train Loss 0.6129 | Val precision 43.3140% | Val recall 73.0392% | Val F1 54.3796%\n",
      "Epoch 00700 | Train Loss 0.6099 | Val precision 43.5811% | Val recall 63.2353% | Val F1 51.6000%\n",
      "Epoch 00800 | Train Loss 0.6064 | Val precision 42.2572% | Val recall 78.9216% | Val F1 55.0427%\n",
      "Epoch 00900 | Train Loss 0.6137 | Val precision 45.3020% | Val recall 66.1765% | Val F1 53.7849%\n",
      "Epoch 01000 | Train Loss 0.6089 | Val precision 43.8127% | Val recall 64.2157% | Val F1 52.0875%\n",
      "Epoch 00100 | Train Loss 0.6131 | Val precision 36.8889% | Val recall 81.3725% | Val F1 50.7645%\n",
      "Epoch 00200 | Train Loss 0.6007 | Val precision 45.2532% | Val recall 70.0980% | Val F1 55.0000%\n",
      "Epoch 00300 | Train Loss 0.6025 | Val precision 45.2381% | Val recall 74.5098% | Val F1 56.2963%\n",
      "Epoch 00400 | Train Loss 0.6000 | Val precision 44.9541% | Val recall 72.0588% | Val F1 55.3672%\n",
      "Epoch 00500 | Train Loss 0.6048 | Val precision 45.3416% | Val recall 71.5686% | Val F1 55.5133%\n",
      "Epoch 00600 | Train Loss 0.5975 | Val precision 44.5748% | Val recall 74.5098% | Val F1 55.7798%\n",
      "Epoch 00700 | Train Loss 0.6015 | Val precision 44.6991% | Val recall 76.4706% | Val F1 56.4195%\n",
      "Epoch 00800 | Train Loss 0.5990 | Val precision 44.7674% | Val recall 75.4902% | Val F1 56.2044%\n",
      "Epoch 00900 | Train Loss 0.6019 | Val precision 43.4783% | Val recall 78.4314% | Val F1 55.9441%\n",
      "Epoch 01000 | Train Loss 0.5987 | Val precision 44.7531% | Val recall 71.0784% | Val F1 54.9242%\n",
      "Epoch 00100 | Train Loss 0.6376 | Val precision 36.2812% | Val recall 78.4314% | Val F1 49.6124%\n",
      "Epoch 00200 | Train Loss 0.6014 | Val precision 44.7592% | Val recall 77.4510% | Val F1 56.7325%\n",
      "Epoch 00300 | Train Loss 0.6027 | Val precision 45.1039% | Val recall 74.5098% | Val F1 56.1922%\n",
      "Epoch 00400 | Train Loss 0.6036 | Val precision 44.2197% | Val recall 75.0000% | Val F1 55.6364%\n",
      "Epoch 00500 | Train Loss 0.6108 | Val precision 45.5197% | Val recall 62.2549% | Val F1 52.5880%\n",
      "Epoch 00600 | Train Loss 0.5998 | Val precision 43.2000% | Val recall 79.4118% | Val F1 55.9585%\n",
      "Epoch 00700 | Train Loss 0.6114 | Val precision 45.2830% | Val recall 58.8235% | Val F1 51.1727%\n",
      "Epoch 00800 | Train Loss 0.6100 | Val precision 37.1308% | Val recall 86.2745% | Val F1 51.9174%\n",
      "Epoch 00900 | Train Loss 0.6044 | Val precision 45.9364% | Val recall 63.7255% | Val F1 53.3881%\n",
      "Epoch 01000 | Train Loss 0.6164 | Val precision 45.8333% | Val recall 64.7059% | Val F1 53.6585%\n",
      "Epoch 00100 | Train Loss 0.6189 | Val precision 33.9960% | Val recall 83.8235% | Val F1 48.3734%\n",
      "Epoch 00200 | Train Loss 0.6009 | Val precision 44.9848% | Val recall 72.5490% | Val F1 55.5347%\n",
      "Epoch 00300 | Train Loss 0.6025 | Val precision 44.0678% | Val recall 76.4706% | Val F1 55.9140%\n",
      "Epoch 00400 | Train Loss 0.5999 | Val precision 44.8505% | Val recall 66.1765% | Val F1 53.4653%\n",
      "Epoch 00500 | Train Loss 0.6046 | Val precision 44.7130% | Val recall 72.5490% | Val F1 55.3271%\n",
      "Epoch 00600 | Train Loss 0.5985 | Val precision 45.3704% | Val recall 72.0588% | Val F1 55.6818%\n",
      "Epoch 00700 | Train Loss 0.6019 | Val precision 43.5262% | Val recall 77.4510% | Val F1 55.7319%\n",
      "Epoch 00800 | Train Loss 0.5988 | Val precision 45.0292% | Val recall 75.4902% | Val F1 56.4103%\n",
      "Epoch 00900 | Train Loss 0.6030 | Val precision 45.3968% | Val recall 70.0980% | Val F1 55.1060%\n",
      "Epoch 01000 | Train Loss 0.5980 | Val precision 45.1613% | Val recall 75.4902% | Val F1 56.5138%\n",
      "Epoch 00100 | Train Loss 0.6595 | Val precision 31.0016% | Val recall 95.5882% | Val F1 46.8187%\n",
      "Epoch 00200 | Train Loss 0.6106 | Val precision 45.1807% | Val recall 73.5294% | Val F1 55.9701%\n",
      "Epoch 00300 | Train Loss 0.6109 | Val precision 42.9333% | Val recall 78.9216% | Val F1 55.6131%\n",
      "Epoch 00400 | Train Loss 0.6035 | Val precision 43.6137% | Val recall 68.6275% | Val F1 53.3333%\n",
      "Epoch 00500 | Train Loss 0.6116 | Val precision 43.9306% | Val recall 74.5098% | Val F1 55.2727%\n",
      "Epoch 00600 | Train Loss 0.6052 | Val precision 44.3038% | Val recall 68.6275% | Val F1 53.8462%\n",
      "Epoch 00700 | Train Loss 0.6059 | Val precision 44.7130% | Val recall 72.5490% | Val F1 55.3271%\n",
      "Epoch 00800 | Train Loss 0.6034 | Val precision 44.1261% | Val recall 75.4902% | Val F1 55.6962%\n",
      "Epoch 00900 | Train Loss 0.6069 | Val precision 44.1176% | Val recall 73.5294% | Val F1 55.1471%\n",
      "Epoch 01000 | Train Loss 0.6043 | Val precision 44.3114% | Val recall 72.5490% | Val F1 55.0186%\n",
      "Epoch 00100 | Train Loss 0.6861 | Val precision 28.6337% | Val recall 96.5686% | Val F1 44.1704%\n",
      "Epoch 00200 | Train Loss 0.6708 | Val precision 31.4815% | Val recall 91.6667% | Val F1 46.8672%\n",
      "Epoch 00300 | Train Loss 0.6164 | Val precision 42.1053% | Val recall 74.5098% | Val F1 53.8053%\n",
      "Epoch 00400 | Train Loss 0.6046 | Val precision 44.2073% | Val recall 71.0784% | Val F1 54.5113%\n",
      "Epoch 00500 | Train Loss 0.6104 | Val precision 44.1281% | Val recall 60.7843% | Val F1 51.1340%\n",
      "Epoch 00600 | Train Loss 0.6075 | Val precision 44.4068% | Val recall 64.2157% | Val F1 52.5050%\n",
      "Epoch 00700 | Train Loss 0.6055 | Val precision 38.6364% | Val recall 83.3333% | Val F1 52.7950%\n",
      "Epoch 00800 | Train Loss 0.6091 | Val precision 42.0779% | Val recall 79.4118% | Val F1 55.0085%\n",
      "Epoch 00900 | Train Loss 0.6092 | Val precision 44.1281% | Val recall 60.7843% | Val F1 51.1340%\n",
      "Epoch 01000 | Train Loss 0.6059 | Val precision 43.3428% | Val recall 75.0000% | Val F1 54.9372%\n",
      "Epoch 00100 | Train Loss 0.6591 | Val precision 29.3255% | Val recall 98.0392% | Val F1 45.1467%\n",
      "Epoch 00200 | Train Loss 0.6106 | Val precision 45.1515% | Val recall 73.0392% | Val F1 55.8052%\n",
      "Epoch 00300 | Train Loss 0.6113 | Val precision 42.7441% | Val recall 79.4118% | Val F1 55.5746%\n",
      "Epoch 00400 | Train Loss 0.6035 | Val precision 43.5976% | Val recall 70.0980% | Val F1 53.7594%\n",
      "Epoch 00500 | Train Loss 0.6100 | Val precision 43.8953% | Val recall 74.0196% | Val F1 55.1095%\n",
      "Epoch 00600 | Train Loss 0.6053 | Val precision 43.9528% | Val recall 73.0392% | Val F1 54.8803%\n",
      "Epoch 00700 | Train Loss 0.6046 | Val precision 44.0476% | Val recall 72.5490% | Val F1 54.8148%\n",
      "Epoch 00800 | Train Loss 0.6039 | Val precision 44.4444% | Val recall 74.5098% | Val F1 55.6777%\n",
      "Epoch 00900 | Train Loss 0.6062 | Val precision 44.0233% | Val recall 74.0196% | Val F1 55.2102%\n",
      "Epoch 01000 | Train Loss 0.6040 | Val precision 44.7712% | Val recall 67.1569% | Val F1 53.7255%\n",
      "Epoch 00100 | Train Loss 0.6539 | Val precision 37.2263% | Val recall 75.0000% | Val F1 49.7561%\n",
      "Epoch 00200 | Train Loss 0.6130 | Val precision 42.9448% | Val recall 68.6275% | Val F1 52.8302%\n",
      "Epoch 00300 | Train Loss 0.6150 | Val precision 42.8962% | Val recall 76.9608% | Val F1 55.0877%\n",
      "Epoch 00400 | Train Loss 0.6076 | Val precision 42.4615% | Val recall 67.6471% | Val F1 52.1739%\n",
      "Epoch 00500 | Train Loss 0.6170 | Val precision 42.9379% | Val recall 74.5098% | Val F1 54.4803%\n",
      "Epoch 00600 | Train Loss 0.6097 | Val precision 42.9799% | Val recall 73.5294% | Val F1 54.2495%\n",
      "Epoch 00700 | Train Loss 0.6099 | Val precision 43.5897% | Val recall 66.6667% | Val F1 52.7132%\n",
      "Epoch 00800 | Train Loss 0.6106 | Val precision 43.7126% | Val recall 71.5686% | Val F1 54.2751%\n",
      "Epoch 00900 | Train Loss 0.6140 | Val precision 43.5897% | Val recall 66.6667% | Val F1 52.7132%\n",
      "Epoch 01000 | Train Loss 0.6102 | Val precision 43.6364% | Val recall 70.5882% | Val F1 53.9326%\n",
      "Epoch 00100 | Train Loss 0.6914 | Val precision 30.7350% | Val recall 67.6471% | Val F1 42.2665%\n",
      "Epoch 00200 | Train Loss 0.6169 | Val precision 45.0617% | Val recall 71.5686% | Val F1 55.3030%\n",
      "Epoch 00300 | Train Loss 0.6153 | Val precision 44.2478% | Val recall 73.5294% | Val F1 55.2486%\n",
      "Epoch 00400 | Train Loss 0.6081 | Val precision 43.7299% | Val recall 66.6667% | Val F1 52.8155%\n",
      "Epoch 00500 | Train Loss 0.6151 | Val precision 43.2787% | Val recall 64.7059% | Val F1 51.8664%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00600 | Train Loss 0.6110 | Val precision 42.8977% | Val recall 74.0196% | Val F1 54.3165%\n",
      "Epoch 00700 | Train Loss 0.6292 | Val precision 43.3594% | Val recall 54.4118% | Val F1 48.2609%\n",
      "Epoch 00800 | Train Loss 0.6196 | Val precision 37.6906% | Val recall 84.8039% | Val F1 52.1870%\n",
      "Epoch 00900 | Train Loss 0.6116 | Val precision 43.0233% | Val recall 54.4118% | Val F1 48.0519%\n",
      "Epoch 01000 | Train Loss 0.6111 | Val precision 39.8104% | Val recall 82.3529% | Val F1 53.6741%\n",
      "Epoch 00100 | Train Loss 0.6376 | Val precision 35.1190% | Val recall 86.7647% | Val F1 50.0000%\n",
      "Epoch 00200 | Train Loss 0.6139 | Val precision 44.1640% | Val recall 68.6275% | Val F1 53.7428%\n",
      "Epoch 00300 | Train Loss 0.6156 | Val precision 42.3592% | Val recall 77.4510% | Val F1 54.7660%\n",
      "Epoch 00400 | Train Loss 0.6076 | Val precision 42.2857% | Val recall 72.5490% | Val F1 53.4296%\n",
      "Epoch 00500 | Train Loss 0.6190 | Val precision 44.0252% | Val recall 68.6275% | Val F1 53.6398%\n",
      "Epoch 00600 | Train Loss 0.6085 | Val precision 43.6306% | Val recall 67.1569% | Val F1 52.8958%\n",
      "Epoch 00700 | Train Loss 0.6113 | Val precision 43.5673% | Val recall 73.0392% | Val F1 54.5788%\n",
      "Epoch 00800 | Train Loss 0.6104 | Val precision 43.5159% | Val recall 74.0196% | Val F1 54.8094%\n",
      "Epoch 00900 | Train Loss 0.6128 | Val precision 42.7245% | Val recall 67.6471% | Val F1 52.3719%\n",
      "Epoch 01000 | Train Loss 0.6108 | Val precision 43.6923% | Val recall 69.6078% | Val F1 53.6862%\n",
      "Epoch 00100 | Train Loss 0.6772 | Val precision 34.5538% | Val recall 74.0196% | Val F1 47.1139%\n",
      "Epoch 00200 | Train Loss 0.6513 | Val precision 30.5864% | Val recall 94.6078% | Val F1 46.2275%\n",
      "Epoch 00300 | Train Loss 0.6086 | Val precision 45.4819% | Val recall 74.0196% | Val F1 56.3433%\n",
      "Epoch 00400 | Train Loss 0.6006 | Val precision 44.4134% | Val recall 77.9412% | Val F1 56.5836%\n",
      "Epoch 00500 | Train Loss 0.5977 | Val precision 44.7531% | Val recall 71.0784% | Val F1 54.9242%\n",
      "Epoch 00600 | Train Loss 0.6015 | Val precision 44.7977% | Val recall 75.9804% | Val F1 56.3636%\n",
      "Epoch 00700 | Train Loss 0.5978 | Val precision 45.3968% | Val recall 70.0980% | Val F1 55.1060%\n",
      "Epoch 00800 | Train Loss 0.5968 | Val precision 45.0617% | Val recall 71.5686% | Val F1 55.3030%\n",
      "Epoch 00900 | Train Loss 0.5983 | Val precision 44.8916% | Val recall 71.0784% | Val F1 55.0285%\n",
      "Epoch 01000 | Train Loss 0.5986 | Val precision 45.3968% | Val recall 70.0980% | Val F1 55.1060%\n",
      "Epoch 00100 | Train Loss 0.6637 | Val precision 30.2508% | Val recall 94.6078% | Val F1 45.8432%\n",
      "Epoch 00200 | Train Loss 0.6051 | Val precision 43.3803% | Val recall 75.4902% | Val F1 55.0984%\n",
      "Epoch 00300 | Train Loss 0.6102 | Val precision 38.9140% | Val recall 84.3137% | Val F1 53.2508%\n",
      "Epoch 00400 | Train Loss 0.6018 | Val precision 43.9776% | Val recall 76.9608% | Val F1 55.9715%\n",
      "Epoch 00500 | Train Loss 0.6004 | Val precision 44.5141% | Val recall 69.6078% | Val F1 54.3021%\n",
      "Epoch 00600 | Train Loss 0.6052 | Val precision 45.5782% | Val recall 65.6863% | Val F1 53.8153%\n",
      "Epoch 00700 | Train Loss 0.6080 | Val precision 39.6313% | Val recall 84.3137% | Val F1 53.9185%\n",
      "Epoch 00800 | Train Loss 0.5995 | Val precision 38.1898% | Val recall 84.8039% | Val F1 52.6636%\n",
      "Epoch 00900 | Train Loss 0.6084 | Val precision 44.0771% | Val recall 78.4314% | Val F1 56.4374%\n",
      "Epoch 01000 | Train Loss 0.6050 | Val precision 37.6087% | Val recall 84.8039% | Val F1 52.1084%\n",
      "Epoch 00100 | Train Loss 0.6772 | Val precision 35.0711% | Val recall 72.5490% | Val F1 47.2843%\n",
      "Epoch 00200 | Train Loss 0.6511 | Val precision 30.7571% | Val recall 95.5882% | Val F1 46.5394%\n",
      "Epoch 00300 | Train Loss 0.6091 | Val precision 44.2529% | Val recall 75.4902% | Val F1 55.7971%\n",
      "Epoch 00400 | Train Loss 0.6007 | Val precision 44.3787% | Val recall 73.5294% | Val F1 55.3506%\n",
      "Epoch 00500 | Train Loss 0.5982 | Val precision 45.0450% | Val recall 73.5294% | Val F1 55.8659%\n",
      "Epoch 00600 | Train Loss 0.6009 | Val precision 45.2532% | Val recall 70.0980% | Val F1 55.0000%\n",
      "Epoch 00700 | Train Loss 0.5977 | Val precision 45.4829% | Val recall 71.5686% | Val F1 55.6190%\n",
      "Epoch 00800 | Train Loss 0.5977 | Val precision 42.9730% | Val recall 77.9412% | Val F1 55.4007%\n",
      "Epoch 00900 | Train Loss 0.6012 | Val precision 46.4151% | Val recall 60.2941% | Val F1 52.4520%\n",
      "Epoch 01000 | Train Loss 0.5984 | Val precision 45.4545% | Val recall 66.1765% | Val F1 53.8922%\n",
      "Epoch 00100 | Train Loss 0.6862 | Val precision 29.1291% | Val recall 95.0980% | Val F1 44.5977%\n",
      "Epoch 00200 | Train Loss 0.6491 | Val precision 32.7526% | Val recall 92.1569% | Val F1 48.3290%\n",
      "Epoch 00300 | Train Loss 0.6134 | Val precision 45.3453% | Val recall 74.0196% | Val F1 56.2384%\n",
      "Epoch 00400 | Train Loss 0.6065 | Val precision 44.5161% | Val recall 67.6471% | Val F1 53.6965%\n",
      "Epoch 00500 | Train Loss 0.6014 | Val precision 44.7761% | Val recall 73.5294% | Val F1 55.6586%\n",
      "Epoch 00600 | Train Loss 0.6056 | Val precision 44.5820% | Val recall 70.5882% | Val F1 54.6490%\n",
      "Epoch 00700 | Train Loss 0.6007 | Val precision 44.6429% | Val recall 61.2745% | Val F1 51.6529%\n",
      "Epoch 00800 | Train Loss 0.6005 | Val precision 44.1176% | Val recall 73.5294% | Val F1 55.1471%\n",
      "Epoch 00900 | Train Loss 0.6048 | Val precision 44.6483% | Val recall 71.5686% | Val F1 54.9906%\n",
      "Epoch 01000 | Train Loss 0.6041 | Val precision 44.6602% | Val recall 67.6471% | Val F1 53.8012%\n",
      "Epoch 00100 | Train Loss 0.6283 | Val precision 40.6736% | Val recall 76.9608% | Val F1 53.2203%\n",
      "Epoch 00200 | Train Loss 0.6122 | Val precision 44.5783% | Val recall 72.5490% | Val F1 55.2239%\n",
      "Epoch 00300 | Train Loss 0.6123 | Val precision 41.4392% | Val recall 81.8627% | Val F1 55.0247%\n",
      "Epoch 00400 | Train Loss 0.6095 | Val precision 42.9752% | Val recall 76.4706% | Val F1 55.0265%\n",
      "Epoch 00500 | Train Loss 0.6024 | Val precision 44.6667% | Val recall 65.6863% | Val F1 53.1746%\n",
      "Epoch 00600 | Train Loss 0.6067 | Val precision 45.1957% | Val recall 62.2549% | Val F1 52.3711%\n",
      "Epoch 00700 | Train Loss 0.6034 | Val precision 44.4043% | Val recall 60.2941% | Val F1 51.1435%\n",
      "Epoch 00800 | Train Loss 0.6095 | Val precision 44.4759% | Val recall 76.9608% | Val F1 56.3734%\n",
      "Epoch 00900 | Train Loss 0.6327 | Val precision 34.8077% | Val recall 88.7255% | Val F1 50.0000%\n",
      "Epoch 01000 | Train Loss 0.6058 | Val precision 42.0918% | Val recall 80.8824% | Val F1 55.3691%\n",
      "Epoch 00100 | Train Loss 0.6305 | Val precision 41.5430% | Val recall 68.6275% | Val F1 51.7560%\n",
      "Epoch 00200 | Train Loss 0.6063 | Val precision 45.3988% | Val recall 72.5490% | Val F1 55.8491%\n",
      "Epoch 00300 | Train Loss 0.6103 | Val precision 43.9759% | Val recall 71.5686% | Val F1 54.4776%\n",
      "Epoch 00400 | Train Loss 0.6061 | Val precision 44.2675% | Val recall 68.1373% | Val F1 53.6680%\n",
      "Epoch 00500 | Train Loss 0.6028 | Val precision 44.0895% | Val recall 67.6471% | Val F1 53.3849%\n",
      "Epoch 00600 | Train Loss 0.6055 | Val precision 44.6945% | Val recall 68.1373% | Val F1 53.9806%\n",
      "Epoch 00700 | Train Loss 0.6010 | Val precision 43.8202% | Val recall 76.4706% | Val F1 55.7143%\n",
      "Epoch 00800 | Train Loss 0.6065 | Val precision 38.0531% | Val recall 84.3137% | Val F1 52.4390%\n",
      "Epoch 00900 | Train Loss 0.6058 | Val precision 44.7205% | Val recall 70.5882% | Val F1 54.7529%\n",
      "Epoch 01000 | Train Loss 0.6056 | Val precision 43.9490% | Val recall 67.6471% | Val F1 53.2819%\n",
      "Epoch 00100 | Train Loss 0.6307 | Val precision 43.8710% | Val recall 66.6667% | Val F1 52.9183%\n",
      "Epoch 00200 | Train Loss 0.6074 | Val precision 44.3425% | Val recall 71.0784% | Val F1 54.6139%\n",
      "Epoch 00300 | Train Loss 0.6136 | Val precision 42.6829% | Val recall 68.6275% | Val F1 52.6316%\n",
      "Epoch 00400 | Train Loss 0.6074 | Val precision 43.1085% | Val recall 72.0588% | Val F1 53.9450%\n",
      "Epoch 00500 | Train Loss 0.6085 | Val precision 44.5946% | Val recall 64.7059% | Val F1 52.8000%\n",
      "Epoch 00600 | Train Loss 0.6107 | Val precision 42.6380% | Val recall 68.1373% | Val F1 52.4528%\n",
      "Epoch 00700 | Train Loss 0.6072 | Val precision 42.6752% | Val recall 65.6863% | Val F1 51.7375%\n",
      "Epoch 00800 | Train Loss 0.6056 | Val precision 42.9379% | Val recall 74.5098% | Val F1 54.4803%\n",
      "Epoch 00900 | Train Loss 0.6149 | Val precision 43.0199% | Val recall 74.0196% | Val F1 54.4144%\n",
      "Epoch 01000 | Train Loss 0.6090 | Val precision 42.5076% | Val recall 68.1373% | Val F1 52.3540%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00100 | Train Loss 0.6552 | Val precision 36.6743% | Val recall 78.9216% | Val F1 50.0778%\n",
      "Epoch 00200 | Train Loss 0.6097 | Val precision 43.8235% | Val recall 73.0392% | Val F1 54.7794%\n",
      "Epoch 00300 | Train Loss 0.6154 | Val precision 44.8171% | Val recall 72.0588% | Val F1 55.2632%\n",
      "Epoch 00400 | Train Loss 0.6073 | Val precision 44.8029% | Val recall 61.2745% | Val F1 51.7598%\n",
      "Epoch 00500 | Train Loss 0.6168 | Val precision 38.8764% | Val recall 84.8039% | Val F1 53.3128%\n",
      "Epoch 00600 | Train Loss 0.6111 | Val precision 43.3225% | Val recall 65.1961% | Val F1 52.0548%\n",
      "Epoch 00700 | Train Loss 0.6102 | Val precision 38.8262% | Val recall 84.3137% | Val F1 53.1685%\n",
      "Epoch 00800 | Train Loss 0.6143 | Val precision 41.8060% | Val recall 61.2745% | Val F1 49.7018%\n",
      "Epoch 00900 | Train Loss 0.6125 | Val precision 43.1734% | Val recall 57.3529% | Val F1 49.2632%\n",
      "Epoch 01000 | Train Loss 0.6197 | Val precision 43.1159% | Val recall 58.3333% | Val F1 49.5833%\n",
      "Epoch 00100 | Train Loss 0.6308 | Val precision 40.8571% | Val recall 70.0980% | Val F1 51.6245%\n",
      "Epoch 00200 | Train Loss 0.6077 | Val precision 44.8485% | Val recall 72.5490% | Val F1 55.4307%\n",
      "Epoch 00300 | Train Loss 0.6145 | Val precision 42.1512% | Val recall 71.0784% | Val F1 52.9197%\n",
      "Epoch 00400 | Train Loss 0.6078 | Val precision 42.6630% | Val recall 76.9608% | Val F1 54.8951%\n",
      "Epoch 00500 | Train Loss 0.6139 | Val precision 42.9448% | Val recall 68.6275% | Val F1 52.8302%\n",
      "Epoch 00600 | Train Loss 0.6115 | Val precision 42.8571% | Val recall 73.5294% | Val F1 54.1516%\n",
      "Epoch 00700 | Train Loss 0.6068 | Val precision 44.0000% | Val recall 59.3137% | Val F1 50.5219%\n",
      "Epoch 00800 | Train Loss 0.6064 | Val precision 42.7793% | Val recall 76.9608% | Val F1 54.9912%\n",
      "Epoch 00900 | Train Loss 0.6144 | Val precision 45.1178% | Val recall 65.6863% | Val F1 53.4930%\n",
      "Epoch 01000 | Train Loss 0.6085 | Val precision 43.5811% | Val recall 63.2353% | Val F1 51.6000%\n",
      "Epoch 00100 | Train Loss 0.6863 | Val precision 26.5971% | Val recall 100.0000% | Val F1 42.0185%\n",
      "Epoch 00200 | Train Loss 0.6184 | Val precision 38.3693% | Val recall 78.4314% | Val F1 51.5298%\n",
      "Epoch 00300 | Train Loss 0.6062 | Val precision 43.9560% | Val recall 78.4314% | Val F1 56.3380%\n",
      "Epoch 00400 | Train Loss 0.6008 | Val precision 44.2815% | Val recall 74.0196% | Val F1 55.4128%\n",
      "Epoch 00500 | Train Loss 0.5979 | Val precision 44.7205% | Val recall 70.5882% | Val F1 54.7529%\n",
      "Epoch 00600 | Train Loss 0.6029 | Val precision 44.5070% | Val recall 77.4510% | Val F1 56.5295%\n",
      "Epoch 00700 | Train Loss 0.5978 | Val precision 45.4829% | Val recall 71.5686% | Val F1 55.6190%\n",
      "Epoch 00800 | Train Loss 0.5971 | Val precision 45.0311% | Val recall 71.0784% | Val F1 55.1331%\n",
      "Epoch 00900 | Train Loss 0.5985 | Val precision 44.6483% | Val recall 71.5686% | Val F1 54.9906%\n",
      "Epoch 01000 | Train Loss 0.5991 | Val precision 45.1713% | Val recall 71.0784% | Val F1 55.2381%\n",
      "Epoch 00100 | Train Loss 0.6660 | Val precision 29.8780% | Val recall 96.0784% | Val F1 45.5814%\n",
      "Epoch 00200 | Train Loss 0.6041 | Val precision 44.5748% | Val recall 74.5098% | Val F1 55.7798%\n",
      "Epoch 00300 | Train Loss 0.6098 | Val precision 41.6244% | Val recall 80.3922% | Val F1 54.8495%\n",
      "Epoch 00400 | Train Loss 0.6016 | Val precision 45.0000% | Val recall 70.5882% | Val F1 54.9618%\n",
      "Epoch 00500 | Train Loss 0.5978 | Val precision 43.0851% | Val recall 79.4118% | Val F1 55.8621%\n",
      "Epoch 00600 | Train Loss 0.6014 | Val precision 44.8680% | Val recall 75.0000% | Val F1 56.1468%\n",
      "Epoch 00700 | Train Loss 0.5983 | Val precision 45.5128% | Val recall 69.6078% | Val F1 55.0388%\n",
      "Epoch 00800 | Train Loss 0.6100 | Val precision 36.8530% | Val recall 87.2549% | Val F1 51.8195%\n",
      "Epoch 00900 | Train Loss 0.6096 | Val precision 40.0468% | Val recall 83.8235% | Val F1 54.1997%\n",
      "Epoch 01000 | Train Loss 0.6029 | Val precision 40.5276% | Val recall 82.8431% | Val F1 54.4283%\n",
      "Epoch 00100 | Train Loss 0.6879 | Val precision 26.8717% | Val recall 98.5294% | Val F1 42.2269%\n",
      "Epoch 00200 | Train Loss 0.6177 | Val precision 38.7409% | Val recall 78.4314% | Val F1 51.8639%\n",
      "Epoch 00300 | Train Loss 0.6067 | Val precision 44.8571% | Val recall 76.9608% | Val F1 56.6787%\n",
      "Epoch 00400 | Train Loss 0.6016 | Val precision 44.6429% | Val recall 73.5294% | Val F1 55.5556%\n",
      "Epoch 00500 | Train Loss 0.5990 | Val precision 44.4444% | Val recall 76.4706% | Val F1 56.2162%\n",
      "Epoch 00600 | Train Loss 0.6009 | Val precision 44.8916% | Val recall 71.0784% | Val F1 55.0285%\n",
      "Epoch 00700 | Train Loss 0.5976 | Val precision 45.9375% | Val recall 72.0588% | Val F1 56.1069%\n",
      "Epoch 00800 | Train Loss 0.5977 | Val precision 42.7441% | Val recall 79.4118% | Val F1 55.5746%\n",
      "Epoch 00900 | Train Loss 0.6011 | Val precision 45.2012% | Val recall 71.5686% | Val F1 55.4080%\n",
      "Epoch 01000 | Train Loss 0.5991 | Val precision 45.3642% | Val recall 67.1569% | Val F1 54.1502%\n",
      "Epoch 00100 | Train Loss 0.6857 | Val precision 30.5317% | Val recall 87.2549% | Val F1 45.2351%\n",
      "Epoch 00200 | Train Loss 0.6656 | Val precision 32.3529% | Val recall 91.6667% | Val F1 47.8261%\n",
      "Epoch 00300 | Train Loss 0.6118 | Val precision 44.6602% | Val recall 67.6471% | Val F1 53.8012%\n",
      "Epoch 00400 | Train Loss 0.6072 | Val precision 43.4921% | Val recall 67.1569% | Val F1 52.7938%\n",
      "Epoch 00500 | Train Loss 0.6012 | Val precision 44.2815% | Val recall 74.0196% | Val F1 55.4128%\n",
      "Epoch 00600 | Train Loss 0.6055 | Val precision 44.6154% | Val recall 71.0784% | Val F1 54.8204%\n",
      "Epoch 00700 | Train Loss 0.6003 | Val precision 44.4840% | Val recall 61.2745% | Val F1 51.5464%\n",
      "Epoch 00800 | Train Loss 0.6004 | Val precision 43.9655% | Val recall 75.0000% | Val F1 55.4348%\n",
      "Epoch 00900 | Train Loss 0.6044 | Val precision 45.3376% | Val recall 69.1176% | Val F1 54.7573%\n",
      "Epoch 01000 | Train Loss 0.6046 | Val precision 44.6875% | Val recall 70.0980% | Val F1 54.5802%\n",
      "Epoch 00100 | Train Loss 0.6899 | Val precision 28.4058% | Val recall 96.0784% | Val F1 43.8479%\n",
      "Epoch 00200 | Train Loss 0.6181 | Val precision 45.4829% | Val recall 71.5686% | Val F1 55.6190%\n",
      "Epoch 00300 | Train Loss 0.6120 | Val precision 45.2012% | Val recall 71.5686% | Val F1 55.4080%\n",
      "Epoch 00400 | Train Loss 0.6072 | Val precision 43.7500% | Val recall 72.0588% | Val F1 54.4444%\n",
      "Epoch 00500 | Train Loss 0.6034 | Val precision 44.5141% | Val recall 69.6078% | Val F1 54.3021%\n",
      "Epoch 00600 | Train Loss 0.6069 | Val precision 45.0704% | Val recall 62.7451% | Val F1 52.4590%\n",
      "Epoch 00700 | Train Loss 0.6012 | Val precision 43.7500% | Val recall 68.6275% | Val F1 53.4351%\n",
      "Epoch 00800 | Train Loss 0.6058 | Val precision 38.6517% | Val recall 84.3137% | Val F1 53.0046%\n",
      "Epoch 00900 | Train Loss 0.6098 | Val precision 42.2872% | Val recall 77.9412% | Val F1 54.8276%\n",
      "Epoch 01000 | Train Loss 0.6067 | Val precision 45.0355% | Val recall 62.2549% | Val F1 52.2634%\n",
      "Epoch 00100 | Train Loss 0.6854 | Val precision 30.8901% | Val recall 86.7647% | Val F1 45.5598%\n",
      "Epoch 00200 | Train Loss 0.6652 | Val precision 31.8792% | Val recall 93.1373% | Val F1 47.5000%\n",
      "Epoch 00300 | Train Loss 0.6116 | Val precision 44.9180% | Val recall 67.1569% | Val F1 53.8310%\n",
      "Epoch 00400 | Train Loss 0.6065 | Val precision 43.9103% | Val recall 67.1569% | Val F1 53.1008%\n",
      "Epoch 00500 | Train Loss 0.6021 | Val precision 44.3730% | Val recall 67.6471% | Val F1 53.5922%\n",
      "Epoch 00600 | Train Loss 0.6058 | Val precision 44.6602% | Val recall 67.6471% | Val F1 53.8012%\n",
      "Epoch 00700 | Train Loss 0.6008 | Val precision 43.7326% | Val recall 76.9608% | Val F1 55.7726%\n",
      "Epoch 00800 | Train Loss 0.6051 | Val precision 44.6203% | Val recall 69.1176% | Val F1 54.2308%\n",
      "Epoch 00900 | Train Loss 0.6056 | Val precision 44.7368% | Val recall 66.6667% | Val F1 53.5433%\n",
      "Epoch 01000 | Train Loss 0.6054 | Val precision 45.2962% | Val recall 63.7255% | Val F1 52.9532%\n",
      "Epoch 00100 | Train Loss 0.6818 | Val precision 30.3318% | Val recall 94.1176% | Val F1 45.8781%\n",
      "Epoch 00200 | Train Loss 0.6160 | Val precision 43.1818% | Val recall 74.5098% | Val F1 54.6763%\n",
      "Epoch 00300 | Train Loss 0.6140 | Val precision 43.3022% | Val recall 68.1373% | Val F1 52.9524%\n",
      "Epoch 00400 | Train Loss 0.6087 | Val precision 42.8954% | Val recall 78.4314% | Val F1 55.4593%\n",
      "Epoch 00500 | Train Loss 0.6128 | Val precision 42.5982% | Val recall 69.1176% | Val F1 52.7103%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00600 | Train Loss 0.6113 | Val precision 42.9003% | Val recall 69.6078% | Val F1 53.0841%\n",
      "Epoch 00700 | Train Loss 0.6074 | Val precision 43.3437% | Val recall 68.6275% | Val F1 53.1309%\n",
      "Epoch 00800 | Train Loss 0.6073 | Val precision 42.3295% | Val recall 73.0392% | Val F1 53.5971%\n",
      "Epoch 00900 | Train Loss 0.6141 | Val precision 42.6593% | Val recall 75.4902% | Val F1 54.5133%\n",
      "Epoch 01000 | Train Loss 0.6089 | Val precision 43.1746% | Val recall 66.6667% | Val F1 52.4085%\n",
      "Epoch 00100 | Train Loss 0.6865 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00200 | Train Loss 0.6106 | Val precision 43.6047% | Val recall 73.5294% | Val F1 54.7445%\n",
      "Epoch 00300 | Train Loss 0.6151 | Val precision 42.9825% | Val recall 72.0588% | Val F1 53.8462%\n",
      "Epoch 00400 | Train Loss 0.6084 | Val precision 44.5902% | Val recall 66.6667% | Val F1 53.4381%\n",
      "Epoch 00500 | Train Loss 0.6129 | Val precision 42.8962% | Val recall 76.9608% | Val F1 55.0877%\n",
      "Epoch 00600 | Train Loss 0.6112 | Val precision 42.9078% | Val recall 59.3137% | Val F1 49.7942%\n",
      "Epoch 00700 | Train Loss 0.6108 | Val precision 40.2381% | Val recall 82.8431% | Val F1 54.1667%\n",
      "Epoch 00800 | Train Loss 0.6184 | Val precision 43.6667% | Val recall 64.2157% | Val F1 51.9841%\n",
      "Epoch 00900 | Train Loss 0.6122 | Val precision 42.8571% | Val recall 69.1176% | Val F1 52.9081%\n",
      "Epoch 01000 | Train Loss 0.6225 | Val precision 42.8571% | Val recall 77.9412% | Val F1 55.3043%\n",
      "Epoch 00100 | Train Loss 0.6811 | Val precision 29.9225% | Val recall 94.6078% | Val F1 45.4653%\n",
      "Epoch 00200 | Train Loss 0.6158 | Val precision 43.4286% | Val recall 74.5098% | Val F1 54.8736%\n",
      "Epoch 00300 | Train Loss 0.6141 | Val precision 43.3022% | Val recall 68.1373% | Val F1 52.9524%\n",
      "Epoch 00400 | Train Loss 0.6087 | Val precision 43.0556% | Val recall 75.9804% | Val F1 54.9645%\n",
      "Epoch 00500 | Train Loss 0.6085 | Val precision 42.7807% | Val recall 78.4314% | Val F1 55.3633%\n",
      "Epoch 00600 | Train Loss 0.6121 | Val precision 43.6137% | Val recall 68.6275% | Val F1 53.3333%\n",
      "Epoch 00700 | Train Loss 0.6078 | Val precision 43.3692% | Val recall 59.3137% | Val F1 50.1035%\n",
      "Epoch 00800 | Train Loss 0.6063 | Val precision 43.5262% | Val recall 77.4510% | Val F1 55.7319%\n",
      "Epoch 00900 | Train Loss 0.6137 | Val precision 44.5902% | Val recall 66.6667% | Val F1 53.4381%\n",
      "Epoch 01000 | Train Loss 0.6096 | Val precision 42.6426% | Val recall 69.6078% | Val F1 52.8864%\n",
      "Epoch 00100 | Train Loss 0.6086 | Val precision 39.3643% | Val recall 78.9216% | Val F1 52.5285%\n",
      "Epoch 00200 | Train Loss 0.6003 | Val precision 44.8916% | Val recall 71.0784% | Val F1 55.0285%\n",
      "Epoch 00300 | Train Loss 0.6023 | Val precision 44.1595% | Val recall 75.9804% | Val F1 55.8559%\n",
      "Epoch 00400 | Train Loss 0.6001 | Val precision 44.7761% | Val recall 73.5294% | Val F1 55.6586%\n",
      "Epoch 00500 | Train Loss 0.6054 | Val precision 44.8171% | Val recall 72.0588% | Val F1 55.2632%\n",
      "Epoch 00600 | Train Loss 0.5978 | Val precision 45.1807% | Val recall 73.5294% | Val F1 55.9701%\n",
      "Epoch 00700 | Train Loss 0.6022 | Val precision 44.5402% | Val recall 75.9804% | Val F1 56.1594%\n",
      "Epoch 00800 | Train Loss 0.5990 | Val precision 44.8980% | Val recall 75.4902% | Val F1 56.3071%\n",
      "Epoch 00900 | Train Loss 0.6031 | Val precision 43.1694% | Val recall 77.4510% | Val F1 55.4386%\n",
      "Epoch 01000 | Train Loss 0.5987 | Val precision 44.7205% | Val recall 70.5882% | Val F1 54.7529%\n",
      "Epoch 00100 | Train Loss 0.6093 | Val precision 39.4161% | Val recall 79.4118% | Val F1 52.6829%\n",
      "Epoch 00200 | Train Loss 0.6009 | Val precision 44.5104% | Val recall 73.5294% | Val F1 55.4529%\n",
      "Epoch 00300 | Train Loss 0.6030 | Val precision 44.7977% | Val recall 75.9804% | Val F1 56.3636%\n",
      "Epoch 00400 | Train Loss 0.6038 | Val precision 42.5197% | Val recall 79.4118% | Val F1 55.3846%\n",
      "Epoch 00500 | Train Loss 0.6049 | Val precision 44.9541% | Val recall 72.0588% | Val F1 55.3672%\n",
      "Epoch 00600 | Train Loss 0.6008 | Val precision 41.4141% | Val recall 80.3922% | Val F1 54.6667%\n",
      "Epoch 00700 | Train Loss 0.6093 | Val precision 45.6250% | Val recall 71.5686% | Val F1 55.7252%\n",
      "Epoch 00800 | Train Loss 0.6124 | Val precision 36.5657% | Val recall 88.7255% | Val F1 51.7883%\n",
      "Epoch 00900 | Train Loss 0.6026 | Val precision 43.4174% | Val recall 75.9804% | Val F1 55.2585%\n",
      "Epoch 01000 | Train Loss 0.6023 | Val precision 44.9568% | Val recall 76.4706% | Val F1 56.6243%\n",
      "Epoch 00100 | Train Loss 0.6079 | Val precision 40.3646% | Val recall 75.9804% | Val F1 52.7211%\n",
      "Epoch 00200 | Train Loss 0.6003 | Val precision 44.8916% | Val recall 71.0784% | Val F1 55.0285%\n",
      "Epoch 00300 | Train Loss 0.6026 | Val precision 45.2096% | Val recall 74.0196% | Val F1 56.1338%\n",
      "Epoch 00400 | Train Loss 0.6002 | Val precision 45.2308% | Val recall 72.0588% | Val F1 55.5766%\n",
      "Epoch 00500 | Train Loss 0.6046 | Val precision 45.3416% | Val recall 71.5686% | Val F1 55.5133%\n",
      "Epoch 00600 | Train Loss 0.5974 | Val precision 45.3453% | Val recall 74.0196% | Val F1 56.2384%\n",
      "Epoch 00700 | Train Loss 0.6016 | Val precision 43.3243% | Val recall 77.9412% | Val F1 55.6918%\n",
      "Epoch 00800 | Train Loss 0.5989 | Val precision 45.0292% | Val recall 75.4902% | Val F1 56.4103%\n",
      "Epoch 00900 | Train Loss 0.6020 | Val precision 45.2229% | Val recall 69.6078% | Val F1 54.8263%\n",
      "Epoch 01000 | Train Loss 0.5981 | Val precision 44.1667% | Val recall 77.9412% | Val F1 56.3830%\n",
      "Epoch 00100 | Train Loss 0.6319 | Val precision 32.7986% | Val recall 90.1961% | Val F1 48.1046%\n",
      "Epoch 00200 | Train Loss 0.6094 | Val precision 44.6945% | Val recall 68.1373% | Val F1 53.9806%\n",
      "Epoch 00300 | Train Loss 0.6111 | Val precision 45.0000% | Val recall 75.0000% | Val F1 56.2500%\n",
      "Epoch 00400 | Train Loss 0.6035 | Val precision 43.5821% | Val recall 71.5686% | Val F1 54.1744%\n",
      "Epoch 00500 | Train Loss 0.6115 | Val precision 44.6108% | Val recall 73.0392% | Val F1 55.3903%\n",
      "Epoch 00600 | Train Loss 0.6052 | Val precision 44.2136% | Val recall 73.0392% | Val F1 55.0832%\n",
      "Epoch 00700 | Train Loss 0.6041 | Val precision 43.5897% | Val recall 75.0000% | Val F1 55.1351%\n",
      "Epoch 00800 | Train Loss 0.6032 | Val precision 43.2961% | Val recall 75.9804% | Val F1 55.1601%\n",
      "Epoch 00900 | Train Loss 0.6066 | Val precision 43.9306% | Val recall 74.5098% | Val F1 55.2727%\n",
      "Epoch 01000 | Train Loss 0.6045 | Val precision 43.7500% | Val recall 75.4902% | Val F1 55.3957%\n",
      "Epoch 00100 | Train Loss 0.6776 | Val precision 30.6090% | Val recall 93.6275% | Val F1 46.1353%\n",
      "Epoch 00200 | Train Loss 0.6253 | Val precision 43.6950% | Val recall 73.0392% | Val F1 54.6789%\n",
      "Epoch 00300 | Train Loss 0.6116 | Val precision 43.8040% | Val recall 74.5098% | Val F1 55.1724%\n",
      "Epoch 00400 | Train Loss 0.6048 | Val precision 44.3077% | Val recall 70.5882% | Val F1 54.4423%\n",
      "Epoch 00500 | Train Loss 0.6115 | Val precision 43.5262% | Val recall 77.4510% | Val F1 55.7319%\n",
      "Epoch 00600 | Train Loss 0.6099 | Val precision 42.7807% | Val recall 78.4314% | Val F1 55.3633%\n",
      "Epoch 00700 | Train Loss 0.6059 | Val precision 41.3882% | Val recall 78.9216% | Val F1 54.3002%\n",
      "Epoch 00800 | Train Loss 0.6050 | Val precision 43.7500% | Val recall 75.4902% | Val F1 55.3957%\n",
      "Epoch 00900 | Train Loss 0.6084 | Val precision 43.0108% | Val recall 78.4314% | Val F1 55.5556%\n",
      "Epoch 01000 | Train Loss 0.6059 | Val precision 43.8040% | Val recall 74.5098% | Val F1 55.1724%\n",
      "Epoch 00100 | Train Loss 0.6409 | Val precision 31.8681% | Val recall 85.2941% | Val F1 46.4000%\n",
      "Epoch 00200 | Train Loss 0.6092 | Val precision 44.1270% | Val recall 68.1373% | Val F1 53.5645%\n",
      "Epoch 00300 | Train Loss 0.6114 | Val precision 44.7059% | Val recall 74.5098% | Val F1 55.8824%\n",
      "Epoch 00400 | Train Loss 0.6038 | Val precision 43.7690% | Val recall 70.5882% | Val F1 54.0338%\n",
      "Epoch 00500 | Train Loss 0.6120 | Val precision 43.9103% | Val recall 67.1569% | Val F1 53.1008%\n",
      "Epoch 00600 | Train Loss 0.6051 | Val precision 44.4776% | Val recall 73.0392% | Val F1 55.2876%\n",
      "Epoch 00700 | Train Loss 0.6044 | Val precision 43.7681% | Val recall 74.0196% | Val F1 55.0091%\n",
      "Epoch 00800 | Train Loss 0.6037 | Val precision 44.7761% | Val recall 73.5294% | Val F1 55.6586%\n",
      "Epoch 00900 | Train Loss 0.6089 | Val precision 43.3702% | Val recall 76.9608% | Val F1 55.4770%\n",
      "Epoch 01000 | Train Loss 0.6067 | Val precision 43.9306% | Val recall 74.5098% | Val F1 55.2727%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00100 | Train Loss 0.6083 | Val precision 44.3769% | Val recall 71.5686% | Val F1 54.7842%\n",
      "Epoch 00200 | Train Loss 0.6125 | Val precision 44.2997% | Val recall 66.6667% | Val F1 53.2290%\n",
      "Epoch 00300 | Train Loss 0.6150 | Val precision 42.0103% | Val recall 79.9020% | Val F1 55.0676%\n",
      "Epoch 00400 | Train Loss 0.6077 | Val precision 43.3962% | Val recall 67.6471% | Val F1 52.8736%\n",
      "Epoch 00500 | Train Loss 0.6171 | Val precision 43.4921% | Val recall 67.1569% | Val F1 52.7938%\n",
      "Epoch 00600 | Train Loss 0.6084 | Val precision 43.3657% | Val recall 65.6863% | Val F1 52.2417%\n",
      "Epoch 00700 | Train Loss 0.6111 | Val precision 42.8977% | Val recall 74.0196% | Val F1 54.3165%\n",
      "Epoch 00800 | Train Loss 0.6104 | Val precision 43.3803% | Val recall 75.4902% | Val F1 55.0984%\n",
      "Epoch 00900 | Train Loss 0.6101 | Val precision 42.9967% | Val recall 64.7059% | Val F1 51.6634%\n",
      "Epoch 01000 | Train Loss 0.6096 | Val precision 43.5897% | Val recall 66.6667% | Val F1 52.7132%\n",
      "Epoch 00100 | Train Loss 0.6092 | Val precision 44.9231% | Val recall 71.5686% | Val F1 55.1985%\n",
      "Epoch 00200 | Train Loss 0.6130 | Val precision 43.0769% | Val recall 68.6275% | Val F1 52.9301%\n",
      "Epoch 00300 | Train Loss 0.6152 | Val precision 43.3908% | Val recall 74.0196% | Val F1 54.7101%\n",
      "Epoch 00400 | Train Loss 0.6112 | Val precision 43.6893% | Val recall 66.1765% | Val F1 52.6316%\n",
      "Epoch 00500 | Train Loss 0.6171 | Val precision 44.2136% | Val recall 73.0392% | Val F1 55.0832%\n",
      "Epoch 00600 | Train Loss 0.6101 | Val precision 42.5714% | Val recall 73.0392% | Val F1 53.7906%\n",
      "Epoch 00700 | Train Loss 0.6149 | Val precision 40.6484% | Val recall 79.9020% | Val F1 53.8843%\n",
      "Epoch 00800 | Train Loss 0.6157 | Val precision 37.1002% | Val recall 85.2941% | Val F1 51.7088%\n",
      "Epoch 00900 | Train Loss 0.6136 | Val precision 43.7037% | Val recall 57.8431% | Val F1 49.7890%\n",
      "Epoch 01000 | Train Loss 0.6113 | Val precision 43.6709% | Val recall 67.6471% | Val F1 53.0769%\n",
      "Epoch 00100 | Train Loss 0.6083 | Val precision 44.3769% | Val recall 71.5686% | Val F1 54.7842%\n",
      "Epoch 00200 | Train Loss 0.6126 | Val precision 44.1558% | Val recall 66.6667% | Val F1 53.1250%\n",
      "Epoch 00300 | Train Loss 0.6168 | Val precision 42.3913% | Val recall 76.4706% | Val F1 54.5455%\n",
      "Epoch 00400 | Train Loss 0.6079 | Val precision 43.1319% | Val recall 76.9608% | Val F1 55.2817%\n",
      "Epoch 00500 | Train Loss 0.6168 | Val precision 43.8776% | Val recall 63.2353% | Val F1 51.8072%\n",
      "Epoch 00600 | Train Loss 0.6084 | Val precision 43.9103% | Val recall 67.1569% | Val F1 53.1008%\n",
      "Epoch 00700 | Train Loss 0.6102 | Val precision 43.1818% | Val recall 74.5098% | Val F1 54.6763%\n",
      "Epoch 00800 | Train Loss 0.6112 | Val precision 43.9528% | Val recall 73.0392% | Val F1 54.8803%\n",
      "Epoch 00900 | Train Loss 0.6109 | Val precision 44.0252% | Val recall 68.6275% | Val F1 53.6398%\n",
      "Epoch 01000 | Train Loss 0.6158 | Val precision 43.6047% | Val recall 73.5294% | Val F1 54.7445%\n",
      "Epoch 00100 | Train Loss 0.6113 | Val precision 42.1053% | Val recall 78.4314% | Val F1 54.7945%\n",
      "Epoch 00200 | Train Loss 0.5994 | Val precision 44.0341% | Val recall 75.9804% | Val F1 55.7554%\n",
      "Epoch 00300 | Train Loss 0.6065 | Val precision 44.5783% | Val recall 72.5490% | Val F1 55.2239%\n",
      "Epoch 00400 | Train Loss 0.6016 | Val precision 43.1694% | Val recall 77.4510% | Val F1 55.4386%\n",
      "Epoch 00500 | Train Loss 0.5982 | Val precision 44.1667% | Val recall 77.9412% | Val F1 56.3830%\n",
      "Epoch 00600 | Train Loss 0.6002 | Val precision 44.8916% | Val recall 71.0784% | Val F1 55.0285%\n",
      "Epoch 00700 | Train Loss 0.5981 | Val precision 45.0311% | Val recall 71.0784% | Val F1 55.1331%\n",
      "Epoch 00800 | Train Loss 0.5985 | Val precision 41.6452% | Val recall 79.4118% | Val F1 54.6374%\n",
      "Epoch 00900 | Train Loss 0.6005 | Val precision 44.0828% | Val recall 73.0392% | Val F1 54.9815%\n",
      "Epoch 01000 | Train Loss 0.5981 | Val precision 46.3333% | Val recall 68.1373% | Val F1 55.1587%\n",
      "Epoch 00100 | Train Loss 0.6106 | Val precision 44.6746% | Val recall 74.0196% | Val F1 55.7196%\n",
      "Epoch 00200 | Train Loss 0.6010 | Val precision 44.5087% | Val recall 75.4902% | Val F1 56.0000%\n",
      "Epoch 00300 | Train Loss 0.6057 | Val precision 43.5135% | Val recall 78.9216% | Val F1 56.0976%\n",
      "Epoch 00400 | Train Loss 0.6014 | Val precision 44.4126% | Val recall 75.9804% | Val F1 56.0579%\n",
      "Epoch 00500 | Train Loss 0.5987 | Val precision 44.7293% | Val recall 76.9608% | Val F1 56.5766%\n",
      "Epoch 00600 | Train Loss 0.6012 | Val precision 43.2065% | Val recall 77.9412% | Val F1 55.5944%\n",
      "Epoch 00700 | Train Loss 0.5983 | Val precision 45.3988% | Val recall 72.5490% | Val F1 55.8491%\n",
      "Epoch 00800 | Train Loss 0.5984 | Val precision 45.1039% | Val recall 74.5098% | Val F1 56.1922%\n",
      "Epoch 00900 | Train Loss 0.6036 | Val precision 43.9560% | Val recall 78.4314% | Val F1 56.3380%\n",
      "Epoch 01000 | Train Loss 0.6052 | Val precision 37.4220% | Val recall 88.2353% | Val F1 52.5547%\n",
      "Epoch 00100 | Train Loss 0.6110 | Val precision 42.4324% | Val recall 76.9608% | Val F1 54.7038%\n",
      "Epoch 00200 | Train Loss 0.5995 | Val precision 43.5393% | Val recall 75.9804% | Val F1 55.3571%\n",
      "Epoch 00300 | Train Loss 0.6068 | Val precision 44.3804% | Val recall 75.4902% | Val F1 55.8984%\n",
      "Epoch 00400 | Train Loss 0.6009 | Val precision 43.6464% | Val recall 77.4510% | Val F1 55.8304%\n",
      "Epoch 00500 | Train Loss 0.5980 | Val precision 44.7977% | Val recall 75.9804% | Val F1 56.3636%\n",
      "Epoch 00600 | Train Loss 0.6013 | Val precision 45.6311% | Val recall 69.1176% | Val F1 54.9708%\n",
      "Epoch 00700 | Train Loss 0.5976 | Val precision 45.2532% | Val recall 70.0980% | Val F1 55.0000%\n",
      "Epoch 00800 | Train Loss 0.5970 | Val precision 41.7722% | Val recall 80.8824% | Val F1 55.0918%\n",
      "Epoch 00900 | Train Loss 0.6016 | Val precision 44.8171% | Val recall 72.0588% | Val F1 55.2632%\n",
      "Epoch 01000 | Train Loss 0.5985 | Val precision 45.5738% | Val recall 68.1373% | Val F1 54.6169%\n",
      "Epoch 00100 | Train Loss 0.6165 | Val precision 45.1220% | Val recall 72.5490% | Val F1 55.6391%\n",
      "Epoch 00200 | Train Loss 0.6055 | Val precision 44.5860% | Val recall 68.6275% | Val F1 54.0541%\n",
      "Epoch 00300 | Train Loss 0.6103 | Val precision 42.9012% | Val recall 68.1373% | Val F1 52.6515%\n",
      "Epoch 00400 | Train Loss 0.6091 | Val precision 44.0514% | Val recall 67.1569% | Val F1 53.2039%\n",
      "Epoch 00500 | Train Loss 0.6037 | Val precision 44.5104% | Val recall 73.5294% | Val F1 55.4529%\n",
      "Epoch 00600 | Train Loss 0.6060 | Val precision 43.3526% | Val recall 73.5294% | Val F1 54.5455%\n",
      "Epoch 00700 | Train Loss 0.6024 | Val precision 43.9114% | Val recall 58.3333% | Val F1 50.1053%\n",
      "Epoch 00800 | Train Loss 0.6000 | Val precision 43.8746% | Val recall 75.4902% | Val F1 55.4955%\n",
      "Epoch 00900 | Train Loss 0.6062 | Val precision 43.8202% | Val recall 76.4706% | Val F1 55.7143%\n",
      "Epoch 01000 | Train Loss 0.6045 | Val precision 45.2145% | Val recall 67.1569% | Val F1 54.0434%\n",
      "Epoch 00100 | Train Loss 0.6169 | Val precision 44.8795% | Val recall 73.0392% | Val F1 55.5970%\n",
      "Epoch 00200 | Train Loss 0.6066 | Val precision 44.2136% | Val recall 73.0392% | Val F1 55.0832%\n",
      "Epoch 00300 | Train Loss 0.6110 | Val precision 42.6273% | Val recall 77.9412% | Val F1 55.1127%\n",
      "Epoch 00400 | Train Loss 0.6073 | Val precision 44.0252% | Val recall 68.6275% | Val F1 53.6398%\n",
      "Epoch 00500 | Train Loss 0.6018 | Val precision 42.5876% | Val recall 77.4510% | Val F1 54.9565%\n",
      "Epoch 00600 | Train Loss 0.6095 | Val precision 45.8781% | Val recall 62.7451% | Val F1 53.0021%\n",
      "Epoch 00700 | Train Loss 0.6015 | Val precision 43.7500% | Val recall 68.6275% | Val F1 53.4351%\n",
      "Epoch 00800 | Train Loss 0.6017 | Val precision 39.0411% | Val recall 83.8235% | Val F1 53.2710%\n",
      "Epoch 00900 | Train Loss 0.6053 | Val precision 44.4444% | Val recall 58.8235% | Val F1 50.6329%\n",
      "Epoch 01000 | Train Loss 0.6204 | Val precision 39.5402% | Val recall 84.3137% | Val F1 53.8341%\n",
      "Epoch 00100 | Train Loss 0.6165 | Val precision 45.1515% | Val recall 73.0392% | Val F1 55.8052%\n",
      "Epoch 00200 | Train Loss 0.6061 | Val precision 44.4089% | Val recall 68.1373% | Val F1 53.7718%\n",
      "Epoch 00300 | Train Loss 0.6109 | Val precision 42.9448% | Val recall 68.6275% | Val F1 52.8302%\n",
      "Epoch 00400 | Train Loss 0.6079 | Val precision 42.9348% | Val recall 77.4510% | Val F1 55.2448%\n",
      "Epoch 00500 | Train Loss 0.6050 | Val precision 44.3787% | Val recall 73.5294% | Val F1 55.3506%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00600 | Train Loss 0.6063 | Val precision 43.8095% | Val recall 67.6471% | Val F1 53.1792%\n",
      "Epoch 00700 | Train Loss 0.6028 | Val precision 42.7419% | Val recall 77.9412% | Val F1 55.2083%\n",
      "Epoch 00800 | Train Loss 0.6013 | Val precision 44.7853% | Val recall 71.5686% | Val F1 55.0943%\n",
      "Epoch 00900 | Train Loss 0.6060 | Val precision 44.7531% | Val recall 71.0784% | Val F1 54.9242%\n",
      "Epoch 01000 | Train Loss 0.6063 | Val precision 45.4259% | Val recall 70.5882% | Val F1 55.2783%\n",
      "Epoch 00100 | Train Loss 0.6243 | Val precision 41.4698% | Val recall 77.4510% | Val F1 54.0171%\n",
      "Epoch 00200 | Train Loss 0.6106 | Val precision 41.4322% | Val recall 79.4118% | Val F1 54.4538%\n",
      "Epoch 00300 | Train Loss 0.6136 | Val precision 42.7273% | Val recall 69.1176% | Val F1 52.8090%\n",
      "Epoch 00400 | Train Loss 0.6073 | Val precision 43.6709% | Val recall 67.6471% | Val F1 53.0769%\n",
      "Epoch 00500 | Train Loss 0.6083 | Val precision 44.2953% | Val recall 64.7059% | Val F1 52.5896%\n",
      "Epoch 00600 | Train Loss 0.6156 | Val precision 42.6380% | Val recall 68.1373% | Val F1 52.4528%\n",
      "Epoch 00700 | Train Loss 0.6102 | Val precision 43.0000% | Val recall 63.2353% | Val F1 51.1905%\n",
      "Epoch 00800 | Train Loss 0.6052 | Val precision 42.8169% | Val recall 74.5098% | Val F1 54.3828%\n",
      "Epoch 00900 | Train Loss 0.6155 | Val precision 42.8571% | Val recall 75.0000% | Val F1 54.5455%\n",
      "Epoch 01000 | Train Loss 0.6085 | Val precision 42.1365% | Val recall 69.6078% | Val F1 52.4954%\n",
      "Epoch 00100 | Train Loss 0.6252 | Val precision 41.7582% | Val recall 74.5098% | Val F1 53.5211%\n",
      "Epoch 00200 | Train Loss 0.6094 | Val precision 44.7876% | Val recall 56.8627% | Val F1 50.1080%\n",
      "Epoch 00300 | Train Loss 0.6169 | Val precision 42.8962% | Val recall 76.9608% | Val F1 55.0877%\n",
      "Epoch 00400 | Train Loss 0.6080 | Val precision 42.1365% | Val recall 69.6078% | Val F1 52.4954%\n",
      "Epoch 00500 | Train Loss 0.6154 | Val precision 40.7960% | Val recall 80.3922% | Val F1 54.1254%\n",
      "Epoch 00600 | Train Loss 0.6126 | Val precision 45.4237% | Val recall 65.6863% | Val F1 53.7074%\n",
      "Epoch 00700 | Train Loss 0.6084 | Val precision 43.8095% | Val recall 67.6471% | Val F1 53.1792%\n",
      "Epoch 00800 | Train Loss 0.6160 | Val precision 43.0939% | Val recall 76.4706% | Val F1 55.1237%\n",
      "Epoch 00900 | Train Loss 0.6124 | Val precision 44.2748% | Val recall 56.8627% | Val F1 49.7854%\n",
      "Epoch 01000 | Train Loss 0.6196 | Val precision 43.2056% | Val recall 60.7843% | Val F1 50.5092%\n",
      "Epoch 00100 | Train Loss 0.6244 | Val precision 41.4248% | Val recall 76.9608% | Val F1 53.8593%\n",
      "Epoch 00200 | Train Loss 0.6147 | Val precision 44.3452% | Val recall 73.0392% | Val F1 55.1852%\n",
      "Epoch 00300 | Train Loss 0.6139 | Val precision 42.8571% | Val recall 70.5882% | Val F1 53.3333%\n",
      "Epoch 00400 | Train Loss 0.6076 | Val precision 42.9412% | Val recall 71.5686% | Val F1 53.6765%\n",
      "Epoch 00500 | Train Loss 0.6101 | Val precision 43.1085% | Val recall 72.0588% | Val F1 53.9450%\n",
      "Epoch 00600 | Train Loss 0.6118 | Val precision 43.8849% | Val recall 59.8039% | Val F1 50.6224%\n",
      "Epoch 00700 | Train Loss 0.6115 | Val precision 42.5876% | Val recall 77.4510% | Val F1 54.9565%\n",
      "Epoch 00800 | Train Loss 0.6053 | Val precision 44.1791% | Val recall 72.5490% | Val F1 54.9165%\n",
      "Epoch 00900 | Train Loss 0.6161 | Val precision 45.2055% | Val recall 64.7059% | Val F1 53.2258%\n",
      "Epoch 01000 | Train Loss 0.6091 | Val precision 43.5252% | Val recall 59.3137% | Val F1 50.2075%\n",
      "Epoch 00100 | Train Loss 0.6687 | Val precision 32.1976% | Val recall 92.6471% | Val F1 47.7876%\n",
      "Epoch 00200 | Train Loss 0.6012 | Val precision 43.6975% | Val recall 76.4706% | Val F1 55.6150%\n",
      "Epoch 00300 | Train Loss 0.6067 | Val precision 44.5783% | Val recall 72.5490% | Val F1 55.2239%\n",
      "Epoch 00400 | Train Loss 0.6020 | Val precision 43.0108% | Val recall 78.4314% | Val F1 55.5556%\n",
      "Epoch 00500 | Train Loss 0.5972 | Val precision 44.4751% | Val recall 78.9216% | Val F1 56.8905%\n",
      "Epoch 00600 | Train Loss 0.6010 | Val precision 45.2012% | Val recall 71.5686% | Val F1 55.4080%\n",
      "Epoch 00700 | Train Loss 0.5981 | Val precision 45.0920% | Val recall 72.0588% | Val F1 55.4717%\n",
      "Epoch 00800 | Train Loss 0.5976 | Val precision 44.5402% | Val recall 75.9804% | Val F1 56.1594%\n",
      "Epoch 00900 | Train Loss 0.6009 | Val precision 45.1411% | Val recall 70.5882% | Val F1 55.0669%\n",
      "Epoch 01000 | Train Loss 0.5983 | Val precision 46.0784% | Val recall 69.1176% | Val F1 55.2941%\n",
      "Epoch 00100 | Train Loss 0.6685 | Val precision 31.9257% | Val recall 92.6471% | Val F1 47.4874%\n",
      "Epoch 00200 | Train Loss 0.6128 | Val precision 38.7097% | Val recall 82.3529% | Val F1 52.6646%\n",
      "Epoch 00300 | Train Loss 0.6055 | Val precision 45.1104% | Val recall 70.0980% | Val F1 54.8944%\n",
      "Epoch 00400 | Train Loss 0.6016 | Val precision 44.3478% | Val recall 75.0000% | Val F1 55.7377%\n",
      "Epoch 00500 | Train Loss 0.5981 | Val precision 44.1989% | Val recall 78.4314% | Val F1 56.5371%\n",
      "Epoch 00600 | Train Loss 0.6022 | Val precision 46.3333% | Val recall 68.1373% | Val F1 55.1587%\n",
      "Epoch 00700 | Train Loss 0.5996 | Val precision 45.5657% | Val recall 73.0392% | Val F1 56.1205%\n",
      "Epoch 00800 | Train Loss 0.5982 | Val precision 42.4802% | Val recall 78.9216% | Val F1 55.2316%\n",
      "Epoch 00900 | Train Loss 0.5991 | Val precision 46.1538% | Val recall 64.7059% | Val F1 53.8776%\n",
      "Epoch 01000 | Train Loss 0.6003 | Val precision 37.8319% | Val recall 83.8235% | Val F1 52.1341%\n",
      "Epoch 00100 | Train Loss 0.6687 | Val precision 32.1429% | Val recall 92.6471% | Val F1 47.7273%\n",
      "Epoch 00200 | Train Loss 0.6017 | Val precision 44.2857% | Val recall 75.9804% | Val F1 55.9567%\n",
      "Epoch 00300 | Train Loss 0.6069 | Val precision 44.2857% | Val recall 75.9804% | Val F1 55.9567%\n",
      "Epoch 00400 | Train Loss 0.6019 | Val precision 43.2432% | Val recall 78.4314% | Val F1 55.7491%\n",
      "Epoch 00500 | Train Loss 0.5978 | Val precision 44.7674% | Val recall 75.4902% | Val F1 56.2044%\n",
      "Epoch 00600 | Train Loss 0.6012 | Val precision 45.6869% | Val recall 70.0980% | Val F1 55.3191%\n",
      "Epoch 00700 | Train Loss 0.5983 | Val precision 44.6483% | Val recall 71.5686% | Val F1 54.9906%\n",
      "Epoch 00800 | Train Loss 0.5972 | Val precision 41.8782% | Val recall 80.8824% | Val F1 55.1839%\n",
      "Epoch 00900 | Train Loss 0.6037 | Val precision 46.8966% | Val recall 66.6667% | Val F1 55.0607%\n",
      "Epoch 01000 | Train Loss 0.5989 | Val precision 45.7317% | Val recall 73.5294% | Val F1 56.3910%\n",
      "Epoch 00100 | Train Loss 0.6727 | Val precision 30.7443% | Val recall 93.1373% | Val F1 46.2287%\n",
      "Epoch 00200 | Train Loss 0.6073 | Val precision 43.7143% | Val recall 75.0000% | Val F1 55.2347%\n",
      "Epoch 00300 | Train Loss 0.6101 | Val precision 43.3437% | Val recall 68.6275% | Val F1 53.1309%\n",
      "Epoch 00400 | Train Loss 0.6077 | Val precision 42.6230% | Val recall 76.4706% | Val F1 54.7368%\n",
      "Epoch 00500 | Train Loss 0.6032 | Val precision 44.7447% | Val recall 73.0392% | Val F1 55.4935%\n",
      "Epoch 00600 | Train Loss 0.6058 | Val precision 43.3428% | Val recall 75.0000% | Val F1 54.9372%\n",
      "Epoch 00700 | Train Loss 0.6021 | Val precision 44.3662% | Val recall 61.7647% | Val F1 51.6393%\n",
      "Epoch 00800 | Train Loss 0.6005 | Val precision 44.0922% | Val recall 75.0000% | Val F1 55.5354%\n",
      "Epoch 00900 | Train Loss 0.6055 | Val precision 44.5748% | Val recall 74.5098% | Val F1 55.7798%\n",
      "Epoch 01000 | Train Loss 0.6046 | Val precision 44.8052% | Val recall 67.6471% | Val F1 53.9062%\n",
      "Epoch 00100 | Train Loss 0.6721 | Val precision 30.6709% | Val recall 94.1176% | Val F1 46.2651%\n",
      "Epoch 00200 | Train Loss 0.6084 | Val precision 45.0617% | Val recall 71.5686% | Val F1 55.3030%\n",
      "Epoch 00300 | Train Loss 0.6113 | Val precision 42.4870% | Val recall 80.3922% | Val F1 55.5932%\n",
      "Epoch 00400 | Train Loss 0.6077 | Val precision 43.8710% | Val recall 66.6667% | Val F1 52.9183%\n",
      "Epoch 00500 | Train Loss 0.6016 | Val precision 43.4174% | Val recall 75.9804% | Val F1 55.2585%\n",
      "Epoch 00600 | Train Loss 0.6063 | Val precision 45.1178% | Val recall 65.6863% | Val F1 53.4930%\n",
      "Epoch 00700 | Train Loss 0.6013 | Val precision 44.1441% | Val recall 72.0588% | Val F1 54.7486%\n",
      "Epoch 00800 | Train Loss 0.6014 | Val precision 42.0103% | Val recall 79.9020% | Val F1 55.0676%\n",
      "Epoch 00900 | Train Loss 0.6048 | Val precision 43.9394% | Val recall 56.8627% | Val F1 49.5726%\n",
      "Epoch 01000 | Train Loss 0.6185 | Val precision 41.6459% | Val recall 81.8627% | Val F1 55.2066%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00100 | Train Loss 0.6726 | Val precision 30.6947% | Val recall 93.1373% | Val F1 46.1725%\n",
      "Epoch 00200 | Train Loss 0.6073 | Val precision 43.5897% | Val recall 75.0000% | Val F1 55.1351%\n",
      "Epoch 00300 | Train Loss 0.6109 | Val precision 42.9012% | Val recall 68.1373% | Val F1 52.6515%\n",
      "Epoch 00400 | Train Loss 0.6075 | Val precision 42.3592% | Val recall 77.4510% | Val F1 54.7660%\n",
      "Epoch 00500 | Train Loss 0.6040 | Val precision 42.4084% | Val recall 79.4118% | Val F1 55.2901%\n",
      "Epoch 00600 | Train Loss 0.6053 | Val precision 44.0895% | Val recall 67.6471% | Val F1 53.3849%\n",
      "Epoch 00700 | Train Loss 0.6009 | Val precision 42.9730% | Val recall 77.9412% | Val F1 55.4007%\n",
      "Epoch 00800 | Train Loss 0.6006 | Val precision 45.5696% | Val recall 70.5882% | Val F1 55.3846%\n",
      "Epoch 00900 | Train Loss 0.6061 | Val precision 46.0432% | Val recall 62.7451% | Val F1 53.1120%\n",
      "Epoch 01000 | Train Loss 0.6064 | Val precision 44.7761% | Val recall 73.5294% | Val F1 55.6586%\n",
      "Epoch 00100 | Train Loss 0.6804 | Val precision 30.1262% | Val recall 93.6275% | Val F1 45.5847%\n",
      "Epoch 00200 | Train Loss 0.6125 | Val precision 44.4444% | Val recall 60.7843% | Val F1 51.3458%\n",
      "Epoch 00300 | Train Loss 0.6133 | Val precision 43.3140% | Val recall 73.0392% | Val F1 54.3796%\n",
      "Epoch 00400 | Train Loss 0.6076 | Val precision 43.6306% | Val recall 67.1569% | Val F1 52.8958%\n",
      "Epoch 00500 | Train Loss 0.6087 | Val precision 43.9344% | Val recall 65.6863% | Val F1 52.6523%\n",
      "Epoch 00600 | Train Loss 0.6150 | Val precision 42.8125% | Val recall 67.1569% | Val F1 52.2901%\n",
      "Epoch 00700 | Train Loss 0.6096 | Val precision 42.9967% | Val recall 64.7059% | Val F1 51.6634%\n",
      "Epoch 00800 | Train Loss 0.6041 | Val precision 44.1640% | Val recall 68.6275% | Val F1 53.7428%\n",
      "Epoch 00900 | Train Loss 0.6161 | Val precision 43.7500% | Val recall 68.6275% | Val F1 53.4351%\n",
      "Epoch 01000 | Train Loss 0.6090 | Val precision 43.1085% | Val recall 72.0588% | Val F1 53.9450%\n",
      "Epoch 00100 | Train Loss 0.6797 | Val precision 29.8762% | Val recall 94.6078% | Val F1 45.4118%\n",
      "Epoch 00200 | Train Loss 0.6114 | Val precision 38.8128% | Val recall 83.3333% | Val F1 52.9595%\n",
      "Epoch 00300 | Train Loss 0.6159 | Val precision 42.6036% | Val recall 70.5882% | Val F1 53.1365%\n",
      "Epoch 00400 | Train Loss 0.6085 | Val precision 42.9348% | Val recall 77.4510% | Val F1 55.2448%\n",
      "Epoch 00500 | Train Loss 0.6120 | Val precision 40.2410% | Val recall 81.8627% | Val F1 53.9580%\n",
      "Epoch 00600 | Train Loss 0.6122 | Val precision 44.6970% | Val recall 57.8431% | Val F1 50.4274%\n",
      "Epoch 00700 | Train Loss 0.6090 | Val precision 44.0559% | Val recall 61.7647% | Val F1 51.4286%\n",
      "Epoch 00800 | Train Loss 0.6075 | Val precision 38.5135% | Val recall 83.8235% | Val F1 52.7778%\n",
      "Epoch 00900 | Train Loss 0.6114 | Val precision 44.7020% | Val recall 66.1765% | Val F1 53.3597%\n",
      "Epoch 01000 | Train Loss 0.6107 | Val precision 43.1373% | Val recall 53.9216% | Val F1 47.9303%\n",
      "Epoch 00100 | Train Loss 0.6803 | Val precision 29.9843% | Val recall 93.6275% | Val F1 45.4221%\n",
      "Epoch 00200 | Train Loss 0.6141 | Val precision 44.1781% | Val recall 63.2353% | Val F1 52.0161%\n",
      "Epoch 00300 | Train Loss 0.6135 | Val precision 43.3048% | Val recall 74.5098% | Val F1 54.7748%\n",
      "Epoch 00400 | Train Loss 0.6075 | Val precision 42.8571% | Val recall 72.0588% | Val F1 53.7477%\n",
      "Epoch 00500 | Train Loss 0.6102 | Val precision 42.8986% | Val recall 72.5490% | Val F1 53.9162%\n",
      "Epoch 00600 | Train Loss 0.6122 | Val precision 45.0704% | Val recall 62.7451% | Val F1 52.4590%\n",
      "Epoch 00700 | Train Loss 0.6132 | Val precision 42.3188% | Val recall 71.5686% | Val F1 53.1876%\n",
      "Epoch 00800 | Train Loss 0.6043 | Val precision 44.4444% | Val recall 68.6275% | Val F1 53.9499%\n",
      "Epoch 00900 | Train Loss 0.6154 | Val precision 45.4237% | Val recall 65.6863% | Val F1 53.7074%\n",
      "Epoch 01000 | Train Loss 0.6100 | Val precision 43.7500% | Val recall 65.1961% | Val F1 52.3622%\n",
      "Epoch 00100 | Train Loss 0.6095 | Val precision 44.2136% | Val recall 73.0392% | Val F1 55.0832%\n",
      "Epoch 00200 | Train Loss 0.6009 | Val precision 45.3925% | Val recall 65.1961% | Val F1 53.5211%\n",
      "Epoch 00300 | Train Loss 0.6022 | Val precision 45.0617% | Val recall 71.5686% | Val F1 55.3030%\n",
      "Epoch 00400 | Train Loss 0.5997 | Val precision 44.8795% | Val recall 73.0392% | Val F1 55.5970%\n",
      "Epoch 00500 | Train Loss 0.6053 | Val precision 44.7811% | Val recall 65.1961% | Val F1 53.0938%\n",
      "Epoch 00600 | Train Loss 0.5969 | Val precision 45.0000% | Val recall 70.5882% | Val F1 54.9618%\n",
      "Epoch 00700 | Train Loss 0.6031 | Val precision 42.5926% | Val recall 78.9216% | Val F1 55.3265%\n",
      "Epoch 00800 | Train Loss 0.5989 | Val precision 44.8378% | Val recall 74.5098% | Val F1 55.9853%\n",
      "Epoch 00900 | Train Loss 0.6021 | Val precision 44.2197% | Val recall 75.0000% | Val F1 55.6364%\n",
      "Epoch 01000 | Train Loss 0.5984 | Val precision 45.1713% | Val recall 71.0784% | Val F1 55.2381%\n",
      "Epoch 00100 | Train Loss 0.6101 | Val precision 44.2136% | Val recall 73.0392% | Val F1 55.0832%\n",
      "Epoch 00200 | Train Loss 0.6139 | Val precision 43.4426% | Val recall 77.9412% | Val F1 55.7895%\n",
      "Epoch 00300 | Train Loss 0.6043 | Val precision 45.6954% | Val recall 67.6471% | Val F1 54.5455%\n",
      "Epoch 00400 | Train Loss 0.6023 | Val precision 43.5262% | Val recall 77.4510% | Val F1 55.7319%\n",
      "Epoch 00500 | Train Loss 0.6079 | Val precision 45.2206% | Val recall 60.2941% | Val F1 51.6807%\n",
      "Epoch 00600 | Train Loss 0.5980 | Val precision 45.0331% | Val recall 66.6667% | Val F1 53.7549%\n",
      "Epoch 00700 | Train Loss 0.6021 | Val precision 43.2065% | Val recall 77.9412% | Val F1 55.5944%\n",
      "Epoch 00800 | Train Loss 0.6013 | Val precision 39.2111% | Val recall 82.8431% | Val F1 53.2283%\n",
      "Epoch 00900 | Train Loss 0.6019 | Val precision 45.0311% | Val recall 71.0784% | Val F1 55.1331%\n",
      "Epoch 01000 | Train Loss 0.5991 | Val precision 44.6991% | Val recall 76.4706% | Val F1 56.4195%\n",
      "Epoch 00100 | Train Loss 0.6096 | Val precision 44.2136% | Val recall 73.0392% | Val F1 55.0832%\n",
      "Epoch 00200 | Train Loss 0.6013 | Val precision 45.6081% | Val recall 66.1765% | Val F1 54.0000%\n",
      "Epoch 00300 | Train Loss 0.6038 | Val precision 44.1341% | Val recall 77.4510% | Val F1 56.2278%\n",
      "Epoch 00400 | Train Loss 0.5997 | Val precision 45.3704% | Val recall 72.0588% | Val F1 55.6818%\n",
      "Epoch 00500 | Train Loss 0.6048 | Val precision 44.5946% | Val recall 64.7059% | Val F1 52.8000%\n",
      "Epoch 00600 | Train Loss 0.5978 | Val precision 45.2096% | Val recall 74.0196% | Val F1 56.1338%\n",
      "Epoch 00700 | Train Loss 0.6018 | Val precision 44.0233% | Val recall 74.0196% | Val F1 55.2102%\n",
      "Epoch 00800 | Train Loss 0.5990 | Val precision 45.1039% | Val recall 74.5098% | Val F1 56.1922%\n",
      "Epoch 00900 | Train Loss 0.6045 | Val precision 44.9848% | Val recall 72.5490% | Val F1 55.5347%\n",
      "Epoch 01000 | Train Loss 0.5982 | Val precision 44.2857% | Val recall 75.9804% | Val F1 55.9567%\n",
      "Epoch 00100 | Train Loss 0.6101 | Val precision 42.7778% | Val recall 75.4902% | Val F1 54.6099%\n",
      "Epoch 00200 | Train Loss 0.6083 | Val precision 43.6047% | Val recall 73.5294% | Val F1 54.7445%\n",
      "Epoch 00300 | Train Loss 0.6124 | Val precision 44.3478% | Val recall 75.0000% | Val F1 55.7377%\n",
      "Epoch 00400 | Train Loss 0.6036 | Val precision 43.6782% | Val recall 74.5098% | Val F1 55.0725%\n",
      "Epoch 00500 | Train Loss 0.6101 | Val precision 43.9446% | Val recall 62.2549% | Val F1 51.5213%\n",
      "Epoch 00600 | Train Loss 0.6048 | Val precision 44.5783% | Val recall 72.5490% | Val F1 55.2239%\n",
      "Epoch 00700 | Train Loss 0.6053 | Val precision 42.4242% | Val recall 75.4902% | Val F1 54.3210%\n",
      "Epoch 00800 | Train Loss 0.6035 | Val precision 43.4903% | Val recall 76.9608% | Val F1 55.5752%\n",
      "Epoch 00900 | Train Loss 0.6050 | Val precision 43.7681% | Val recall 74.0196% | Val F1 55.0091%\n",
      "Epoch 01000 | Train Loss 0.6039 | Val precision 42.8571% | Val recall 72.0588% | Val F1 53.7477%\n",
      "Epoch 00100 | Train Loss 0.6107 | Val precision 43.1755% | Val recall 75.9804% | Val F1 55.0622%\n",
      "Epoch 00200 | Train Loss 0.6097 | Val precision 44.1791% | Val recall 72.5490% | Val F1 54.9165%\n",
      "Epoch 00300 | Train Loss 0.6151 | Val precision 44.0299% | Val recall 57.8431% | Val F1 50.0000%\n",
      "Epoch 00400 | Train Loss 0.6039 | Val precision 44.9640% | Val recall 61.2745% | Val F1 51.8672%\n",
      "Epoch 00500 | Train Loss 0.6113 | Val precision 44.0994% | Val recall 69.6078% | Val F1 53.9924%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00600 | Train Loss 0.6067 | Val precision 44.5161% | Val recall 67.6471% | Val F1 53.6965%\n",
      "Epoch 00700 | Train Loss 0.6057 | Val precision 38.3073% | Val recall 84.3137% | Val F1 52.6799%\n",
      "Epoch 00800 | Train Loss 0.6039 | Val precision 42.3377% | Val recall 79.9020% | Val F1 55.3480%\n",
      "Epoch 00900 | Train Loss 0.6082 | Val precision 44.1176% | Val recall 73.5294% | Val F1 55.1471%\n",
      "Epoch 01000 | Train Loss 0.6055 | Val precision 44.3787% | Val recall 73.5294% | Val F1 55.3506%\n",
      "Epoch 00100 | Train Loss 0.6102 | Val precision 42.7778% | Val recall 75.4902% | Val F1 54.6099%\n",
      "Epoch 00200 | Train Loss 0.6083 | Val precision 44.6809% | Val recall 72.0588% | Val F1 55.1595%\n",
      "Epoch 00300 | Train Loss 0.6115 | Val precision 44.4776% | Val recall 73.0392% | Val F1 55.2876%\n",
      "Epoch 00400 | Train Loss 0.6035 | Val precision 43.6416% | Val recall 74.0196% | Val F1 54.9091%\n",
      "Epoch 00500 | Train Loss 0.6101 | Val precision 44.1696% | Val recall 61.2745% | Val F1 51.3347%\n",
      "Epoch 00600 | Train Loss 0.6048 | Val precision 44.7447% | Val recall 73.0392% | Val F1 55.4935%\n",
      "Epoch 00700 | Train Loss 0.6055 | Val precision 42.3377% | Val recall 79.9020% | Val F1 55.3480%\n",
      "Epoch 00800 | Train Loss 0.6036 | Val precision 44.0000% | Val recall 75.4902% | Val F1 55.5957%\n",
      "Epoch 00900 | Train Loss 0.6057 | Val precision 44.7531% | Val recall 71.0784% | Val F1 54.9242%\n",
      "Epoch 01000 | Train Loss 0.6060 | Val precision 43.1319% | Val recall 76.9608% | Val F1 55.2817%\n",
      "Epoch 00100 | Train Loss 0.6174 | Val precision 41.9890% | Val recall 74.5098% | Val F1 53.7102%\n",
      "Epoch 00200 | Train Loss 0.6127 | Val precision 43.3140% | Val recall 73.0392% | Val F1 54.3796%\n",
      "Epoch 00300 | Train Loss 0.6152 | Val precision 43.6747% | Val recall 71.0784% | Val F1 54.1045%\n",
      "Epoch 00400 | Train Loss 0.6079 | Val precision 43.9490% | Val recall 67.6471% | Val F1 53.2819%\n",
      "Epoch 00500 | Train Loss 0.6150 | Val precision 44.2568% | Val recall 64.2157% | Val F1 52.4000%\n",
      "Epoch 00600 | Train Loss 0.6089 | Val precision 43.4641% | Val recall 65.1961% | Val F1 52.1569%\n",
      "Epoch 00700 | Train Loss 0.6104 | Val precision 42.6966% | Val recall 74.5098% | Val F1 54.2857%\n",
      "Epoch 00800 | Train Loss 0.6107 | Val precision 43.4286% | Val recall 74.5098% | Val F1 54.8736%\n",
      "Epoch 00900 | Train Loss 0.6102 | Val precision 43.4084% | Val recall 66.1765% | Val F1 52.4272%\n",
      "Epoch 01000 | Train Loss 0.6095 | Val precision 43.1310% | Val recall 66.1765% | Val F1 52.2244%\n",
      "Epoch 00100 | Train Loss 0.6178 | Val precision 42.4929% | Val recall 73.5294% | Val F1 53.8600%\n",
      "Epoch 00200 | Train Loss 0.6128 | Val precision 43.9597% | Val recall 64.2157% | Val F1 52.1912%\n",
      "Epoch 00300 | Train Loss 0.6171 | Val precision 44.0233% | Val recall 74.0196% | Val F1 55.2102%\n",
      "Epoch 00400 | Train Loss 0.6092 | Val precision 43.2602% | Val recall 67.6471% | Val F1 52.7725%\n",
      "Epoch 00500 | Train Loss 0.6204 | Val precision 42.8169% | Val recall 74.5098% | Val F1 54.3828%\n",
      "Epoch 00600 | Train Loss 0.6096 | Val precision 43.8776% | Val recall 63.2353% | Val F1 51.8072%\n",
      "Epoch 00700 | Train Loss 0.6101 | Val precision 41.8478% | Val recall 75.4902% | Val F1 53.8462%\n",
      "Epoch 00800 | Train Loss 0.6134 | Val precision 44.0233% | Val recall 74.0196% | Val F1 55.2102%\n",
      "Epoch 00900 | Train Loss 0.6138 | Val precision 42.6877% | Val recall 52.9412% | Val F1 47.2648%\n",
      "Epoch 01000 | Train Loss 0.6113 | Val precision 43.7500% | Val recall 72.0588% | Val F1 54.4444%\n",
      "Epoch 00100 | Train Loss 0.6174 | Val precision 42.1053% | Val recall 74.5098% | Val F1 53.8053%\n",
      "Epoch 00200 | Train Loss 0.6126 | Val precision 42.8994% | Val recall 71.0784% | Val F1 53.5055%\n",
      "Epoch 00300 | Train Loss 0.6153 | Val precision 43.8438% | Val recall 71.5686% | Val F1 54.3762%\n",
      "Epoch 00400 | Train Loss 0.6078 | Val precision 42.1053% | Val recall 70.5882% | Val F1 52.7473%\n",
      "Epoch 00500 | Train Loss 0.6147 | Val precision 44.1077% | Val recall 64.2157% | Val F1 52.2954%\n",
      "Epoch 00600 | Train Loss 0.6085 | Val precision 43.4641% | Val recall 65.1961% | Val F1 52.1569%\n",
      "Epoch 00700 | Train Loss 0.6104 | Val precision 42.5352% | Val recall 74.0196% | Val F1 54.0250%\n",
      "Epoch 00800 | Train Loss 0.6103 | Val precision 43.5897% | Val recall 75.0000% | Val F1 55.1351%\n",
      "Epoch 00900 | Train Loss 0.6102 | Val precision 44.1176% | Val recall 73.5294% | Val F1 55.1471%\n",
      "Epoch 01000 | Train Loss 0.6094 | Val precision 42.5150% | Val recall 69.6078% | Val F1 52.7881%\n",
      "Epoch 00100 | Train Loss 0.6178 | Val precision 39.9491% | Val recall 76.9608% | Val F1 52.5963%\n",
      "Epoch 00200 | Train Loss 0.6018 | Val precision 44.7368% | Val recall 75.0000% | Val F1 56.0440%\n",
      "Epoch 00300 | Train Loss 0.6063 | Val precision 45.0479% | Val recall 69.1176% | Val F1 54.5455%\n",
      "Epoch 00400 | Train Loss 0.6010 | Val precision 44.7761% | Val recall 73.5294% | Val F1 55.6586%\n",
      "Epoch 00500 | Train Loss 0.5972 | Val precision 44.6875% | Val recall 70.0980% | Val F1 54.5802%\n",
      "Epoch 00600 | Train Loss 0.6005 | Val precision 43.1694% | Val recall 77.4510% | Val F1 55.4386%\n",
      "Epoch 00700 | Train Loss 0.5977 | Val precision 45.7237% | Val recall 68.1373% | Val F1 54.7244%\n",
      "Epoch 00800 | Train Loss 0.5964 | Val precision 44.5087% | Val recall 75.4902% | Val F1 56.0000%\n",
      "Epoch 00900 | Train Loss 0.6001 | Val precision 45.9016% | Val recall 68.6275% | Val F1 55.0098%\n",
      "Epoch 01000 | Train Loss 0.5991 | Val precision 45.3172% | Val recall 73.5294% | Val F1 56.0748%\n",
      "Epoch 00100 | Train Loss 0.6178 | Val precision 39.9491% | Val recall 76.9608% | Val F1 52.5963%\n",
      "Epoch 00200 | Train Loss 0.6022 | Val precision 44.0000% | Val recall 75.4902% | Val F1 55.5957%\n",
      "Epoch 00300 | Train Loss 0.6063 | Val precision 45.0479% | Val recall 69.1176% | Val F1 54.5455%\n",
      "Epoch 00400 | Train Loss 0.6012 | Val precision 45.2599% | Val recall 72.5490% | Val F1 55.7439%\n",
      "Epoch 00500 | Train Loss 0.5976 | Val precision 44.8916% | Val recall 71.0784% | Val F1 55.0285%\n",
      "Epoch 00600 | Train Loss 0.6008 | Val precision 42.9730% | Val recall 77.9412% | Val F1 55.4007%\n",
      "Epoch 00700 | Train Loss 0.5979 | Val precision 45.0794% | Val recall 69.6078% | Val F1 54.7206%\n",
      "Epoch 00800 | Train Loss 0.5969 | Val precision 44.8485% | Val recall 72.5490% | Val F1 55.4307%\n",
      "Epoch 00900 | Train Loss 0.5985 | Val precision 45.3674% | Val recall 69.6078% | Val F1 54.9323%\n",
      "Epoch 01000 | Train Loss 0.5997 | Val precision 44.9686% | Val recall 70.0980% | Val F1 54.7893%\n",
      "Epoch 00100 | Train Loss 0.6178 | Val precision 39.9491% | Val recall 76.9608% | Val F1 52.5963%\n",
      "Epoch 00200 | Train Loss 0.6018 | Val precision 44.4767% | Val recall 75.0000% | Val F1 55.8394%\n",
      "Epoch 00300 | Train Loss 0.6063 | Val precision 45.0479% | Val recall 69.1176% | Val F1 54.5455%\n",
      "Epoch 00400 | Train Loss 0.6010 | Val precision 44.9102% | Val recall 73.5294% | Val F1 55.7621%\n",
      "Epoch 00500 | Train Loss 0.5972 | Val precision 44.6875% | Val recall 70.0980% | Val F1 54.5802%\n",
      "Epoch 00600 | Train Loss 0.6005 | Val precision 43.0518% | Val recall 77.4510% | Val F1 55.3415%\n",
      "Epoch 00700 | Train Loss 0.5977 | Val precision 45.5738% | Val recall 68.1373% | Val F1 54.6169%\n",
      "Epoch 00800 | Train Loss 0.5965 | Val precision 44.6377% | Val recall 75.4902% | Val F1 56.1020%\n",
      "Epoch 00900 | Train Loss 0.5992 | Val precision 45.6954% | Val recall 67.6471% | Val F1 54.5455%\n",
      "Epoch 01000 | Train Loss 0.5994 | Val precision 44.9231% | Val recall 71.5686% | Val F1 55.1985%\n",
      "Epoch 00100 | Train Loss 0.6233 | Val precision 44.1088% | Val recall 71.5686% | Val F1 54.5794%\n",
      "Epoch 00200 | Train Loss 0.6106 | Val precision 42.8571% | Val recall 75.0000% | Val F1 54.5455%\n",
      "Epoch 00300 | Train Loss 0.6106 | Val precision 45.3416% | Val recall 71.5686% | Val F1 55.5133%\n",
      "Epoch 00400 | Train Loss 0.6072 | Val precision 44.7130% | Val recall 72.5490% | Val F1 55.3271%\n",
      "Epoch 00500 | Train Loss 0.6013 | Val precision 44.5902% | Val recall 66.6667% | Val F1 53.4381%\n",
      "Epoch 00600 | Train Loss 0.6052 | Val precision 44.4785% | Val recall 71.0784% | Val F1 54.7170%\n",
      "Epoch 00700 | Train Loss 0.6026 | Val precision 44.6667% | Val recall 65.6863% | Val F1 53.1746%\n",
      "Epoch 00800 | Train Loss 0.6005 | Val precision 42.3592% | Val recall 77.4510% | Val F1 54.7660%\n",
      "Epoch 00900 | Train Loss 0.6056 | Val precision 43.0894% | Val recall 77.9412% | Val F1 55.4974%\n",
      "Epoch 01000 | Train Loss 0.6041 | Val precision 42.6357% | Val recall 80.8824% | Val F1 55.8376%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00100 | Train Loss 0.6233 | Val precision 43.9759% | Val recall 71.5686% | Val F1 54.4776%\n",
      "Epoch 00200 | Train Loss 0.6122 | Val precision 44.6483% | Val recall 71.5686% | Val F1 54.9906%\n",
      "Epoch 00300 | Train Loss 0.6107 | Val precision 45.3416% | Val recall 71.5686% | Val F1 55.5133%\n",
      "Epoch 00400 | Train Loss 0.6078 | Val precision 45.2012% | Val recall 71.5686% | Val F1 55.4080%\n",
      "Epoch 00500 | Train Loss 0.6018 | Val precision 44.7368% | Val recall 66.6667% | Val F1 53.5433%\n",
      "Epoch 00600 | Train Loss 0.6058 | Val precision 43.6578% | Val recall 72.5490% | Val F1 54.5120%\n",
      "Epoch 00700 | Train Loss 0.6010 | Val precision 44.0299% | Val recall 57.8431% | Val F1 50.0000%\n",
      "Epoch 00800 | Train Loss 0.6008 | Val precision 42.2460% | Val recall 77.4510% | Val F1 54.6713%\n",
      "Epoch 00900 | Train Loss 0.6061 | Val precision 45.2381% | Val recall 65.1961% | Val F1 53.4137%\n",
      "Epoch 01000 | Train Loss 0.6043 | Val precision 44.5902% | Val recall 66.6667% | Val F1 53.4381%\n",
      "Epoch 00100 | Train Loss 0.6233 | Val precision 44.1088% | Val recall 71.5686% | Val F1 54.5794%\n",
      "Epoch 00200 | Train Loss 0.6107 | Val precision 42.8571% | Val recall 75.0000% | Val F1 54.5455%\n",
      "Epoch 00300 | Train Loss 0.6106 | Val precision 45.3416% | Val recall 71.5686% | Val F1 55.5133%\n",
      "Epoch 00400 | Train Loss 0.6074 | Val precision 44.9848% | Val recall 72.5490% | Val F1 55.5347%\n",
      "Epoch 00500 | Train Loss 0.6013 | Val precision 44.5902% | Val recall 66.6667% | Val F1 53.4381%\n",
      "Epoch 00600 | Train Loss 0.6052 | Val precision 44.4785% | Val recall 71.0784% | Val F1 54.7170%\n",
      "Epoch 00700 | Train Loss 0.6029 | Val precision 44.7458% | Val recall 64.7059% | Val F1 52.9058%\n",
      "Epoch 00800 | Train Loss 0.6006 | Val precision 42.2872% | Val recall 77.9412% | Val F1 54.8276%\n",
      "Epoch 00900 | Train Loss 0.6059 | Val precision 43.2065% | Val recall 77.9412% | Val F1 55.5944%\n",
      "Epoch 01000 | Train Loss 0.6044 | Val precision 42.7083% | Val recall 80.3922% | Val F1 55.7823%\n",
      "Epoch 00100 | Train Loss 0.6300 | Val precision 43.8438% | Val recall 71.5686% | Val F1 54.3762%\n",
      "Epoch 00200 | Train Loss 0.6123 | Val precision 43.0986% | Val recall 75.0000% | Val F1 54.7406%\n",
      "Epoch 00300 | Train Loss 0.6153 | Val precision 41.5584% | Val recall 78.4314% | Val F1 54.3294%\n",
      "Epoch 00400 | Train Loss 0.6084 | Val precision 43.8596% | Val recall 73.5294% | Val F1 54.9451%\n",
      "Epoch 00500 | Train Loss 0.6088 | Val precision 42.2872% | Val recall 77.9412% | Val F1 54.8276%\n",
      "Epoch 00600 | Train Loss 0.6118 | Val precision 43.0267% | Val recall 71.0784% | Val F1 53.6044%\n",
      "Epoch 00700 | Train Loss 0.6085 | Val precision 43.3657% | Val recall 65.6863% | Val F1 52.2417%\n",
      "Epoch 00800 | Train Loss 0.6069 | Val precision 42.8571% | Val recall 76.4706% | Val F1 54.9296%\n",
      "Epoch 00900 | Train Loss 0.6110 | Val precision 42.1196% | Val recall 75.9804% | Val F1 54.1958%\n",
      "Epoch 01000 | Train Loss 0.6092 | Val precision 40.9877% | Val recall 81.3725% | Val F1 54.5156%\n",
      "Epoch 00100 | Train Loss 0.6301 | Val precision 44.0120% | Val recall 72.0588% | Val F1 54.6468%\n",
      "Epoch 00200 | Train Loss 0.6127 | Val precision 44.5545% | Val recall 66.1765% | Val F1 53.2544%\n",
      "Epoch 00300 | Train Loss 0.6154 | Val precision 41.5144% | Val recall 77.9412% | Val F1 54.1738%\n",
      "Epoch 00400 | Train Loss 0.6090 | Val precision 43.8235% | Val recall 73.0392% | Val F1 54.7794%\n",
      "Epoch 00500 | Train Loss 0.6091 | Val precision 42.4000% | Val recall 77.9412% | Val F1 54.9223%\n",
      "Epoch 00600 | Train Loss 0.6114 | Val precision 43.7107% | Val recall 68.1373% | Val F1 53.2567%\n",
      "Epoch 00700 | Train Loss 0.6082 | Val precision 43.2056% | Val recall 60.7843% | Val F1 50.5092%\n",
      "Epoch 00800 | Train Loss 0.6069 | Val precision 41.3706% | Val recall 79.9020% | Val F1 54.5151%\n",
      "Epoch 00900 | Train Loss 0.6114 | Val precision 42.5824% | Val recall 75.9804% | Val F1 54.5775%\n",
      "Epoch 01000 | Train Loss 0.6094 | Val precision 40.4878% | Val recall 81.3725% | Val F1 54.0717%\n",
      "Epoch 00100 | Train Loss 0.6300 | Val precision 43.8438% | Val recall 71.5686% | Val F1 54.3762%\n",
      "Epoch 00200 | Train Loss 0.6125 | Val precision 43.1818% | Val recall 74.5098% | Val F1 54.6763%\n",
      "Epoch 00300 | Train Loss 0.6153 | Val precision 41.5584% | Val recall 78.4314% | Val F1 54.3294%\n",
      "Epoch 00400 | Train Loss 0.6085 | Val precision 43.8596% | Val recall 73.5294% | Val F1 54.9451%\n",
      "Epoch 00500 | Train Loss 0.6089 | Val precision 42.2872% | Val recall 77.9412% | Val F1 54.8276%\n",
      "Epoch 00600 | Train Loss 0.6118 | Val precision 42.7711% | Val recall 69.6078% | Val F1 52.9851%\n",
      "Epoch 00700 | Train Loss 0.6086 | Val precision 43.5065% | Val recall 65.6863% | Val F1 52.3438%\n",
      "Epoch 00800 | Train Loss 0.6070 | Val precision 42.6630% | Val recall 76.9608% | Val F1 54.8951%\n",
      "Epoch 00900 | Train Loss 0.6109 | Val precision 42.2764% | Val recall 76.4706% | Val F1 54.4503%\n",
      "Epoch 01000 | Train Loss 0.6086 | Val precision 40.5340% | Val recall 81.8627% | Val F1 54.2208%\n",
      "Epoch 00100 | Train Loss 0.6211 | Val precision 43.3526% | Val recall 73.5294% | Val F1 54.5455%\n",
      "Epoch 00200 | Train Loss 0.6008 | Val precision 45.3731% | Val recall 74.5098% | Val F1 56.4007%\n",
      "Epoch 00300 | Train Loss 0.6051 | Val precision 45.3947% | Val recall 67.6471% | Val F1 54.3307%\n",
      "Epoch 00400 | Train Loss 0.6011 | Val precision 44.7447% | Val recall 73.0392% | Val F1 55.4935%\n",
      "Epoch 00500 | Train Loss 0.5972 | Val precision 45.0000% | Val recall 70.5882% | Val F1 54.9618%\n",
      "Epoch 00600 | Train Loss 0.5999 | Val precision 42.4000% | Val recall 77.9412% | Val F1 54.9223%\n",
      "Epoch 00700 | Train Loss 0.5973 | Val precision 45.4259% | Val recall 70.5882% | Val F1 55.2783%\n",
      "Epoch 00800 | Train Loss 0.5967 | Val precision 44.6746% | Val recall 74.0196% | Val F1 55.7196%\n",
      "Epoch 00900 | Train Loss 0.5988 | Val precision 46.0784% | Val recall 69.1176% | Val F1 55.2941%\n",
      "Epoch 01000 | Train Loss 0.5995 | Val precision 45.0000% | Val recall 70.5882% | Val F1 54.9618%\n",
      "Epoch 00100 | Train Loss 0.6211 | Val precision 43.5159% | Val recall 74.0196% | Val F1 54.8094%\n",
      "Epoch 00200 | Train Loss 0.6010 | Val precision 45.3731% | Val recall 74.5098% | Val F1 56.4007%\n",
      "Epoch 00300 | Train Loss 0.6053 | Val precision 45.3947% | Val recall 67.6471% | Val F1 54.3307%\n",
      "Epoch 00400 | Train Loss 0.6012 | Val precision 45.3416% | Val recall 71.5686% | Val F1 55.5133%\n",
      "Epoch 00500 | Train Loss 0.5976 | Val precision 45.1220% | Val recall 72.5490% | Val F1 55.6391%\n",
      "Epoch 00600 | Train Loss 0.6003 | Val precision 42.4802% | Val recall 78.9216% | Val F1 55.2316%\n",
      "Epoch 00700 | Train Loss 0.5975 | Val precision 45.1411% | Val recall 70.5882% | Val F1 55.0669%\n",
      "Epoch 00800 | Train Loss 0.5970 | Val precision 45.0000% | Val recall 70.5882% | Val F1 54.9618%\n",
      "Epoch 00900 | Train Loss 0.5986 | Val precision 45.2830% | Val recall 70.5882% | Val F1 55.1724%\n",
      "Epoch 01000 | Train Loss 0.6004 | Val precision 45.2532% | Val recall 70.0980% | Val F1 55.0000%\n",
      "Epoch 00100 | Train Loss 0.6211 | Val precision 43.2277% | Val recall 73.5294% | Val F1 54.4465%\n",
      "Epoch 00200 | Train Loss 0.6008 | Val precision 45.3731% | Val recall 74.5098% | Val F1 56.4007%\n",
      "Epoch 00300 | Train Loss 0.6051 | Val precision 45.3947% | Val recall 67.6471% | Val F1 54.3307%\n",
      "Epoch 00400 | Train Loss 0.6012 | Val precision 44.8795% | Val recall 73.0392% | Val F1 55.5970%\n",
      "Epoch 00500 | Train Loss 0.5972 | Val precision 44.8598% | Val recall 70.5882% | Val F1 54.8571%\n",
      "Epoch 00600 | Train Loss 0.5999 | Val precision 42.4000% | Val recall 77.9412% | Val F1 54.9223%\n",
      "Epoch 00700 | Train Loss 0.5973 | Val precision 45.5696% | Val recall 70.5882% | Val F1 55.3846%\n",
      "Epoch 00800 | Train Loss 0.5967 | Val precision 45.0746% | Val recall 74.0196% | Val F1 56.0297%\n",
      "Epoch 00900 | Train Loss 0.5987 | Val precision 45.6311% | Val recall 69.1176% | Val F1 54.9708%\n",
      "Epoch 01000 | Train Loss 0.5995 | Val precision 45.2830% | Val recall 70.5882% | Val F1 55.1724%\n",
      "Epoch 00100 | Train Loss 0.6274 | Val precision 43.0168% | Val recall 75.4902% | Val F1 54.8043%\n",
      "Epoch 00200 | Train Loss 0.6063 | Val precision 44.8485% | Val recall 72.5490% | Val F1 55.4307%\n",
      "Epoch 00300 | Train Loss 0.6086 | Val precision 44.4089% | Val recall 68.1373% | Val F1 53.7718%\n",
      "Epoch 00400 | Train Loss 0.6072 | Val precision 44.7020% | Val recall 66.1765% | Val F1 53.3597%\n",
      "Epoch 00500 | Train Loss 0.6022 | Val precision 44.5161% | Val recall 67.6471% | Val F1 53.6965%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00600 | Train Loss 0.6048 | Val precision 43.9528% | Val recall 73.0392% | Val F1 54.8803%\n",
      "Epoch 00700 | Train Loss 0.6009 | Val precision 44.2029% | Val recall 59.8039% | Val F1 50.8333%\n",
      "Epoch 00800 | Train Loss 0.5998 | Val precision 42.1189% | Val recall 79.9020% | Val F1 55.1607%\n",
      "Epoch 00900 | Train Loss 0.6068 | Val precision 43.1755% | Val recall 75.9804% | Val F1 55.0622%\n",
      "Epoch 01000 | Train Loss 0.6050 | Val precision 42.5926% | Val recall 78.9216% | Val F1 55.3265%\n",
      "Epoch 00100 | Train Loss 0.6274 | Val precision 43.0168% | Val recall 75.4902% | Val F1 54.8043%\n",
      "Epoch 00200 | Train Loss 0.6065 | Val precision 44.9231% | Val recall 71.5686% | Val F1 55.1985%\n",
      "Epoch 00300 | Train Loss 0.6087 | Val precision 44.5183% | Val recall 65.6863% | Val F1 53.0693%\n",
      "Epoch 00400 | Train Loss 0.6068 | Val precision 45.0704% | Val recall 62.7451% | Val F1 52.4590%\n",
      "Epoch 00500 | Train Loss 0.6032 | Val precision 43.8486% | Val recall 68.1373% | Val F1 53.3589%\n",
      "Epoch 00600 | Train Loss 0.6050 | Val precision 43.3428% | Val recall 75.0000% | Val F1 54.9372%\n",
      "Epoch 00700 | Train Loss 0.6004 | Val precision 44.3262% | Val recall 61.2745% | Val F1 51.4403%\n",
      "Epoch 00800 | Train Loss 0.6005 | Val precision 43.7143% | Val recall 75.0000% | Val F1 55.2347%\n",
      "Epoch 00900 | Train Loss 0.6043 | Val precision 45.5479% | Val recall 65.1961% | Val F1 53.6290%\n",
      "Epoch 01000 | Train Loss 0.6045 | Val precision 44.6254% | Val recall 67.1569% | Val F1 53.6204%\n",
      "Epoch 00100 | Train Loss 0.6274 | Val precision 43.0168% | Val recall 75.4902% | Val F1 54.8043%\n",
      "Epoch 00200 | Train Loss 0.6064 | Val precision 44.8485% | Val recall 72.5490% | Val F1 55.4307%\n",
      "Epoch 00300 | Train Loss 0.6086 | Val precision 44.4089% | Val recall 68.1373% | Val F1 53.7718%\n",
      "Epoch 00400 | Train Loss 0.6072 | Val precision 44.6667% | Val recall 65.6863% | Val F1 53.1746%\n",
      "Epoch 00500 | Train Loss 0.6024 | Val precision 44.2308% | Val recall 67.6471% | Val F1 53.4884%\n",
      "Epoch 00600 | Train Loss 0.6048 | Val precision 43.9528% | Val recall 73.0392% | Val F1 54.8803%\n",
      "Epoch 00700 | Train Loss 0.6006 | Val precision 43.9114% | Val recall 58.3333% | Val F1 50.1053%\n",
      "Epoch 00800 | Train Loss 0.5998 | Val precision 42.1189% | Val recall 79.9020% | Val F1 55.1607%\n",
      "Epoch 00900 | Train Loss 0.6075 | Val precision 43.7853% | Val recall 75.9804% | Val F1 55.5556%\n",
      "Epoch 01000 | Train Loss 0.6055 | Val precision 42.6997% | Val recall 75.9804% | Val F1 54.6737%\n",
      "Epoch 00100 | Train Loss 0.6335 | Val precision 42.8977% | Val recall 74.0196% | Val F1 54.3165%\n",
      "Epoch 00200 | Train Loss 0.6088 | Val precision 44.1781% | Val recall 63.2353% | Val F1 52.0161%\n",
      "Epoch 00300 | Train Loss 0.6129 | Val precision 42.1466% | Val recall 78.9216% | Val F1 54.9488%\n",
      "Epoch 00400 | Train Loss 0.6085 | Val precision 43.1250% | Val recall 67.6471% | Val F1 52.6718%\n",
      "Epoch 00500 | Train Loss 0.6085 | Val precision 42.3529% | Val recall 70.5882% | Val F1 52.9412%\n",
      "Epoch 00600 | Train Loss 0.6105 | Val precision 42.9825% | Val recall 72.0588% | Val F1 53.8462%\n",
      "Epoch 00700 | Train Loss 0.6065 | Val precision 43.1579% | Val recall 60.2941% | Val F1 50.3067%\n",
      "Epoch 00800 | Train Loss 0.6046 | Val precision 40.9201% | Val recall 82.8431% | Val F1 54.7812%\n",
      "Epoch 00900 | Train Loss 0.6125 | Val precision 43.1818% | Val recall 74.5098% | Val F1 54.6763%\n",
      "Epoch 01000 | Train Loss 0.6087 | Val precision 39.7163% | Val recall 82.3529% | Val F1 53.5885%\n",
      "Epoch 00100 | Train Loss 0.6336 | Val precision 42.6966% | Val recall 74.5098% | Val F1 54.2857%\n",
      "Epoch 00200 | Train Loss 0.6083 | Val precision 43.8406% | Val recall 59.3137% | Val F1 50.4167%\n",
      "Epoch 00300 | Train Loss 0.6132 | Val precision 42.6630% | Val recall 76.9608% | Val F1 54.8951%\n",
      "Epoch 00400 | Train Loss 0.6083 | Val precision 43.5897% | Val recall 66.6667% | Val F1 52.7132%\n",
      "Epoch 00500 | Train Loss 0.6083 | Val precision 43.0818% | Val recall 67.1569% | Val F1 52.4904%\n",
      "Epoch 00600 | Train Loss 0.6115 | Val precision 42.3077% | Val recall 75.4902% | Val F1 54.2254%\n",
      "Epoch 00700 | Train Loss 0.6072 | Val precision 42.9577% | Val recall 59.8039% | Val F1 50.0000%\n",
      "Epoch 00800 | Train Loss 0.6056 | Val precision 41.5385% | Val recall 79.4118% | Val F1 54.5455%\n",
      "Epoch 00900 | Train Loss 0.6107 | Val precision 43.4402% | Val recall 73.0392% | Val F1 54.4790%\n",
      "Epoch 01000 | Train Loss 0.6084 | Val precision 43.1085% | Val recall 72.0588% | Val F1 53.9450%\n",
      "Epoch 00100 | Train Loss 0.6335 | Val precision 42.8977% | Val recall 74.0196% | Val F1 54.3165%\n",
      "Epoch 00200 | Train Loss 0.6088 | Val precision 44.2907% | Val recall 62.7451% | Val F1 51.9270%\n",
      "Epoch 00300 | Train Loss 0.6129 | Val precision 42.1466% | Val recall 78.9216% | Val F1 54.9488%\n",
      "Epoch 00400 | Train Loss 0.6085 | Val precision 43.0818% | Val recall 67.1569% | Val F1 52.4904%\n",
      "Epoch 00500 | Train Loss 0.6085 | Val precision 42.2619% | Val recall 69.6078% | Val F1 52.5926%\n",
      "Epoch 00600 | Train Loss 0.6105 | Val precision 42.6513% | Val recall 72.5490% | Val F1 53.7205%\n",
      "Epoch 00700 | Train Loss 0.6065 | Val precision 43.0605% | Val recall 59.3137% | Val F1 49.8969%\n",
      "Epoch 00800 | Train Loss 0.6047 | Val precision 40.7317% | Val recall 81.8627% | Val F1 54.3974%\n",
      "Epoch 00900 | Train Loss 0.6125 | Val precision 43.7318% | Val recall 73.5294% | Val F1 54.8446%\n",
      "Epoch 01000 | Train Loss 0.6092 | Val precision 40.3382% | Val recall 81.8627% | Val F1 54.0453%\n",
      "Epoch 00100 | Train Loss 0.6132 | Val precision 44.9848% | Val recall 72.5490% | Val F1 55.5347%\n",
      "Epoch 00200 | Train Loss 0.6022 | Val precision 47.3684% | Val recall 61.7647% | Val F1 53.6170%\n",
      "Epoch 00300 | Train Loss 0.6025 | Val precision 44.2577% | Val recall 77.4510% | Val F1 56.3280%\n",
      "Epoch 00400 | Train Loss 0.6019 | Val precision 44.7761% | Val recall 73.5294% | Val F1 55.6586%\n",
      "Epoch 00500 | Train Loss 0.6049 | Val precision 44.8378% | Val recall 74.5098% | Val F1 55.9853%\n",
      "Epoch 00600 | Train Loss 0.5973 | Val precision 45.2599% | Val recall 72.5490% | Val F1 55.7439%\n",
      "Epoch 00700 | Train Loss 0.6022 | Val precision 44.6429% | Val recall 73.5294% | Val F1 55.5556%\n",
      "Epoch 00800 | Train Loss 0.5989 | Val precision 44.6064% | Val recall 75.0000% | Val F1 55.9415%\n",
      "Epoch 00900 | Train Loss 0.6019 | Val precision 43.6813% | Val recall 77.9412% | Val F1 55.9859%\n",
      "Epoch 01000 | Train Loss 0.5974 | Val precision 45.3453% | Val recall 74.0196% | Val F1 56.2384%\n",
      "Epoch 00100 | Train Loss 0.6133 | Val precision 44.9848% | Val recall 72.5490% | Val F1 55.5347%\n",
      "Epoch 00200 | Train Loss 0.6036 | Val precision 47.2119% | Val recall 62.2549% | Val F1 53.6998%\n",
      "Epoch 00300 | Train Loss 0.6026 | Val precision 44.2577% | Val recall 77.4510% | Val F1 56.3280%\n",
      "Epoch 00400 | Train Loss 0.6012 | Val precision 44.6686% | Val recall 75.9804% | Val F1 56.2613%\n",
      "Epoch 00500 | Train Loss 0.6053 | Val precision 44.8171% | Val recall 72.0588% | Val F1 55.2632%\n",
      "Epoch 00600 | Train Loss 0.5975 | Val precision 45.3125% | Val recall 71.0784% | Val F1 55.3435%\n",
      "Epoch 00700 | Train Loss 0.6024 | Val precision 44.6108% | Val recall 73.0392% | Val F1 55.3903%\n",
      "Epoch 00800 | Train Loss 0.5990 | Val precision 43.8889% | Val recall 77.4510% | Val F1 56.0284%\n",
      "Epoch 00900 | Train Loss 0.6020 | Val precision 43.1267% | Val recall 78.4314% | Val F1 55.6522%\n",
      "Epoch 01000 | Train Loss 0.5976 | Val precision 45.2888% | Val recall 73.0392% | Val F1 55.9099%\n",
      "Epoch 00100 | Train Loss 0.6132 | Val precision 44.9848% | Val recall 72.5490% | Val F1 55.5347%\n",
      "Epoch 00200 | Train Loss 0.6022 | Val precision 47.3684% | Val recall 61.7647% | Val F1 53.6170%\n",
      "Epoch 00300 | Train Loss 0.6025 | Val precision 44.2577% | Val recall 77.4510% | Val F1 56.3280%\n",
      "Epoch 00400 | Train Loss 0.6017 | Val precision 44.5104% | Val recall 73.5294% | Val F1 55.4529%\n",
      "Epoch 00500 | Train Loss 0.6049 | Val precision 44.8378% | Val recall 74.5098% | Val F1 55.9853%\n",
      "Epoch 00600 | Train Loss 0.5973 | Val precision 45.5108% | Val recall 72.0588% | Val F1 55.7875%\n",
      "Epoch 00700 | Train Loss 0.6023 | Val precision 44.6429% | Val recall 73.5294% | Val F1 55.5556%\n",
      "Epoch 00800 | Train Loss 0.5990 | Val precision 44.4126% | Val recall 75.9804% | Val F1 56.0579%\n",
      "Epoch 00900 | Train Loss 0.6018 | Val precision 43.5616% | Val recall 77.9412% | Val F1 55.8875%\n",
      "Epoch 01000 | Train Loss 0.5974 | Val precision 45.2381% | Val recall 74.5098% | Val F1 56.2963%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00100 | Train Loss 0.6179 | Val precision 44.8598% | Val recall 70.5882% | Val F1 54.8571%\n",
      "Epoch 00200 | Train Loss 0.6090 | Val precision 43.8596% | Val recall 73.5294% | Val F1 54.9451%\n",
      "Epoch 00300 | Train Loss 0.6112 | Val precision 42.1875% | Val recall 79.4118% | Val F1 55.1020%\n",
      "Epoch 00400 | Train Loss 0.6034 | Val precision 44.4805% | Val recall 67.1569% | Val F1 53.5156%\n",
      "Epoch 00500 | Train Loss 0.6108 | Val precision 44.1270% | Val recall 68.1373% | Val F1 53.5645%\n",
      "Epoch 00600 | Train Loss 0.6050 | Val precision 43.5530% | Val recall 74.5098% | Val F1 54.9729%\n",
      "Epoch 00700 | Train Loss 0.6041 | Val precision 42.3497% | Val recall 75.9804% | Val F1 54.3860%\n",
      "Epoch 00800 | Train Loss 0.6033 | Val precision 44.5428% | Val recall 74.0196% | Val F1 55.6169%\n",
      "Epoch 00900 | Train Loss 0.6062 | Val precision 43.5294% | Val recall 72.5490% | Val F1 54.4118%\n",
      "Epoch 01000 | Train Loss 0.6039 | Val precision 43.5185% | Val recall 69.1176% | Val F1 53.4091%\n",
      "Epoch 00100 | Train Loss 0.6180 | Val precision 44.8598% | Val recall 70.5882% | Val F1 54.8571%\n",
      "Epoch 00200 | Train Loss 0.6091 | Val precision 43.4783% | Val recall 73.5294% | Val F1 54.6448%\n",
      "Epoch 00300 | Train Loss 0.6112 | Val precision 41.8367% | Val recall 80.3922% | Val F1 55.0336%\n",
      "Epoch 00400 | Train Loss 0.6035 | Val precision 44.5902% | Val recall 66.6667% | Val F1 53.4381%\n",
      "Epoch 00500 | Train Loss 0.6106 | Val precision 44.1270% | Val recall 68.1373% | Val F1 53.5645%\n",
      "Epoch 00600 | Train Loss 0.6052 | Val precision 43.7870% | Val recall 72.5490% | Val F1 54.6125%\n",
      "Epoch 00700 | Train Loss 0.6049 | Val precision 44.3114% | Val recall 72.5490% | Val F1 55.0186%\n",
      "Epoch 00800 | Train Loss 0.6037 | Val precision 44.3787% | Val recall 73.5294% | Val F1 55.3506%\n",
      "Epoch 00900 | Train Loss 0.6058 | Val precision 43.6047% | Val recall 73.5294% | Val F1 54.7445%\n",
      "Epoch 01000 | Train Loss 0.6036 | Val precision 43.9628% | Val recall 69.6078% | Val F1 53.8899%\n",
      "Epoch 00100 | Train Loss 0.6179 | Val precision 44.8598% | Val recall 70.5882% | Val F1 54.8571%\n",
      "Epoch 00200 | Train Loss 0.6090 | Val precision 43.7318% | Val recall 73.5294% | Val F1 54.8446%\n",
      "Epoch 00300 | Train Loss 0.6112 | Val precision 42.1189% | Val recall 79.9020% | Val F1 55.1607%\n",
      "Epoch 00400 | Train Loss 0.6035 | Val precision 44.4805% | Val recall 67.1569% | Val F1 53.5156%\n",
      "Epoch 00500 | Train Loss 0.6108 | Val precision 44.2675% | Val recall 68.1373% | Val F1 53.6680%\n",
      "Epoch 00600 | Train Loss 0.6050 | Val precision 43.5530% | Val recall 74.5098% | Val F1 54.9729%\n",
      "Epoch 00700 | Train Loss 0.6041 | Val precision 42.8969% | Val recall 75.4902% | Val F1 54.7069%\n",
      "Epoch 00800 | Train Loss 0.6033 | Val precision 44.6429% | Val recall 73.5294% | Val F1 55.5556%\n",
      "Epoch 00900 | Train Loss 0.6061 | Val precision 43.6047% | Val recall 73.5294% | Val F1 54.7445%\n",
      "Epoch 01000 | Train Loss 0.6038 | Val precision 43.9024% | Val recall 70.5882% | Val F1 54.1353%\n",
      "Epoch 00100 | Train Loss 0.6209 | Val precision 44.0994% | Val recall 69.6078% | Val F1 53.9924%\n",
      "Epoch 00200 | Train Loss 0.6138 | Val precision 43.1438% | Val recall 63.2353% | Val F1 51.2922%\n",
      "Epoch 00300 | Train Loss 0.6144 | Val precision 43.6416% | Val recall 74.0196% | Val F1 54.9091%\n",
      "Epoch 00400 | Train Loss 0.6079 | Val precision 43.3775% | Val recall 64.2157% | Val F1 51.7787%\n",
      "Epoch 00500 | Train Loss 0.6152 | Val precision 44.0860% | Val recall 60.2941% | Val F1 50.9317%\n",
      "Epoch 00600 | Train Loss 0.6086 | Val precision 43.6893% | Val recall 66.1765% | Val F1 52.6316%\n",
      "Epoch 00700 | Train Loss 0.6103 | Val precision 43.4650% | Val recall 70.0980% | Val F1 53.6585%\n",
      "Epoch 00800 | Train Loss 0.6104 | Val precision 43.2665% | Val recall 74.0196% | Val F1 54.6112%\n",
      "Epoch 00900 | Train Loss 0.6106 | Val precision 43.7700% | Val recall 67.1569% | Val F1 52.9981%\n",
      "Epoch 01000 | Train Loss 0.6107 | Val precision 42.0118% | Val recall 69.6078% | Val F1 52.3985%\n",
      "Epoch 00100 | Train Loss 0.6211 | Val precision 44.0994% | Val recall 69.6078% | Val F1 53.9924%\n",
      "Epoch 00200 | Train Loss 0.6138 | Val precision 43.0000% | Val recall 63.2353% | Val F1 51.1905%\n",
      "Epoch 00300 | Train Loss 0.6147 | Val precision 43.6416% | Val recall 74.0196% | Val F1 54.9091%\n",
      "Epoch 00400 | Train Loss 0.6078 | Val precision 43.3775% | Val recall 64.2157% | Val F1 51.7787%\n",
      "Epoch 00500 | Train Loss 0.6147 | Val precision 45.0704% | Val recall 62.7451% | Val F1 52.4590%\n",
      "Epoch 00600 | Train Loss 0.6086 | Val precision 43.6893% | Val recall 66.1765% | Val F1 52.6316%\n",
      "Epoch 00700 | Train Loss 0.6106 | Val precision 43.8066% | Val recall 71.0784% | Val F1 54.2056%\n",
      "Epoch 00800 | Train Loss 0.6116 | Val precision 41.7323% | Val recall 77.9412% | Val F1 54.3590%\n",
      "Epoch 00900 | Train Loss 0.6106 | Val precision 43.7107% | Val recall 68.1373% | Val F1 53.2567%\n",
      "Epoch 01000 | Train Loss 0.6100 | Val precision 43.2177% | Val recall 67.1569% | Val F1 52.5912%\n",
      "Epoch 00100 | Train Loss 0.6209 | Val precision 44.0994% | Val recall 69.6078% | Val F1 53.9924%\n",
      "Epoch 00200 | Train Loss 0.6138 | Val precision 43.1438% | Val recall 63.2353% | Val F1 51.2922%\n",
      "Epoch 00300 | Train Loss 0.6144 | Val precision 43.6416% | Val recall 74.0196% | Val F1 54.9091%\n",
      "Epoch 00400 | Train Loss 0.6078 | Val precision 43.3775% | Val recall 64.2157% | Val F1 51.7787%\n",
      "Epoch 00500 | Train Loss 0.6151 | Val precision 44.2857% | Val recall 60.7843% | Val F1 51.2397%\n",
      "Epoch 00600 | Train Loss 0.6086 | Val precision 43.6893% | Val recall 66.1765% | Val F1 52.6316%\n",
      "Epoch 00700 | Train Loss 0.6103 | Val precision 43.6364% | Val recall 70.5882% | Val F1 53.9326%\n",
      "Epoch 00800 | Train Loss 0.6105 | Val precision 43.2665% | Val recall 74.0196% | Val F1 54.6112%\n",
      "Epoch 00900 | Train Loss 0.6106 | Val precision 43.7700% | Val recall 67.1569% | Val F1 52.9981%\n",
      "Epoch 01000 | Train Loss 0.6105 | Val precision 41.9162% | Val recall 68.6275% | Val F1 52.0446%\n",
      "[tensor(0.5662), 0.3, 3, 0.2, 5e-05]\n"
     ]
    }
   ],
   "source": [
    "results_LP = grid_search_LP(learning_rate, pol_order,p_dropout,weight_decay)\n",
    "print(results_LP)\n",
    "\n",
    "# results with features transformed and  without standardization : \n",
    "# best_F1 = 0.5662, learning_rate = 0.3, pol_order = 3, p_dropout = 0.2, weight_decay = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_graph_filter_response(coeff: np.array, lam: np.ndarray):\n",
    "    \"\"\" \n",
    "        DESCRIPTION : Compute response of filtering using a polynomial filter \n",
    "        INPUT:\n",
    "            |--- coeff: [np.array] coeffiicients of polynomial filter\n",
    "            |--- lam: [np.ndarray] eigenvalues \n",
    "        OUTPUT:\n",
    "            |--- response: [np.ndarray] response[i] is the spectral response at frequency lam[i]\n",
    "    \"\"\"\n",
    "    V = np.vander(lam,coeff.shape[0],increasing=True)\n",
    "    response = V@coeff\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_decomposition(laplacian: np.ndarray):\n",
    "    \"\"\" \n",
    "        DESCRIPTION : Compute spectral decomposition of a graph using the graph Laplacian\n",
    "        INPUT:\n",
    "            |--- laplacian: [np.ndarray] graph laplacian \n",
    "        OUTPUT:\n",
    "            |--- lamb: [np.ndarray] containing graph eigenvalues\n",
    "            |--- U: [np.ndarray] containing corresponding graph eigenvectors\n",
    "    \"\"\"\n",
    "    # compute the eigenvalues and eigenvectors\n",
    "    if np.allclose(laplacian, laplacian.T, 1e-12):\n",
    "        lamb, U = np.linalg.eigh(laplacian)\n",
    "    else:\n",
    "        lamb, U = np.linalg.eig(laplacian)\n",
    "        #sort them\n",
    "        idx = np.argsort(lamb, axis=0)\n",
    "        lamb = lamb[idx]\n",
    "        U = U[:,idx]\n",
    "    \n",
    "    return lamb, U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_laplacian(adjacency: np.ndarray, normalize: bool):\n",
    "    \"\"\" \n",
    "        DESCRIPTION : Compute spectral decomposition of a graph using the graph Laplacian\n",
    "        INPUT:\n",
    "            |--- adjacency: [np.ndarray] adjacency matrix of the graph\n",
    "            |--- normalize: [bool] if normalize laplacian or not\n",
    "        OUTPUT:\n",
    "            |--- L: [n x n ndarray] combinatorial or symmetric normalized Laplacian. of the graph \n",
    "    \"\"\"\n",
    "    # degrees\n",
    "    I = np.identity(adjacency.shape[0])\n",
    "    degree = np.sum(adjacency, axis=1)\n",
    "    # Compute laplacian\n",
    "    D = I.copy()\n",
    "    np.fill_diagonal(D, degree)\n",
    "    L = D - adjacency\n",
    "    # normalized if requested \n",
    "    if normalize:\n",
    "        D12 = np.where(D > 0, np.power(D, -0.5, where=D>0), 0)\n",
    "        L = D12 @ L @ D12\n",
    "        \n",
    "    return L"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "L_norm = compute_laplacian(adjacency =adj_mat, normalize = True)\n",
    "lamb_, _ = spectral_decomposition(laplacian = L_norm)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.semilogy(lamb_,np.abs(polynomial_graph_filter_response(model.pol_weights.detach().numpy(), lamb_)))\n",
    "ax.set_xlabel('$\\lambda$')\n",
    "ax.set_ylabel('Spectral response (db)')\n",
    "ax.set_title('Spectral Response of Graph Filter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Graph Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model: Combine GraphConv layers first then two fully connected layers --> seems less stable over epochs\n",
    "class Linear_GNN(nn.Module):\n",
    "    def __init__(self, in_feats: int, out_feats: int, first_layer_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self._in_feats = in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self._first_layer_size = first_layer_size\n",
    "        self._hidden_size = hidden_size\n",
    "        \n",
    "        # Layers --> as much GraphConv as diameter --> reach everywhere\n",
    "        layer_size = 128\n",
    "        self.linear = nn.Linear(self._in_feats, self._first_layer_size)\n",
    "        self.gcn1 = dgl_nn.conv.GraphConv(self._first_layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn2 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn3 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn4 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn5 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn6 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn7 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn8 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn9 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn10 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn11 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.linear1 = nn.Linear(layer_size, self._hidden_size)\n",
    "        self.linear2 = nn.Linear(self._hidden_size, self._out_feats)\n",
    "        \n",
    "    def forward(self, graph, feat):\n",
    "        h = F.relu(self.linear(feat))\n",
    "        h = self.gcn1(graph, h)\n",
    "        h = self.gcn2(graph, h)\n",
    "        h = self.gcn3(graph, h)\n",
    "        h = self.gcn4(graph, h)\n",
    "        h = self.gcn5(graph, h)\n",
    "        h = self.gcn6(graph, h)\n",
    "        h = self.gcn7(graph, h)\n",
    "        h = self.gcn8(graph, h)\n",
    "        h = self.gcn9(graph, h)\n",
    "        h = self.gcn10(graph, h)\n",
    "        h = self.gcn11(graph, h)\n",
    "        h = self.linear1(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.linear2(h)\n",
    "        h = F.log_softmax(h, dim=1)\n",
    "        return h \n",
    "\n",
    "# Model : Only GraphConv layers --> seems more stable\n",
    "class Pure_GNN(nn.Module):\n",
    "    def __init__(self, in_feats: int, out_feats: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self._in_feats = in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self._hidden_size = hidden_size\n",
    "        \n",
    "        # Layers\n",
    "        self.gcn1 = dgl_nn.conv.GraphConv(self._in_feats, 32, activation=F.relu)\n",
    "        self.gcn2 = dgl_nn.conv.GraphConv(32, 32, activation=F.relu)\n",
    "        self.gcn3 = dgl_nn.conv.GraphConv(32, 32, activation=F.relu)\n",
    "        self.gcn4 = dgl_nn.conv.GraphConv(32, 32, activation=F.relu)\n",
    "        self.gcn5 = dgl_nn.conv.GraphConv(32, 32, activation=F.relu)\n",
    "        self.gcn6 = dgl_nn.conv.GraphConv(32, 32, activation=F.relu)\n",
    "        self.gcn7 = dgl_nn.conv.GraphConv(32, 32, activation=F.relu)\n",
    "        self.gcn8 = dgl_nn.conv.GraphConv(32, 32, activation=F.relu)\n",
    "        self.gcn9 = dgl_nn.conv.GraphConv(32, 32, activation=F.relu)\n",
    "        self.gcn10 = dgl_nn.conv.GraphConv(32, 32, activation=F.relu)\n",
    "        self.gcn11 = dgl_nn.conv.GraphConv(32,  self._out_feats, activation=None)\n",
    "        \n",
    "    def forward(self, graph, feat):\n",
    "        h = self.gcn1(graph, feat)\n",
    "        h = self.gcn2(graph, h)\n",
    "        h = self.gcn3(graph, h)\n",
    "        h = self.gcn4(graph, h)\n",
    "        h = self.gcn5(graph, h)\n",
    "        h = self.gcn6(graph, h)\n",
    "        h = self.gcn7(graph, h)\n",
    "        h = self.gcn8(graph, h)\n",
    "        h = self.gcn9(graph, h)\n",
    "        h = self.gcn10(graph, h)\n",
    "        h = self.gcn11(graph, h)\n",
    "        h = F.log_softmax(h, dim=1)\n",
    "        return h \n",
    "\n",
    "# model : Use and APPNP layer with k=7 (the network diameter) followed by 2 fully connected linears. \n",
    "class Simple_APPNP(nn.Module):\n",
    "    def __init__(self, in_feats: int, out_feats: int, hidden_size: int, k: int):\n",
    "        super().__init__()\n",
    "        self._k = k\n",
    "        self._in_feats = in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self._hidden_size = hidden_size\n",
    "        \n",
    "        # Layers\n",
    "        self.appnpconv1 = dgl_nn.conv.APPNPConv(self._k, 0.1, 0) #alpha teleport proba = 0.1 (cf paper)\n",
    "        self.linear1 = nn.Linear(self._hidden_size, self._hidden_size)\n",
    "        self.linear2 = nn.Linear(self._hidden_size, self._out_feats)\n",
    "        \n",
    "    def forward(self, graph, feat):\n",
    "        h = self.appnpconv1(graph, feat)\n",
    "        h = self.linear1(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.linear2(h)\n",
    "        h = F.log_softmax(h, dim=1)\n",
    "        return h \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check network diameter --> take a lot of time to run => diameter 11\n",
    "import networkx as nx\n",
    "Gnx = nx.from_numpy_array(adj_mat)\n",
    "G_large = max(nx.connected_component_subgraphs(Gnx), key=len)\n",
    "d = nx.diameter(G_large) \n",
    "print(f'diameter : {d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Graph NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed values\n",
    "in_feats = tensor_data.shape[1]\n",
    "out_feats = 2\n",
    "\n",
    "k = 11 # number of hop (how far to look) usually best to use the network diameter (according to paper)\n",
    "\n",
    "# Not relevant parameters\n",
    "n_epochs = 1000\n",
    "\n",
    "# To tune in the grid search\n",
    "learning_rate = [1e-3,1e-4,1e-5]\n",
    "first_layer_size = [16,32,64]\n",
    "hidden_size = [256,512]\n",
    "weight_decay = [0,5e-5,5e-6] # by default = 0\n",
    "\n",
    "# results with features transformed and  without standardization : \n",
    "#best F1 = 0.5138, learning_rate = 0.0001, first_layer =  16,  hidden_size = 512, weight_decay = 5e-06]\n",
    "\n",
    "\n",
    "#lr2 = 2e-5\n",
    "#lr3 = 8e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_NN(learning_rate, first_layer_size,hidden_size,weight_decay):\n",
    "    \n",
    "    performances = torch.zeros(len(learning_rate),len(first_layer_size),len(hidden_size),len(weight_decay))\n",
    "    for l, lr_ in enumerate(learning_rate):\n",
    "        for f, f_layer in enumerate(first_layer_size):\n",
    "            for h, hidden in enumerate(hidden_size):\n",
    "                for w, weight in enumerate(weight_decay):\n",
    "                        \n",
    "                    model = Linear_GNN(in_feats, out_feats, f_layer, hidden)\n",
    "\n",
    "                    loss_fcn = torch.nn.CrossEntropyLoss(weight=weights_loss)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(),lr=lr_, weight_decay=weight)\n",
    "                    losses_tr = []\n",
    "                        \n",
    "                    for epoch in range(n_epochs):\n",
    "                        loss = train(model, G, tensor_data, tensor_labels, train_mask, loss_fcn, optimizer)\n",
    "                        losses_tr.append(loss.item())\n",
    "                        pre, rec, f1, sup, C = evaluate(model, G, tensor_data, val_mask, tensor_labels) \n",
    "                        performances[l,f,h,w] = f1\n",
    "                        if (epoch+1)%100 == 0:\n",
    "                                print(\"Epoch {:05d} | Train Loss {:.4f} | Val precision {:.4%} | Val recall {:.4%} | Val F1 {:.4%}\". format(epoch+1, loss.item(), pre, rec, f1))\n",
    "                            \n",
    "\n",
    "        best_performance = torch.max(performances)\n",
    "        best_idx = (performances == best_performance).nonzero();\n",
    "            \n",
    "        best_lr = learning_rate[best_idx[0,0]]\n",
    "        best_first_layer = first_layer_size[best_idx[0,1]]\n",
    "        best_hidden_layer = hidden_size[best_idx[0,2]]\n",
    "        best_weight = weight_decay[best_idx[0,3]]\n",
    "                \n",
    "        results = [best_performance, best_lr, best_first_layer, best_hidden_layer,best_weight]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00100 | Train Loss 0.6812 | Val precision 49.2754% | Val recall 16.6667% | Val F1 24.9084%\n",
      "Epoch 00200 | Train Loss 0.6286 | Val precision 33.6283% | Val recall 74.5098% | Val F1 46.3415%\n",
      "Epoch 00300 | Train Loss 0.6006 | Val precision 38.5638% | Val recall 71.0784% | Val F1 50.0000%\n",
      "Epoch 00400 | Train Loss 0.6239 | Val precision 48.0916% | Val recall 30.8824% | Val F1 37.6119%\n",
      "Epoch 00500 | Train Loss 0.5971 | Val precision 40.7080% | Val recall 67.6471% | Val F1 50.8287%\n",
      "Epoch 00600 | Train Loss 0.5802 | Val precision 39.1421% | Val recall 71.5686% | Val F1 50.6066%\n",
      "Epoch 00700 | Train Loss 0.6141 | Val precision 35.9914% | Val recall 81.8627% | Val F1 50.0000%\n",
      "Epoch 00800 | Train Loss 0.5871 | Val precision 39.8352% | Val recall 71.0784% | Val F1 51.0563%\n",
      "Epoch 00900 | Train Loss 0.5695 | Val precision 39.7695% | Val recall 67.6471% | Val F1 50.0907%\n",
      "Epoch 01000 | Train Loss 0.5812 | Val precision 38.4040% | Val recall 75.4902% | Val F1 50.9091%\n",
      "Epoch 00100 | Train Loss 0.6526 | Val precision 34.7395% | Val recall 68.6275% | Val F1 46.1285%\n",
      "Epoch 00200 | Train Loss 0.6235 | Val precision 35.9102% | Val recall 70.5882% | Val F1 47.6033%\n",
      "Epoch 00300 | Train Loss 0.6331 | Val precision 34.3964% | Val recall 74.0196% | Val F1 46.9673%\n",
      "Epoch 00400 | Train Loss 0.6097 | Val precision 39.3750% | Val recall 61.7647% | Val F1 48.0916%\n",
      "Epoch 00500 | Train Loss 0.5926 | Val precision 37.3096% | Val recall 72.0588% | Val F1 49.1639%\n",
      "Epoch 00600 | Train Loss 0.6575 | Val precision 37.1429% | Val recall 50.9804% | Val F1 42.9752%\n",
      "Epoch 00700 | Train Loss 0.5915 | Val precision 38.9205% | Val recall 67.1569% | Val F1 49.2806%\n",
      "Epoch 00800 | Train Loss 0.5812 | Val precision 38.5714% | Val recall 66.1765% | Val F1 48.7365%\n",
      "Epoch 00900 | Train Loss 0.5740 | Val precision 40.1198% | Val recall 65.6863% | Val F1 49.8141%\n",
      "Epoch 01000 | Train Loss 0.5699 | Val precision 37.0370% | Val recall 73.5294% | Val F1 49.2611%\n",
      "Epoch 00100 | Train Loss 0.6465 | Val precision 35.2550% | Val recall 77.9412% | Val F1 48.5496%\n",
      "Epoch 00200 | Train Loss 0.6049 | Val precision 36.9620% | Val recall 71.5686% | Val F1 48.7479%\n",
      "Epoch 00300 | Train Loss 0.5963 | Val precision 36.3431% | Val recall 78.9216% | Val F1 49.7682%\n",
      "Epoch 00400 | Train Loss 0.6433 | Val precision 46.1140% | Val recall 43.6275% | Val F1 44.8363%\n",
      "Epoch 00500 | Train Loss 0.6127 | Val precision 33.3333% | Val recall 84.8039% | Val F1 47.8562%\n",
      "Epoch 00600 | Train Loss 0.5881 | Val precision 37.2881% | Val recall 75.4902% | Val F1 49.9190%\n",
      "Epoch 00700 | Train Loss 0.5795 | Val precision 39.0244% | Val recall 70.5882% | Val F1 50.2618%\n",
      "Epoch 00800 | Train Loss 0.5749 | Val precision 40.1216% | Val recall 64.7059% | Val F1 49.5310%\n",
      "Epoch 00900 | Train Loss 0.5946 | Val precision 37.7990% | Val recall 77.4510% | Val F1 50.8039%\n",
      "Epoch 01000 | Train Loss 0.5949 | Val precision 39.8827% | Val recall 66.6667% | Val F1 49.9083%\n",
      "Epoch 00100 | Train Loss 0.6393 | Val precision 28.5962% | Val recall 80.8824% | Val F1 42.2535%\n",
      "Epoch 00200 | Train Loss 0.6212 | Val precision 40.9253% | Val recall 56.3725% | Val F1 47.4227%\n",
      "Epoch 00300 | Train Loss 0.6054 | Val precision 37.3711% | Val recall 71.0784% | Val F1 48.9865%\n",
      "Epoch 00400 | Train Loss 0.6129 | Val precision 43.7956% | Val recall 58.8235% | Val F1 50.2092%\n",
      "Epoch 00500 | Train Loss 0.5823 | Val precision 40.6061% | Val recall 65.6863% | Val F1 50.1873%\n",
      "Epoch 00600 | Train Loss 0.5873 | Val precision 39.5722% | Val recall 72.5490% | Val F1 51.2111%\n",
      "Epoch 00700 | Train Loss 0.5857 | Val precision 39.8329% | Val recall 70.0980% | Val F1 50.7993%\n",
      "Epoch 00800 | Train Loss 0.5861 | Val precision 39.7727% | Val recall 68.6275% | Val F1 50.3597%\n",
      "Epoch 00900 | Train Loss 0.6048 | Val precision 45.0758% | Val recall 58.3333% | Val F1 50.8547%\n",
      "Epoch 01000 | Train Loss 0.6064 | Val precision 43.1579% | Val recall 60.2941% | Val F1 50.3067%\n",
      "Epoch 00100 | Train Loss 0.6621 | Val precision 28.5924% | Val recall 95.5882% | Val F1 44.0181%\n",
      "Epoch 00200 | Train Loss 0.6133 | Val precision 40.9556% | Val recall 58.8235% | Val F1 48.2897%\n",
      "Epoch 00300 | Train Loss 0.6013 | Val precision 37.4677% | Val recall 71.0784% | Val F1 49.0694%\n",
      "Epoch 00400 | Train Loss 0.5967 | Val precision 37.3057% | Val recall 70.5882% | Val F1 48.8136%\n",
      "Epoch 00500 | Train Loss 0.5914 | Val precision 37.5648% | Val recall 71.0784% | Val F1 49.1525%\n",
      "Epoch 00600 | Train Loss 0.5770 | Val precision 44.5255% | Val recall 59.8039% | Val F1 51.0460%\n",
      "Epoch 00700 | Train Loss 0.6831 | Val precision 32.8859% | Val recall 48.0392% | Val F1 39.0438%\n",
      "Epoch 00800 | Train Loss 0.6154 | Val precision 41.5584% | Val recall 47.0588% | Val F1 44.1379%\n",
      "Epoch 00900 | Train Loss 0.5876 | Val precision 39.3531% | Val recall 71.5686% | Val F1 50.7826%\n",
      "Epoch 01000 | Train Loss 0.5695 | Val precision 39.1931% | Val recall 66.6667% | Val F1 49.3648%\n",
      "Epoch 00100 | Train Loss 0.6837 | Val precision 26.6319% | Val recall 100.0000% | Val F1 42.0619%\n",
      "Epoch 00200 | Train Loss 0.6235 | Val precision 40.4082% | Val recall 48.5294% | Val F1 44.0980%\n",
      "Epoch 00300 | Train Loss 0.6081 | Val precision 39.8230% | Val recall 66.1765% | Val F1 49.7238%\n",
      "Epoch 00400 | Train Loss 0.5979 | Val precision 40.3458% | Val recall 68.6275% | Val F1 50.8167%\n",
      "Epoch 00500 | Train Loss 0.5915 | Val precision 38.0615% | Val recall 78.9216% | Val F1 51.3557%\n",
      "Epoch 00600 | Train Loss 0.5781 | Val precision 39.1858% | Val recall 75.4902% | Val F1 51.5913%\n",
      "Epoch 00700 | Train Loss 0.5810 | Val precision 40.2174% | Val recall 72.5490% | Val F1 51.7483%\n",
      "Epoch 00800 | Train Loss 0.5845 | Val precision 40.9836% | Val recall 73.5294% | Val F1 52.6316%\n",
      "Epoch 00900 | Train Loss 0.5604 | Val precision 39.5349% | Val recall 66.6667% | Val F1 49.6350%\n",
      "Epoch 01000 | Train Loss 0.6009 | Val precision 42.7451% | Val recall 53.4314% | Val F1 47.4946%\n",
      "Epoch 00100 | Train Loss 0.6917 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00200 | Train Loss 0.6832 | Val precision 26.5971% | Val recall 100.0000% | Val F1 42.0185%\n",
      "Epoch 00300 | Train Loss 0.6305 | Val precision 33.5697% | Val recall 69.6078% | Val F1 45.2951%\n",
      "Epoch 00400 | Train Loss 0.6085 | Val precision 37.0000% | Val recall 72.5490% | Val F1 49.0066%\n",
      "Epoch 00500 | Train Loss 0.6060 | Val precision 37.2960% | Val recall 78.4314% | Val F1 50.5529%\n",
      "Epoch 00600 | Train Loss 0.5956 | Val precision 38.4000% | Val recall 70.5882% | Val F1 49.7409%\n",
      "Epoch 00700 | Train Loss 0.5945 | Val precision 38.6740% | Val recall 68.6275% | Val F1 49.4700%\n",
      "Epoch 00800 | Train Loss 0.5973 | Val precision 38.4615% | Val recall 66.1765% | Val F1 48.6486%\n",
      "Epoch 00900 | Train Loss 0.5916 | Val precision 39.4286% | Val recall 67.6471% | Val F1 49.8195%\n",
      "Epoch 01000 | Train Loss 0.5990 | Val precision 35.7143% | Val recall 80.8824% | Val F1 49.5495%\n",
      "Epoch 00100 | Train Loss 0.6679 | Val precision 31.2625% | Val recall 76.4706% | Val F1 44.3812%\n",
      "Epoch 00200 | Train Loss 0.6176 | Val precision 37.0466% | Val recall 70.0980% | Val F1 48.4746%\n",
      "Epoch 00300 | Train Loss 0.6045 | Val precision 39.6166% | Val recall 60.7843% | Val F1 47.9691%\n",
      "Epoch 00400 | Train Loss 0.6147 | Val precision 36.9159% | Val recall 77.4510% | Val F1 50.0000%\n",
      "Epoch 00500 | Train Loss 0.5923 | Val precision 39.8714% | Val recall 60.7843% | Val F1 48.1553%\n",
      "Epoch 00600 | Train Loss 0.6178 | Val precision 36.9668% | Val recall 76.4706% | Val F1 49.8403%\n",
      "Epoch 00700 | Train Loss 0.5849 | Val precision 37.9902% | Val recall 75.9804% | Val F1 50.6536%\n",
      "Epoch 00800 | Train Loss 0.5884 | Val precision 42.4460% | Val recall 57.8431% | Val F1 48.9627%\n",
      "Epoch 00900 | Train Loss 0.5714 | Val precision 39.4203% | Val recall 66.6667% | Val F1 49.5446%\n",
      "Epoch 01000 | Train Loss 0.5676 | Val precision 38.9535% | Val recall 65.6863% | Val F1 48.9051%\n",
      "Epoch 00100 | Train Loss 0.6534 | Val precision 33.8542% | Val recall 63.7255% | Val F1 44.2177%\n",
      "Epoch 00200 | Train Loss 0.6172 | Val precision 35.4402% | Val recall 76.9608% | Val F1 48.5317%\n",
      "Epoch 00300 | Train Loss 0.6350 | Val precision 39.1691% | Val recall 64.7059% | Val F1 48.7985%\n",
      "Epoch 00400 | Train Loss 0.6043 | Val precision 35.4023% | Val recall 75.4902% | Val F1 48.2003%\n",
      "Epoch 00500 | Train Loss 0.5934 | Val precision 43.4263% | Val recall 53.4314% | Val F1 47.9121%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00600 | Train Loss 0.5872 | Val precision 38.3420% | Val recall 72.5490% | Val F1 50.1695%\n",
      "Epoch 00700 | Train Loss 0.6047 | Val precision 38.7006% | Val recall 67.1569% | Val F1 49.1039%\n",
      "Epoch 00800 | Train Loss 0.6910 | Val precision 28.9370% | Val recall 72.0588% | Val F1 41.2921%\n",
      "Epoch 00900 | Train Loss 0.6122 | Val precision 35.1351% | Val recall 70.0980% | Val F1 46.8085%\n",
      "Epoch 01000 | Train Loss 0.6007 | Val precision 37.1968% | Val recall 67.6471% | Val F1 48.0000%\n",
      "Epoch 00100 | Train Loss 0.6238 | Val precision 45.6044% | Val recall 40.6863% | Val F1 43.0052%\n",
      "Epoch 00200 | Train Loss 0.6034 | Val precision 35.7631% | Val recall 76.9608% | Val F1 48.8336%\n",
      "Epoch 00300 | Train Loss 0.6161 | Val precision 36.9775% | Val recall 56.3725% | Val F1 44.6602%\n",
      "Epoch 00400 | Train Loss 0.5935 | Val precision 36.7150% | Val recall 74.5098% | Val F1 49.1909%\n",
      "Epoch 00500 | Train Loss 0.5752 | Val precision 43.3333% | Val recall 63.7255% | Val F1 51.5873%\n",
      "Epoch 00600 | Train Loss 0.5881 | Val precision 39.4203% | Val recall 66.6667% | Val F1 49.5446%\n",
      "Epoch 00700 | Train Loss 0.5859 | Val precision 38.9646% | Val recall 70.0980% | Val F1 50.0876%\n",
      "Epoch 00800 | Train Loss 0.5669 | Val precision 39.4444% | Val recall 69.6078% | Val F1 50.3546%\n",
      "Epoch 00900 | Train Loss 0.5549 | Val precision 39.7059% | Val recall 66.1765% | Val F1 49.6324%\n",
      "Epoch 01000 | Train Loss 0.5518 | Val precision 42.0000% | Val recall 61.7647% | Val F1 50.0000%\n",
      "Epoch 00100 | Train Loss 0.6808 | Val precision 30.4569% | Val recall 88.2353% | Val F1 45.2830%\n",
      "Epoch 00200 | Train Loss 0.6171 | Val precision 36.5789% | Val recall 68.1373% | Val F1 47.6027%\n",
      "Epoch 00300 | Train Loss 0.6405 | Val precision 40.7080% | Val recall 45.0980% | Val F1 42.7907%\n",
      "Epoch 00400 | Train Loss 0.6009 | Val precision 39.8089% | Val recall 61.2745% | Val F1 48.2625%\n",
      "Epoch 00500 | Train Loss 0.5973 | Val precision 37.2197% | Val recall 81.3725% | Val F1 51.0769%\n",
      "Epoch 00600 | Train Loss 0.6010 | Val precision 34.7913% | Val recall 85.7843% | Val F1 49.5050%\n",
      "Epoch 00700 | Train Loss 0.6052 | Val precision 48.1928% | Val recall 39.2157% | Val F1 43.2432%\n",
      "Epoch 00800 | Train Loss 0.5881 | Val precision 41.3897% | Val recall 67.1569% | Val F1 51.2150%\n",
      "Epoch 00900 | Train Loss 0.5860 | Val precision 38.3333% | Val recall 78.9216% | Val F1 51.6026%\n",
      "Epoch 01000 | Train Loss 0.5782 | Val precision 39.6226% | Val recall 72.0588% | Val F1 51.1304%\n",
      "Epoch 00100 | Train Loss 0.6406 | Val precision 38.6986% | Val recall 55.3922% | Val F1 45.5645%\n",
      "Epoch 00200 | Train Loss 0.6077 | Val precision 39.6341% | Val recall 63.7255% | Val F1 48.8722%\n",
      "Epoch 00300 | Train Loss 0.6004 | Val precision 36.0092% | Val recall 76.9608% | Val F1 49.0625%\n",
      "Epoch 00400 | Train Loss 0.6094 | Val precision 36.9830% | Val recall 74.5098% | Val F1 49.4309%\n",
      "Epoch 00500 | Train Loss 0.5976 | Val precision 36.8664% | Val recall 78.4314% | Val F1 50.1567%\n",
      "Epoch 00600 | Train Loss 0.5921 | Val precision 36.9955% | Val recall 80.8824% | Val F1 50.7692%\n",
      "Epoch 00700 | Train Loss 0.6141 | Val precision 36.0802% | Val recall 79.4118% | Val F1 49.6172%\n",
      "Epoch 00800 | Train Loss 0.5941 | Val precision 39.0533% | Val recall 64.7059% | Val F1 48.7085%\n",
      "Epoch 00900 | Train Loss 0.5889 | Val precision 38.1313% | Val recall 74.0196% | Val F1 50.3333%\n",
      "Epoch 01000 | Train Loss 0.6585 | Val precision 28.0401% | Val recall 96.0784% | Val F1 43.4109%\n",
      "Epoch 00100 | Train Loss 0.6720 | Val precision 34.1935% | Val recall 25.9804% | Val F1 29.5265%\n",
      "Epoch 00200 | Train Loss 0.6374 | Val precision 33.4661% | Val recall 82.3529% | Val F1 47.5921%\n",
      "Epoch 00300 | Train Loss 0.6056 | Val precision 36.1607% | Val recall 79.4118% | Val F1 49.6933%\n",
      "Epoch 00400 | Train Loss 0.5933 | Val precision 36.6834% | Val recall 71.5686% | Val F1 48.5050%\n",
      "Epoch 00500 | Train Loss 0.5848 | Val precision 39.5349% | Val recall 66.6667% | Val F1 49.6350%\n",
      "Epoch 00600 | Train Loss 0.6255 | Val precision 36.9272% | Val recall 67.1569% | Val F1 47.6522%\n",
      "Epoch 00700 | Train Loss 0.5972 | Val precision 37.2152% | Val recall 72.0588% | Val F1 49.0818%\n",
      "Epoch 00800 | Train Loss 0.5834 | Val precision 37.1053% | Val recall 69.1176% | Val F1 48.2877%\n",
      "Epoch 00900 | Train Loss 0.6862 | Val precision 26.8882% | Val recall 87.2549% | Val F1 41.1085%\n",
      "Epoch 01000 | Train Loss 0.6838 | Val precision 26.6312% | Val recall 98.0392% | Val F1 41.8848%\n",
      "Epoch 00100 | Train Loss 0.6797 | Val precision 46.8468% | Val recall 25.4902% | Val F1 33.0159%\n",
      "Epoch 00200 | Train Loss 0.6245 | Val precision 36.0215% | Val recall 65.6863% | Val F1 46.5278%\n",
      "Epoch 00300 | Train Loss 0.6091 | Val precision 36.7758% | Val recall 71.5686% | Val F1 48.5857%\n",
      "Epoch 00400 | Train Loss 0.6011 | Val precision 36.4508% | Val recall 74.5098% | Val F1 48.9533%\n",
      "Epoch 00500 | Train Loss 0.6043 | Val precision 36.2445% | Val recall 81.3725% | Val F1 50.1511%\n",
      "Epoch 00600 | Train Loss 0.6042 | Val precision 40.9938% | Val recall 64.7059% | Val F1 50.1901%\n",
      "Epoch 00700 | Train Loss 0.5853 | Val precision 39.3048% | Val recall 72.0588% | Val F1 50.8651%\n",
      "Epoch 00800 | Train Loss 0.5776 | Val precision 44.4853% | Val recall 59.3137% | Val F1 50.8403%\n",
      "Epoch 00900 | Train Loss 0.5948 | Val precision 29.2763% | Val recall 87.2549% | Val F1 43.8424%\n",
      "Epoch 01000 | Train Loss 0.5830 | Val precision 41.3793% | Val recall 64.7059% | Val F1 50.4780%\n",
      "Epoch 00100 | Train Loss 0.6413 | Val precision 32.1663% | Val recall 72.0588% | Val F1 44.4781%\n",
      "Epoch 00200 | Train Loss 0.6300 | Val precision 35.3960% | Val recall 70.0980% | Val F1 47.0395%\n",
      "Epoch 00300 | Train Loss 0.5960 | Val precision 37.8299% | Val recall 63.2353% | Val F1 47.3394%\n",
      "Epoch 00400 | Train Loss 0.5901 | Val precision 39.5415% | Val recall 67.6471% | Val F1 49.9096%\n",
      "Epoch 00500 | Train Loss 0.5876 | Val precision 38.7597% | Val recall 73.5294% | Val F1 50.7614%\n",
      "Epoch 00600 | Train Loss 0.6072 | Val precision 40.5488% | Val recall 65.1961% | Val F1 50.0000%\n",
      "Epoch 00700 | Train Loss 0.6114 | Val precision 41.1765% | Val recall 54.9020% | Val F1 47.0588%\n",
      "Epoch 00800 | Train Loss 0.6078 | Val precision 39.8734% | Val recall 61.7647% | Val F1 48.4615%\n",
      "Epoch 00900 | Train Loss 0.5863 | Val precision 38.6598% | Val recall 73.5294% | Val F1 50.6757%\n",
      "Epoch 01000 | Train Loss 0.5814 | Val precision 38.0383% | Val recall 77.9412% | Val F1 51.1254%\n",
      "Epoch 00100 | Train Loss 0.6884 | Val precision 26.7300% | Val recall 96.5686% | Val F1 41.8704%\n",
      "Epoch 00200 | Train Loss 0.6932 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00300 | Train Loss 0.6826 | Val precision 29.8569% | Val recall 71.5686% | Val F1 42.1356%\n",
      "Epoch 00400 | Train Loss 0.6837 | Val precision 37.7193% | Val recall 42.1569% | Val F1 39.8148%\n",
      "Epoch 00500 | Train Loss 0.6923 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00600 | Train Loss 0.6944 | Val precision 31.1688% | Val recall 11.7647% | Val F1 17.0819%\n",
      "Epoch 00700 | Train Loss 0.6176 | Val precision 36.1979% | Val recall 68.1373% | Val F1 47.2789%\n",
      "Epoch 00800 | Train Loss 0.6042 | Val precision 40.3509% | Val recall 56.3725% | Val F1 47.0348%\n",
      "Epoch 00900 | Train Loss 0.5998 | Val precision 36.8286% | Val recall 70.5882% | Val F1 48.4034%\n",
      "Epoch 01000 | Train Loss 0.6345 | Val precision 43.2203% | Val recall 50.0000% | Val F1 46.3636%\n",
      "Epoch 00100 | Train Loss 0.6727 | Val precision 38.3212% | Val recall 51.4706% | Val F1 43.9331%\n",
      "Epoch 00200 | Train Loss 0.6806 | Val precision 32.1862% | Val recall 77.9412% | Val F1 45.5587%\n",
      "Epoch 00300 | Train Loss 0.6172 | Val precision 38.6301% | Val recall 69.1176% | Val F1 49.5606%\n",
      "Epoch 00400 | Train Loss 0.6041 | Val precision 40.0641% | Val recall 61.2745% | Val F1 48.4496%\n",
      "Epoch 00500 | Train Loss 0.6005 | Val precision 37.1359% | Val recall 75.0000% | Val F1 49.6753%\n",
      "Epoch 00600 | Train Loss 0.6219 | Val precision 42.0455% | Val recall 54.4118% | Val F1 47.4359%\n",
      "Epoch 00700 | Train Loss 0.6092 | Val precision 36.9615% | Val recall 79.9020% | Val F1 50.5426%\n",
      "Epoch 00800 | Train Loss 0.5917 | Val precision 37.8378% | Val recall 68.6275% | Val F1 48.7805%\n",
      "Epoch 00900 | Train Loss 0.6083 | Val precision 38.2550% | Val recall 83.8235% | Val F1 52.5346%\n",
      "Epoch 01000 | Train Loss 0.5809 | Val precision 39.4286% | Val recall 67.6471% | Val F1 49.8195%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00100 | Train Loss 0.6973 | Val precision 33.3333% | Val recall 0.9804% | Val F1 1.9048%\n",
      "Epoch 00200 | Train Loss 0.6502 | Val precision 39.1144% | Val recall 51.9608% | Val F1 44.6316%\n",
      "Epoch 00300 | Train Loss 0.6082 | Val precision 36.8146% | Val recall 69.1176% | Val F1 48.0409%\n",
      "Epoch 00400 | Train Loss 0.5969 | Val precision 39.1421% | Val recall 71.5686% | Val F1 50.6066%\n",
      "Epoch 00500 | Train Loss 0.5884 | Val precision 39.8876% | Val recall 69.6078% | Val F1 50.7143%\n",
      "Epoch 00600 | Train Loss 0.5891 | Val precision 40.4984% | Val recall 63.7255% | Val F1 49.5238%\n",
      "Epoch 00700 | Train Loss 0.5737 | Val precision 39.8176% | Val recall 64.2157% | Val F1 49.1557%\n",
      "Epoch 00800 | Train Loss 0.5735 | Val precision 40.2597% | Val recall 60.7843% | Val F1 48.4375%\n",
      "Epoch 00900 | Train Loss 0.5843 | Val precision 40.6154% | Val recall 64.7059% | Val F1 49.9055%\n",
      "Epoch 01000 | Train Loss 0.5687 | Val precision 45.0000% | Val recall 57.3529% | Val F1 50.4310%\n",
      "Epoch 00100 | Train Loss 0.6568 | Val precision 35.7664% | Val recall 48.0392% | Val F1 41.0042%\n",
      "Epoch 00200 | Train Loss 0.6299 | Val precision 34.7181% | Val recall 57.3529% | Val F1 43.2532%\n",
      "Epoch 00300 | Train Loss 0.6239 | Val precision 36.0882% | Val recall 64.2157% | Val F1 46.2081%\n",
      "Epoch 00400 | Train Loss 0.6138 | Val precision 36.2245% | Val recall 69.6078% | Val F1 47.6510%\n",
      "Epoch 00500 | Train Loss 0.6084 | Val precision 37.2973% | Val recall 67.6471% | Val F1 48.0836%\n",
      "Epoch 00600 | Train Loss 0.6052 | Val precision 35.2535% | Val recall 75.0000% | Val F1 47.9624%\n",
      "Epoch 00700 | Train Loss 0.6041 | Val precision 39.1566% | Val recall 63.7255% | Val F1 48.5075%\n",
      "Epoch 00800 | Train Loss 0.5999 | Val precision 34.4828% | Val recall 83.3333% | Val F1 48.7805%\n",
      "Epoch 00900 | Train Loss 0.5975 | Val precision 35.0840% | Val recall 81.8627% | Val F1 49.1176%\n",
      "Epoch 01000 | Train Loss 0.5970 | Val precision 39.3586% | Val recall 66.1765% | Val F1 49.3601%\n",
      "Epoch 00100 | Train Loss 0.6805 | Val precision 35.8974% | Val recall 54.9020% | Val F1 43.4109%\n",
      "Epoch 00200 | Train Loss 0.6515 | Val precision 41.6290% | Val recall 45.0980% | Val F1 43.2941%\n",
      "Epoch 00300 | Train Loss 0.6200 | Val precision 35.0365% | Val recall 70.5882% | Val F1 46.8293%\n",
      "Epoch 00400 | Train Loss 0.6127 | Val precision 36.2963% | Val recall 72.0588% | Val F1 48.2759%\n",
      "Epoch 00500 | Train Loss 0.6097 | Val precision 35.3488% | Val recall 74.5098% | Val F1 47.9495%\n",
      "Epoch 00600 | Train Loss 0.6086 | Val precision 37.1501% | Val recall 71.5686% | Val F1 48.9112%\n",
      "Epoch 00700 | Train Loss 0.6073 | Val precision 34.9206% | Val recall 75.4902% | Val F1 47.7519%\n",
      "Epoch 00800 | Train Loss 0.6051 | Val precision 36.2319% | Val recall 73.5294% | Val F1 48.5437%\n",
      "Epoch 00900 | Train Loss 0.6053 | Val precision 35.5556% | Val recall 78.4314% | Val F1 48.9297%\n",
      "Epoch 01000 | Train Loss 0.6029 | Val precision 38.1089% | Val recall 65.1961% | Val F1 48.1013%\n",
      "Epoch 00100 | Train Loss 0.6601 | Val precision 43.5644% | Val recall 43.1373% | Val F1 43.3498%\n",
      "Epoch 00200 | Train Loss 0.6337 | Val precision 38.7543% | Val recall 54.9020% | Val F1 45.4361%\n",
      "Epoch 00300 | Train Loss 0.6273 | Val precision 33.1263% | Val recall 78.4314% | Val F1 46.5793%\n",
      "Epoch 00400 | Train Loss 0.6147 | Val precision 36.1905% | Val recall 74.5098% | Val F1 48.7179%\n",
      "Epoch 00500 | Train Loss 0.6099 | Val precision 34.9272% | Val recall 82.3529% | Val F1 49.0511%\n",
      "Epoch 00600 | Train Loss 0.6099 | Val precision 34.2574% | Val recall 84.8039% | Val F1 48.8011%\n",
      "Epoch 00700 | Train Loss 0.6074 | Val precision 34.1948% | Val recall 84.3137% | Val F1 48.6563%\n",
      "Epoch 00800 | Train Loss 0.6031 | Val precision 35.9813% | Val recall 75.4902% | Val F1 48.7342%\n",
      "Epoch 00900 | Train Loss 0.6039 | Val precision 37.9939% | Val recall 61.2745% | Val F1 46.9043%\n",
      "Epoch 01000 | Train Loss 0.6009 | Val precision 36.1111% | Val recall 76.4706% | Val F1 49.0566%\n",
      "Epoch 00100 | Train Loss 0.6840 | Val precision 28.9520% | Val recall 79.9020% | Val F1 42.5033%\n",
      "Epoch 00200 | Train Loss 0.6601 | Val precision 33.7379% | Val recall 68.1373% | Val F1 45.1299%\n",
      "Epoch 00300 | Train Loss 0.6341 | Val precision 34.6883% | Val recall 62.7451% | Val F1 44.6771%\n",
      "Epoch 00400 | Train Loss 0.6242 | Val precision 34.3964% | Val recall 74.0196% | Val F1 46.9673%\n",
      "Epoch 00500 | Train Loss 0.6145 | Val precision 35.1415% | Val recall 73.0392% | Val F1 47.4522%\n",
      "Epoch 00600 | Train Loss 0.6495 | Val precision 30.5112% | Val recall 93.6275% | Val F1 46.0241%\n",
      "Epoch 00700 | Train Loss 0.6056 | Val precision 35.4762% | Val recall 73.0392% | Val F1 47.7564%\n",
      "Epoch 00800 | Train Loss 0.6259 | Val precision 36.4796% | Val recall 70.0980% | Val F1 47.9866%\n",
      "Epoch 00900 | Train Loss 0.6050 | Val precision 39.0909% | Val recall 63.2353% | Val F1 48.3146%\n",
      "Epoch 01000 | Train Loss 0.6021 | Val precision 35.2697% | Val recall 83.3333% | Val F1 49.5627%\n",
      "Epoch 00100 | Train Loss 0.6650 | Val precision 32.9759% | Val recall 60.2941% | Val F1 42.6343%\n",
      "Epoch 00200 | Train Loss 0.6284 | Val precision 35.7724% | Val recall 64.7059% | Val F1 46.0733%\n",
      "Epoch 00300 | Train Loss 0.6168 | Val precision 35.4610% | Val recall 73.5294% | Val F1 47.8469%\n",
      "Epoch 00400 | Train Loss 0.6098 | Val precision 38.4824% | Val recall 69.6078% | Val F1 49.5637%\n",
      "Epoch 00500 | Train Loss 0.6061 | Val precision 37.7778% | Val recall 66.6667% | Val F1 48.2270%\n",
      "Epoch 00600 | Train Loss 0.6029 | Val precision 37.6404% | Val recall 65.6863% | Val F1 47.8571%\n",
      "Epoch 00700 | Train Loss 0.6007 | Val precision 35.0763% | Val recall 78.9216% | Val F1 48.5671%\n",
      "Epoch 00800 | Train Loss 0.5993 | Val precision 37.5691% | Val recall 66.6667% | Val F1 48.0565%\n",
      "Epoch 00900 | Train Loss 0.6033 | Val precision 34.0637% | Val recall 83.8235% | Val F1 48.4419%\n",
      "Epoch 01000 | Train Loss 0.5954 | Val precision 37.3711% | Val recall 71.0784% | Val F1 48.9865%\n",
      "Epoch 00100 | Train Loss 0.6838 | Val precision 31.4220% | Val recall 67.1569% | Val F1 42.8125%\n",
      "Epoch 00200 | Train Loss 0.6302 | Val precision 35.3425% | Val recall 63.2353% | Val F1 45.3427%\n",
      "Epoch 00300 | Train Loss 0.6141 | Val precision 35.9897% | Val recall 68.6275% | Val F1 47.2175%\n",
      "Epoch 00400 | Train Loss 0.6080 | Val precision 38.5757% | Val recall 63.7255% | Val F1 48.0591%\n",
      "Epoch 00500 | Train Loss 0.6062 | Val precision 36.3043% | Val recall 81.8627% | Val F1 50.3012%\n",
      "Epoch 00600 | Train Loss 0.6017 | Val precision 38.2051% | Val recall 73.0392% | Val F1 50.1684%\n",
      "Epoch 00700 | Train Loss 0.6006 | Val precision 36.5801% | Val recall 82.8431% | Val F1 50.7508%\n",
      "Epoch 00800 | Train Loss 0.5958 | Val precision 37.1230% | Val recall 78.4314% | Val F1 50.3937%\n",
      "Epoch 00900 | Train Loss 0.5935 | Val precision 37.0023% | Val recall 77.4510% | Val F1 50.0792%\n",
      "Epoch 01000 | Train Loss 0.5929 | Val precision 37.4439% | Val recall 81.8627% | Val F1 51.3846%\n",
      "Epoch 00100 | Train Loss 0.6613 | Val precision 32.7500% | Val recall 64.2157% | Val F1 43.3775%\n",
      "Epoch 00200 | Train Loss 0.6300 | Val precision 34.9367% | Val recall 67.6471% | Val F1 46.0768%\n",
      "Epoch 00300 | Train Loss 0.6249 | Val precision 31.5254% | Val recall 91.1765% | Val F1 46.8514%\n",
      "Epoch 00400 | Train Loss 0.6087 | Val precision 39.4822% | Val recall 59.8039% | Val F1 47.5634%\n",
      "Epoch 00500 | Train Loss 0.6084 | Val precision 38.3784% | Val recall 69.6078% | Val F1 49.4774%\n",
      "Epoch 00600 | Train Loss 0.6017 | Val precision 36.0802% | Val recall 79.4118% | Val F1 49.6172%\n",
      "Epoch 00700 | Train Loss 0.6021 | Val precision 36.2812% | Val recall 78.4314% | Val F1 49.6124%\n",
      "Epoch 00800 | Train Loss 0.5995 | Val precision 39.4030% | Val recall 64.7059% | Val F1 48.9796%\n",
      "Epoch 00900 | Train Loss 0.5960 | Val precision 37.4083% | Val recall 75.0000% | Val F1 49.9184%\n",
      "Epoch 01000 | Train Loss 0.5986 | Val precision 34.1270% | Val recall 84.3137% | Val F1 48.5876%\n",
      "Epoch 00100 | Train Loss 0.6739 | Val precision 37.5465% | Val recall 49.5098% | Val F1 42.7061%\n",
      "Epoch 00200 | Train Loss 0.6365 | Val precision 35.3093% | Val recall 67.1569% | Val F1 46.2838%\n",
      "Epoch 00300 | Train Loss 0.6205 | Val precision 35.1621% | Val recall 69.1176% | Val F1 46.6116%\n",
      "Epoch 00400 | Train Loss 0.6133 | Val precision 36.6391% | Val recall 65.1961% | Val F1 46.9136%\n",
      "Epoch 00500 | Train Loss 0.6080 | Val precision 34.9451% | Val recall 77.9412% | Val F1 48.2549%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00600 | Train Loss 0.6086 | Val precision 35.4217% | Val recall 72.0588% | Val F1 47.4960%\n",
      "Epoch 00700 | Train Loss 0.6029 | Val precision 36.8831% | Val recall 69.6078% | Val F1 48.2173%\n",
      "Epoch 00800 | Train Loss 0.5991 | Val precision 37.0844% | Val recall 71.0784% | Val F1 48.7395%\n",
      "Epoch 00900 | Train Loss 0.5976 | Val precision 36.9231% | Val recall 70.5882% | Val F1 48.4848%\n",
      "Epoch 01000 | Train Loss 0.5968 | Val precision 35.2144% | Val recall 76.4706% | Val F1 48.2226%\n",
      "Epoch 00100 | Train Loss 0.6616 | Val precision 32.6754% | Val recall 73.0392% | Val F1 45.1515%\n",
      "Epoch 00200 | Train Loss 0.6239 | Val precision 39.5904% | Val recall 56.8627% | Val F1 46.6801%\n",
      "Epoch 00300 | Train Loss 0.6216 | Val precision 39.2739% | Val recall 58.3333% | Val F1 46.9428%\n",
      "Epoch 00400 | Train Loss 0.6103 | Val precision 38.2090% | Val recall 62.7451% | Val F1 47.4954%\n",
      "Epoch 00500 | Train Loss 0.6076 | Val precision 35.3075% | Val recall 75.9804% | Val F1 48.2115%\n",
      "Epoch 00600 | Train Loss 0.6085 | Val precision 39.0476% | Val recall 60.2941% | Val F1 47.3988%\n",
      "Epoch 00700 | Train Loss 0.6017 | Val precision 37.6119% | Val recall 61.7647% | Val F1 46.7532%\n",
      "Epoch 00800 | Train Loss 0.5977 | Val precision 38.0403% | Val recall 64.7059% | Val F1 47.9129%\n",
      "Epoch 00900 | Train Loss 0.5986 | Val precision 34.5188% | Val recall 80.8824% | Val F1 48.3871%\n",
      "Epoch 01000 | Train Loss 0.5961 | Val precision 37.5000% | Val recall 66.1765% | Val F1 47.8723%\n",
      "Epoch 00100 | Train Loss 0.6800 | Val precision 29.1057% | Val recall 87.7451% | Val F1 43.7118%\n",
      "Epoch 00200 | Train Loss 0.6375 | Val precision 37.4194% | Val recall 56.8627% | Val F1 45.1362%\n",
      "Epoch 00300 | Train Loss 0.6259 | Val precision 39.6491% | Val recall 55.3922% | Val F1 46.2168%\n",
      "Epoch 00400 | Train Loss 0.6184 | Val precision 39.0476% | Val recall 60.2941% | Val F1 47.3988%\n",
      "Epoch 00500 | Train Loss 0.6149 | Val precision 34.9425% | Val recall 74.5098% | Val F1 47.5743%\n",
      "Epoch 00600 | Train Loss 0.6119 | Val precision 34.9892% | Val recall 79.4118% | Val F1 48.5757%\n",
      "Epoch 00700 | Train Loss 0.6065 | Val precision 37.2449% | Val recall 71.5686% | Val F1 48.9933%\n",
      "Epoch 00800 | Train Loss 0.6065 | Val precision 36.0619% | Val recall 79.9020% | Val F1 49.6951%\n",
      "Epoch 00900 | Train Loss 0.6027 | Val precision 38.8235% | Val recall 64.7059% | Val F1 48.5294%\n",
      "Epoch 01000 | Train Loss 0.6135 | Val precision 33.1450% | Val recall 86.2745% | Val F1 47.8912%\n",
      "Epoch 00100 | Train Loss 0.6792 | Val precision 35.8974% | Val recall 54.9020% | Val F1 43.4109%\n",
      "Epoch 00200 | Train Loss 0.6279 | Val precision 34.6756% | Val recall 75.9804% | Val F1 47.6190%\n",
      "Epoch 00300 | Train Loss 0.6171 | Val precision 34.8416% | Val recall 75.4902% | Val F1 47.6780%\n",
      "Epoch 00400 | Train Loss 0.6110 | Val precision 34.3750% | Val recall 80.8824% | Val F1 48.2456%\n",
      "Epoch 00500 | Train Loss 0.6050 | Val precision 36.0360% | Val recall 78.4314% | Val F1 49.3827%\n",
      "Epoch 00600 | Train Loss 0.6044 | Val precision 35.2688% | Val recall 80.3922% | Val F1 49.0284%\n",
      "Epoch 00700 | Train Loss 0.6046 | Val precision 38.8406% | Val recall 65.6863% | Val F1 48.8160%\n",
      "Epoch 00800 | Train Loss 0.5985 | Val precision 35.8916% | Val recall 77.9412% | Val F1 49.1499%\n",
      "Epoch 00900 | Train Loss 0.5984 | Val precision 38.0165% | Val recall 67.6471% | Val F1 48.6772%\n",
      "Epoch 01000 | Train Loss 0.5943 | Val precision 38.2514% | Val recall 68.6275% | Val F1 49.1228%\n",
      "Epoch 00100 | Train Loss 0.6320 | Val precision 35.7558% | Val recall 60.2941% | Val F1 44.8905%\n",
      "Epoch 00200 | Train Loss 0.6180 | Val precision 34.3545% | Val recall 76.9608% | Val F1 47.5038%\n",
      "Epoch 00300 | Train Loss 0.6098 | Val precision 38.3562% | Val recall 68.6275% | Val F1 49.2091%\n",
      "Epoch 00400 | Train Loss 0.6080 | Val precision 35.5705% | Val recall 77.9412% | Val F1 48.8479%\n",
      "Epoch 00500 | Train Loss 0.6042 | Val precision 35.3448% | Val recall 80.3922% | Val F1 49.1018%\n",
      "Epoch 00600 | Train Loss 0.5982 | Val precision 37.8788% | Val recall 73.5294% | Val F1 50.0000%\n",
      "Epoch 00700 | Train Loss 0.6000 | Val precision 34.8269% | Val recall 83.8235% | Val F1 49.2086%\n",
      "Epoch 00800 | Train Loss 0.5951 | Val precision 37.4065% | Val recall 73.5294% | Val F1 49.5868%\n",
      "Epoch 00900 | Train Loss 0.5906 | Val precision 36.8550% | Val recall 73.5294% | Val F1 49.0998%\n",
      "Epoch 01000 | Train Loss 0.5902 | Val precision 35.7631% | Val recall 76.9608% | Val F1 48.8336%\n",
      "Epoch 00100 | Train Loss 0.6454 | Val precision 32.0093% | Val recall 67.1569% | Val F1 43.3544%\n",
      "Epoch 00200 | Train Loss 0.6304 | Val precision 37.3494% | Val recall 60.7843% | Val F1 46.2687%\n",
      "Epoch 00300 | Train Loss 0.6234 | Val precision 40.0697% | Val recall 56.3725% | Val F1 46.8432%\n",
      "Epoch 00400 | Train Loss 0.6183 | Val precision 40.4844% | Val recall 57.3529% | Val F1 47.4645%\n",
      "Epoch 00500 | Train Loss 0.6155 | Val precision 35.6979% | Val recall 76.4706% | Val F1 48.6739%\n",
      "Epoch 00600 | Train Loss 0.6080 | Val precision 37.8151% | Val recall 66.1765% | Val F1 48.1283%\n",
      "Epoch 00700 | Train Loss 0.6057 | Val precision 38.7931% | Val recall 66.1765% | Val F1 48.9130%\n",
      "Epoch 00800 | Train Loss 0.6058 | Val precision 37.4648% | Val recall 65.1961% | Val F1 47.5850%\n",
      "Epoch 00900 | Train Loss 0.6026 | Val precision 35.3183% | Val recall 84.3137% | Val F1 49.7829%\n",
      "Epoch 01000 | Train Loss 0.6020 | Val precision 34.9593% | Val recall 84.3137% | Val F1 49.4253%\n",
      "Epoch 00100 | Train Loss 0.6680 | Val precision 38.5666% | Val recall 55.3922% | Val F1 45.4728%\n",
      "Epoch 00200 | Train Loss 0.6276 | Val precision 32.5740% | Val recall 70.0980% | Val F1 44.4790%\n",
      "Epoch 00300 | Train Loss 0.6181 | Val precision 33.9535% | Val recall 71.5686% | Val F1 46.0568%\n",
      "Epoch 00400 | Train Loss 0.6161 | Val precision 37.2928% | Val recall 66.1765% | Val F1 47.7032%\n",
      "Epoch 00500 | Train Loss 0.6108 | Val precision 37.6045% | Val recall 66.1765% | Val F1 47.9574%\n",
      "Epoch 00600 | Train Loss 0.6112 | Val precision 38.6792% | Val recall 60.2941% | Val F1 47.1264%\n",
      "Epoch 00700 | Train Loss 0.6081 | Val precision 37.9518% | Val recall 61.7647% | Val F1 47.0149%\n",
      "Epoch 00800 | Train Loss 0.6059 | Val precision 36.1702% | Val recall 66.6667% | Val F1 46.8966%\n",
      "Epoch 00900 | Train Loss 0.6031 | Val precision 35.2697% | Val recall 83.3333% | Val F1 49.5627%\n",
      "Epoch 01000 | Train Loss 0.6035 | Val precision 34.5168% | Val recall 85.7843% | Val F1 49.2264%\n",
      "Epoch 00100 | Train Loss 0.6534 | Val precision 32.7913% | Val recall 59.3137% | Val F1 42.2339%\n",
      "Epoch 00200 | Train Loss 0.6233 | Val precision 36.2117% | Val recall 63.7255% | Val F1 46.1812%\n",
      "Epoch 00300 | Train Loss 0.6134 | Val precision 38.8889% | Val recall 65.1961% | Val F1 48.7179%\n",
      "Epoch 00400 | Train Loss 0.6120 | Val precision 38.6581% | Val recall 59.3137% | Val F1 46.8085%\n",
      "Epoch 00500 | Train Loss 0.6043 | Val precision 38.2114% | Val recall 69.1176% | Val F1 49.2147%\n",
      "Epoch 00600 | Train Loss 0.6017 | Val precision 38.7097% | Val recall 70.5882% | Val F1 50.0000%\n",
      "Epoch 00700 | Train Loss 0.6003 | Val precision 36.2416% | Val recall 79.4118% | Val F1 49.7696%\n",
      "Epoch 00800 | Train Loss 0.6037 | Val precision 33.2083% | Val recall 86.7647% | Val F1 48.0326%\n",
      "Epoch 00900 | Train Loss 0.5978 | Val precision 35.7456% | Val recall 79.9020% | Val F1 49.3939%\n",
      "Epoch 01000 | Train Loss 0.5969 | Val precision 38.8571% | Val recall 66.6667% | Val F1 49.0975%\n",
      "Epoch 00100 | Train Loss 0.6673 | Val precision 34.3465% | Val recall 55.3922% | Val F1 42.4015%\n",
      "Epoch 00200 | Train Loss 0.6276 | Val precision 34.4828% | Val recall 73.5294% | Val F1 46.9484%\n",
      "Epoch 00300 | Train Loss 0.6189 | Val precision 38.6503% | Val recall 61.7647% | Val F1 47.5472%\n",
      "Epoch 00400 | Train Loss 0.6191 | Val precision 39.6166% | Val recall 60.7843% | Val F1 47.9691%\n",
      "Epoch 00500 | Train Loss 0.6124 | Val precision 33.9216% | Val recall 84.8039% | Val F1 48.4594%\n",
      "Epoch 00600 | Train Loss 0.6065 | Val precision 38.6301% | Val recall 69.1176% | Val F1 49.5606%\n",
      "Epoch 00700 | Train Loss 0.6066 | Val precision 39.9340% | Val recall 59.3137% | Val F1 47.7318%\n",
      "Epoch 00800 | Train Loss 0.6061 | Val precision 35.9551% | Val recall 78.4314% | Val F1 49.3066%\n",
      "Epoch 00900 | Train Loss 0.6015 | Val precision 39.2962% | Val recall 65.6863% | Val F1 49.1743%\n",
      "Epoch 01000 | Train Loss 0.5973 | Val precision 38.3954% | Val recall 65.6863% | Val F1 48.4629%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00100 | Train Loss 0.6776 | Val precision 36.7003% | Val recall 53.4314% | Val F1 43.5130%\n",
      "Epoch 00200 | Train Loss 0.6367 | Val precision 33.9806% | Val recall 68.6275% | Val F1 45.4545%\n",
      "Epoch 00300 | Train Loss 0.6302 | Val precision 35.7309% | Val recall 75.4902% | Val F1 48.5039%\n",
      "Epoch 00400 | Train Loss 0.6233 | Val precision 36.4611% | Val recall 66.6667% | Val F1 47.1404%\n",
      "Epoch 00500 | Train Loss 0.6188 | Val precision 36.1364% | Val recall 77.9412% | Val F1 49.3789%\n",
      "Epoch 00600 | Train Loss 0.6093 | Val precision 37.8082% | Val recall 67.6471% | Val F1 48.5062%\n",
      "Epoch 00700 | Train Loss 0.6045 | Val precision 37.9032% | Val recall 69.1176% | Val F1 48.9583%\n",
      "Epoch 00800 | Train Loss 0.6057 | Val precision 39.1850% | Val recall 61.2745% | Val F1 47.8011%\n",
      "Epoch 00900 | Train Loss 0.5949 | Val precision 38.3598% | Val recall 71.0784% | Val F1 49.8282%\n",
      "Epoch 01000 | Train Loss 0.5925 | Val precision 38.1657% | Val recall 63.2353% | Val F1 47.6015%\n",
      "Epoch 00100 | Train Loss 0.6592 | Val precision 31.5294% | Val recall 65.6863% | Val F1 42.6073%\n",
      "Epoch 00200 | Train Loss 0.6315 | Val precision 30.9917% | Val recall 73.5294% | Val F1 43.6047%\n",
      "Epoch 00300 | Train Loss 0.6201 | Val precision 35.9173% | Val recall 68.1373% | Val F1 47.0389%\n",
      "Epoch 00400 | Train Loss 0.6161 | Val precision 35.5000% | Val recall 69.6078% | Val F1 47.0199%\n",
      "Epoch 00500 | Train Loss 0.6086 | Val precision 35.3349% | Val recall 75.0000% | Val F1 48.0377%\n",
      "Epoch 00600 | Train Loss 0.6062 | Val precision 37.9679% | Val recall 69.6078% | Val F1 49.1349%\n",
      "Epoch 00700 | Train Loss 0.6065 | Val precision 35.5204% | Val recall 76.9608% | Val F1 48.6068%\n",
      "Epoch 00800 | Train Loss 0.6020 | Val precision 36.2416% | Val recall 79.4118% | Val F1 49.7696%\n",
      "Epoch 00900 | Train Loss 0.6158 | Val precision 38.9408% | Val recall 61.2745% | Val F1 47.6190%\n",
      "Epoch 01000 | Train Loss 0.6003 | Val precision 37.0732% | Val recall 74.5098% | Val F1 49.5114%\n",
      "Epoch 00100 | Train Loss 0.6917 | Val precision 26.5359% | Val recall 99.5098% | Val F1 41.8989%\n",
      "Epoch 00200 | Train Loss 0.6916 | Val precision 26.5359% | Val recall 99.5098% | Val F1 41.8989%\n",
      "Epoch 00300 | Train Loss 0.6914 | Val precision 26.5013% | Val recall 99.5098% | Val F1 41.8557%\n",
      "Epoch 00400 | Train Loss 0.6911 | Val precision 26.5013% | Val recall 99.5098% | Val F1 41.8557%\n",
      "Epoch 00500 | Train Loss 0.6901 | Val precision 26.5359% | Val recall 99.5098% | Val F1 41.8989%\n",
      "Epoch 00600 | Train Loss 0.6852 | Val precision 27.2727% | Val recall 85.2941% | Val F1 41.3302%\n",
      "Epoch 00700 | Train Loss 0.6731 | Val precision 32.6027% | Val recall 58.3333% | Val F1 41.8278%\n",
      "Epoch 00800 | Train Loss 0.6512 | Val precision 32.6478% | Val recall 62.2549% | Val F1 42.8331%\n",
      "Epoch 00900 | Train Loss 0.6412 | Val precision 32.6582% | Val recall 63.2353% | Val F1 43.0718%\n",
      "Epoch 01000 | Train Loss 0.6362 | Val precision 32.5062% | Val recall 64.2157% | Val F1 43.1631%\n",
      "Epoch 00100 | Train Loss 0.6915 | Val precision 26.6399% | Val recall 97.5490% | Val F1 41.8507%\n",
      "Epoch 00200 | Train Loss 0.6912 | Val precision 26.5688% | Val recall 97.5490% | Val F1 41.7629%\n",
      "Epoch 00300 | Train Loss 0.6907 | Val precision 26.6667% | Val recall 98.0392% | Val F1 41.9287%\n",
      "Epoch 00400 | Train Loss 0.6894 | Val precision 26.8392% | Val recall 96.5686% | Val F1 42.0043%\n",
      "Epoch 00500 | Train Loss 0.6851 | Val precision 26.8868% | Val recall 83.8235% | Val F1 40.7143%\n",
      "Epoch 00600 | Train Loss 0.6765 | Val precision 35.8553% | Val recall 53.4314% | Val F1 42.9134%\n",
      "Epoch 00700 | Train Loss 0.6697 | Val precision 37.8571% | Val recall 51.9608% | Val F1 43.8017%\n",
      "Epoch 00800 | Train Loss 0.6632 | Val precision 36.6883% | Val recall 55.3922% | Val F1 44.1406%\n",
      "Epoch 00900 | Train Loss 0.6555 | Val precision 37.0031% | Val recall 59.3137% | Val F1 45.5744%\n",
      "Epoch 01000 | Train Loss 0.6496 | Val precision 34.6561% | Val recall 64.2157% | Val F1 45.0172%\n",
      "Epoch 00100 | Train Loss 0.6916 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00200 | Train Loss 0.6909 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00300 | Train Loss 0.6889 | Val precision 26.5971% | Val recall 100.0000% | Val F1 42.0185%\n",
      "Epoch 00400 | Train Loss 0.6836 | Val precision 29.1439% | Val recall 78.4314% | Val F1 42.4967%\n",
      "Epoch 00500 | Train Loss 0.6764 | Val precision 33.8889% | Val recall 59.8039% | Val F1 43.2624%\n",
      "Epoch 00600 | Train Loss 0.6698 | Val precision 33.4337% | Val recall 54.4118% | Val F1 41.4179%\n",
      "Epoch 00700 | Train Loss 0.6522 | Val precision 33.2447% | Val recall 61.2745% | Val F1 43.1034%\n",
      "Epoch 00800 | Train Loss 0.6395 | Val precision 33.5038% | Val recall 64.2157% | Val F1 44.0336%\n",
      "Epoch 00900 | Train Loss 0.6350 | Val precision 33.6735% | Val recall 64.7059% | Val F1 44.2953%\n",
      "Epoch 01000 | Train Loss 0.6319 | Val precision 33.5000% | Val recall 65.6863% | Val F1 44.3709%\n",
      "Epoch 00100 | Train Loss 0.6917 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00200 | Train Loss 0.6910 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00300 | Train Loss 0.6898 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00400 | Train Loss 0.6847 | Val precision 26.6667% | Val recall 100.0000% | Val F1 42.1053%\n",
      "Epoch 00500 | Train Loss 0.6702 | Val precision 36.1478% | Val recall 67.1569% | Val F1 46.9983%\n",
      "Epoch 00600 | Train Loss 0.6523 | Val precision 39.1608% | Val recall 54.9020% | Val F1 45.7143%\n",
      "Epoch 00700 | Train Loss 0.6372 | Val precision 39.3651% | Val recall 60.7843% | Val F1 47.7842%\n",
      "Epoch 00800 | Train Loss 0.6299 | Val precision 39.4904% | Val recall 60.7843% | Val F1 47.8764%\n",
      "Epoch 00900 | Train Loss 0.6253 | Val precision 39.2638% | Val recall 62.7451% | Val F1 48.3019%\n",
      "Epoch 01000 | Train Loss 0.6223 | Val precision 38.8379% | Val recall 62.2549% | Val F1 47.8343%\n",
      "Epoch 00100 | Train Loss 0.6917 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00200 | Train Loss 0.6914 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00300 | Train Loss 0.6909 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00400 | Train Loss 0.6888 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00500 | Train Loss 0.6785 | Val precision 29.9827% | Val recall 84.8039% | Val F1 44.3022%\n",
      "Epoch 00600 | Train Loss 0.6578 | Val precision 32.9700% | Val recall 59.3137% | Val F1 42.3818%\n",
      "Epoch 00700 | Train Loss 0.6453 | Val precision 34.5930% | Val recall 58.3333% | Val F1 43.4307%\n",
      "Epoch 00800 | Train Loss 0.6383 | Val precision 34.6479% | Val recall 60.2941% | Val F1 44.0072%\n",
      "Epoch 00900 | Train Loss 0.6325 | Val precision 35.0384% | Val recall 67.1569% | Val F1 46.0504%\n",
      "Epoch 01000 | Train Loss 0.6279 | Val precision 34.5771% | Val recall 68.1373% | Val F1 45.8746%\n",
      "Epoch 00100 | Train Loss 0.6917 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00200 | Train Loss 0.6915 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00300 | Train Loss 0.6914 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00400 | Train Loss 0.6913 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00500 | Train Loss 0.6911 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00600 | Train Loss 0.6904 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00700 | Train Loss 0.6841 | Val precision 27.7311% | Val recall 97.0588% | Val F1 43.1373%\n",
      "Epoch 00800 | Train Loss 0.6570 | Val precision 33.0969% | Val recall 68.6275% | Val F1 44.6571%\n",
      "Epoch 00900 | Train Loss 0.6384 | Val precision 34.8837% | Val recall 66.1765% | Val F1 45.6853%\n",
      "Epoch 01000 | Train Loss 0.6311 | Val precision 35.1064% | Val recall 64.7059% | Val F1 45.5172%\n",
      "Epoch 00100 | Train Loss 0.6915 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00200 | Train Loss 0.6901 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00300 | Train Loss 0.6851 | Val precision 27.0156% | Val recall 93.6275% | Val F1 41.9319%\n",
      "Epoch 00400 | Train Loss 0.6768 | Val precision 35.5372% | Val recall 63.2353% | Val F1 45.5026%\n",
      "Epoch 00500 | Train Loss 0.6691 | Val precision 37.6506% | Val recall 61.2745% | Val F1 46.6418%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00600 | Train Loss 0.6559 | Val precision 36.6197% | Val recall 63.7255% | Val F1 46.5116%\n",
      "Epoch 00700 | Train Loss 0.6328 | Val precision 38.8715% | Val recall 60.7843% | Val F1 47.4187%\n",
      "Epoch 00800 | Train Loss 0.6250 | Val precision 37.4670% | Val recall 69.6078% | Val F1 48.7136%\n",
      "Epoch 00900 | Train Loss 0.6216 | Val precision 38.7812% | Val recall 68.6275% | Val F1 49.5575%\n",
      "Epoch 01000 | Train Loss 0.6194 | Val precision 37.5959% | Val recall 72.0588% | Val F1 49.4118%\n",
      "Epoch 00100 | Train Loss 0.6914 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00200 | Train Loss 0.6899 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00300 | Train Loss 0.6837 | Val precision 27.0667% | Val recall 99.5098% | Val F1 42.5577%\n",
      "Epoch 00400 | Train Loss 0.6673 | Val precision 35.3887% | Val recall 64.7059% | Val F1 45.7539%\n",
      "Epoch 00500 | Train Loss 0.6407 | Val precision 38.3686% | Val recall 62.2549% | Val F1 47.4766%\n",
      "Epoch 00600 | Train Loss 0.6267 | Val precision 37.7465% | Val recall 65.6863% | Val F1 47.9428%\n",
      "Epoch 00700 | Train Loss 0.6206 | Val precision 35.5670% | Val recall 67.6471% | Val F1 46.6216%\n",
      "Epoch 00800 | Train Loss 0.6175 | Val precision 36.5979% | Val recall 69.6078% | Val F1 47.9730%\n",
      "Epoch 00900 | Train Loss 0.6155 | Val precision 36.9393% | Val recall 68.6275% | Val F1 48.0274%\n",
      "Epoch 01000 | Train Loss 0.6141 | Val precision 37.3684% | Val recall 69.6078% | Val F1 48.6301%\n",
      "Epoch 00100 | Train Loss 0.6914 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00200 | Train Loss 0.6909 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00300 | Train Loss 0.6897 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00400 | Train Loss 0.6852 | Val precision 26.5625% | Val recall 100.0000% | Val F1 41.9753%\n",
      "Epoch 00500 | Train Loss 0.6753 | Val precision 33.6043% | Val recall 60.7843% | Val F1 43.2810%\n",
      "Epoch 00600 | Train Loss 0.6590 | Val precision 33.7731% | Val recall 62.7451% | Val F1 43.9108%\n",
      "Epoch 00700 | Train Loss 0.6415 | Val precision 34.8649% | Val recall 63.2353% | Val F1 44.9477%\n",
      "Epoch 00800 | Train Loss 0.6313 | Val precision 37.7841% | Val recall 65.1961% | Val F1 47.8417%\n",
      "Epoch 00900 | Train Loss 0.6251 | Val precision 38.3234% | Val recall 62.7451% | Val F1 47.5836%\n",
      "Epoch 01000 | Train Loss 0.6208 | Val precision 38.0952% | Val recall 62.7451% | Val F1 47.4074%\n",
      "Epoch 00100 | Train Loss 0.6911 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00200 | Train Loss 0.6889 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00300 | Train Loss 0.6799 | Val precision 33.1002% | Val recall 69.6078% | Val F1 44.8657%\n",
      "Epoch 00400 | Train Loss 0.6641 | Val precision 34.0845% | Val recall 59.3137% | Val F1 43.2916%\n",
      "Epoch 00500 | Train Loss 0.6422 | Val precision 36.1823% | Val recall 62.2549% | Val F1 45.7658%\n",
      "Epoch 00600 | Train Loss 0.6266 | Val precision 38.1471% | Val recall 68.6275% | Val F1 49.0368%\n",
      "Epoch 00700 | Train Loss 0.6207 | Val precision 38.0952% | Val recall 66.6667% | Val F1 48.4848%\n",
      "Epoch 00800 | Train Loss 0.6179 | Val precision 37.5335% | Val recall 68.6275% | Val F1 48.5269%\n",
      "Epoch 00900 | Train Loss 0.6161 | Val precision 37.4670% | Val recall 69.6078% | Val F1 48.7136%\n",
      "Epoch 01000 | Train Loss 0.6147 | Val precision 36.9898% | Val recall 71.0784% | Val F1 48.6577%\n",
      "Epoch 00100 | Train Loss 0.6914 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00200 | Train Loss 0.6905 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00300 | Train Loss 0.6869 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00400 | Train Loss 0.6790 | Val precision 36.1022% | Val recall 55.3922% | Val F1 43.7137%\n",
      "Epoch 00500 | Train Loss 0.6726 | Val precision 37.2937% | Val recall 55.3922% | Val F1 44.5759%\n",
      "Epoch 00600 | Train Loss 0.6496 | Val precision 35.5556% | Val recall 62.7451% | Val F1 45.3901%\n",
      "Epoch 00700 | Train Loss 0.6330 | Val precision 36.6947% | Val recall 64.2157% | Val F1 46.7023%\n",
      "Epoch 00800 | Train Loss 0.6262 | Val precision 37.1758% | Val recall 63.2353% | Val F1 46.8240%\n",
      "Epoch 00900 | Train Loss 0.6219 | Val precision 36.7847% | Val recall 66.1765% | Val F1 47.2855%\n",
      "Epoch 01000 | Train Loss 0.6189 | Val precision 36.0313% | Val recall 67.6471% | Val F1 47.0187%\n",
      "Epoch 00100 | Train Loss 0.6915 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00200 | Train Loss 0.6913 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00300 | Train Loss 0.6910 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00400 | Train Loss 0.6899 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00500 | Train Loss 0.6834 | Val precision 27.8970% | Val recall 95.5882% | Val F1 43.1894%\n",
      "Epoch 00600 | Train Loss 0.6629 | Val precision 32.7751% | Val recall 67.1569% | Val F1 44.0514%\n",
      "Epoch 00700 | Train Loss 0.6479 | Val precision 33.2461% | Val recall 62.2549% | Val F1 43.3447%\n",
      "Epoch 00800 | Train Loss 0.6414 | Val precision 32.8244% | Val recall 63.2353% | Val F1 43.2161%\n",
      "Epoch 00900 | Train Loss 0.6352 | Val precision 32.7628% | Val recall 65.6863% | Val F1 43.7194%\n",
      "Epoch 01000 | Train Loss 0.6298 | Val precision 33.1683% | Val recall 65.6863% | Val F1 44.0789%\n",
      "Epoch 00100 | Train Loss 0.6915 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00200 | Train Loss 0.6909 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00300 | Train Loss 0.6886 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00400 | Train Loss 0.6817 | Val precision 29.4798% | Val recall 75.0000% | Val F1 42.3237%\n",
      "Epoch 00500 | Train Loss 0.6732 | Val precision 33.6986% | Val recall 60.2941% | Val F1 43.2337%\n",
      "Epoch 00600 | Train Loss 0.6602 | Val precision 31.7734% | Val recall 63.2353% | Val F1 42.2951%\n",
      "Epoch 00700 | Train Loss 0.6431 | Val precision 31.7778% | Val recall 70.0980% | Val F1 43.7309%\n",
      "Epoch 00800 | Train Loss 0.6368 | Val precision 32.9466% | Val recall 69.6078% | Val F1 44.7244%\n",
      "Epoch 00900 | Train Loss 0.6328 | Val precision 32.3462% | Val recall 69.6078% | Val F1 44.1680%\n",
      "Epoch 01000 | Train Loss 0.6298 | Val precision 34.6633% | Val recall 68.1373% | Val F1 45.9504%\n",
      "Epoch 00100 | Train Loss 0.6911 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00200 | Train Loss 0.6891 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00300 | Train Loss 0.6831 | Val precision 27.8956% | Val recall 83.8235% | Val F1 41.8605%\n",
      "Epoch 00400 | Train Loss 0.6744 | Val precision 34.9693% | Val recall 55.8824% | Val F1 43.0189%\n",
      "Epoch 00500 | Train Loss 0.6647 | Val precision 35.1906% | Val recall 58.8235% | Val F1 44.0367%\n",
      "Epoch 00600 | Train Loss 0.6502 | Val precision 34.8404% | Val recall 64.2157% | Val F1 45.1724%\n",
      "Epoch 00700 | Train Loss 0.6401 | Val precision 34.5960% | Val recall 67.1569% | Val F1 45.6667%\n",
      "Epoch 00800 | Train Loss 0.6325 | Val precision 34.7044% | Val recall 66.1765% | Val F1 45.5312%\n",
      "Epoch 00900 | Train Loss 0.6268 | Val precision 34.7150% | Val recall 65.6863% | Val F1 45.4237%\n",
      "Epoch 01000 | Train Loss 0.6227 | Val precision 34.7607% | Val recall 67.6471% | Val F1 45.9235%\n",
      "Epoch 00100 | Train Loss 0.6917 | Val precision 26.5522% | Val recall 98.5294% | Val F1 41.8314%\n",
      "Epoch 00200 | Train Loss 0.6914 | Val precision 26.5013% | Val recall 99.5098% | Val F1 41.8557%\n",
      "Epoch 00300 | Train Loss 0.6908 | Val precision 26.5013% | Val recall 99.5098% | Val F1 41.8557%\n",
      "Epoch 00400 | Train Loss 0.6888 | Val precision 26.5440% | Val recall 99.0196% | Val F1 41.8653%\n",
      "Epoch 00500 | Train Loss 0.6793 | Val precision 32.9060% | Val recall 75.4902% | Val F1 45.8333%\n",
      "Epoch 00600 | Train Loss 0.6560 | Val precision 33.8501% | Val recall 64.2157% | Val F1 44.3316%\n",
      "Epoch 00700 | Train Loss 0.6355 | Val precision 35.0685% | Val recall 62.7451% | Val F1 44.9912%\n",
      "Epoch 00800 | Train Loss 0.6279 | Val precision 35.3591% | Val recall 62.7451% | Val F1 45.2297%\n",
      "Epoch 00900 | Train Loss 0.6232 | Val precision 35.2785% | Val recall 65.1961% | Val F1 45.7831%\n",
      "Epoch 01000 | Train Loss 0.6203 | Val precision 35.9043% | Val recall 66.1765% | Val F1 46.5517%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00100 | Train Loss 0.6910 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00200 | Train Loss 0.6878 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00300 | Train Loss 0.6759 | Val precision 30.5243% | Val recall 79.9020% | Val F1 44.1734%\n",
      "Epoch 00400 | Train Loss 0.6537 | Val precision 31.8501% | Val recall 66.6667% | Val F1 43.1062%\n",
      "Epoch 00500 | Train Loss 0.6387 | Val precision 34.6479% | Val recall 60.2941% | Val F1 44.0072%\n",
      "Epoch 00600 | Train Loss 0.6321 | Val precision 35.0543% | Val recall 63.2353% | Val F1 45.1049%\n",
      "Epoch 00700 | Train Loss 0.6280 | Val precision 35.1562% | Val recall 66.1765% | Val F1 45.9184%\n",
      "Epoch 00800 | Train Loss 0.6246 | Val precision 35.4922% | Val recall 67.1569% | Val F1 46.4407%\n",
      "Epoch 00900 | Train Loss 0.6218 | Val precision 35.7326% | Val recall 68.1373% | Val F1 46.8803%\n",
      "Epoch 01000 | Train Loss 0.6198 | Val precision 36.6755% | Val recall 68.1373% | Val F1 47.6844%\n",
      "Epoch 00100 | Train Loss 0.6916 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00200 | Train Loss 0.6913 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00300 | Train Loss 0.6909 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00400 | Train Loss 0.6891 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00500 | Train Loss 0.6824 | Val precision 33.5681% | Val recall 70.0980% | Val F1 45.3968%\n",
      "Epoch 00600 | Train Loss 0.6720 | Val precision 37.6761% | Val recall 52.4510% | Val F1 43.8525%\n",
      "Epoch 00700 | Train Loss 0.6554 | Val precision 37.4214% | Val recall 58.3333% | Val F1 45.5939%\n",
      "Epoch 00800 | Train Loss 0.6394 | Val precision 36.4641% | Val recall 64.7059% | Val F1 46.6431%\n",
      "Epoch 00900 | Train Loss 0.6291 | Val precision 35.7895% | Val recall 66.6667% | Val F1 46.5753%\n",
      "Epoch 01000 | Train Loss 0.6233 | Val precision 35.6410% | Val recall 68.1373% | Val F1 46.8013%\n",
      "Epoch 00100 | Train Loss 0.6907 | Val precision 26.5280% | Val recall 100.0000% | Val F1 41.9322%\n",
      "Epoch 00200 | Train Loss 0.6872 | Val precision 26.7016% | Val recall 100.0000% | Val F1 42.1488%\n",
      "Epoch 00300 | Train Loss 0.6791 | Val precision 31.8841% | Val recall 64.7059% | Val F1 42.7184%\n",
      "Epoch 00400 | Train Loss 0.6689 | Val precision 33.3333% | Val recall 56.8627% | Val F1 42.0290%\n",
      "Epoch 00500 | Train Loss 0.6495 | Val precision 34.8315% | Val recall 60.7843% | Val F1 44.2857%\n",
      "Epoch 00600 | Train Loss 0.6373 | Val precision 33.8346% | Val recall 66.1765% | Val F1 44.7761%\n",
      "Epoch 00700 | Train Loss 0.6304 | Val precision 34.2640% | Val recall 66.1765% | Val F1 45.1505%\n",
      "Epoch 00800 | Train Loss 0.6252 | Val precision 35.5438% | Val recall 65.6863% | Val F1 46.1274%\n",
      "Epoch 00900 | Train Loss 0.6204 | Val precision 35.5330% | Val recall 68.6275% | Val F1 46.8227%\n",
      "Epoch 01000 | Train Loss 0.6167 | Val precision 35.3659% | Val recall 71.0784% | Val F1 47.2313%\n",
      "[tensor(0.5138), 0.0001, 16, 512, 5e-06]\n"
     ]
    }
   ],
   "source": [
    "results_NN = grid_search_NN(learning_rate, first_layer_size,hidden_size,weight_decay)\n",
    "print(results_NN)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model = Linear_GNN(in_feats, out_feats, first_layer_size, hidden_size)\n",
    "\n",
    "#model = Simple_APPNP(in_feats, out_feats, 6, k)\n",
    "\n",
    "loss_fcn = torch.nn.CrossEntropyLoss(weight=weights_loss)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr) #weight_decay=weight_decay)\n",
    "losses_tr = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # adapt learning rate\n",
    "    #if epoch == 100: \n",
    "        #for param_group in optimizer.param_groups:\n",
    "            #param_group['lr'] = lr2\n",
    "    #elif epoch == 300: \n",
    "        #for param_group in optimizer.param_groups:\n",
    "            #param_group['lr'] = lr3\n",
    "    \n",
    "    loss = train(model, G, tensor_data, tensor_labels, train_mask, loss_fcn, optimizer)\n",
    "    losses_tr.append(loss.item())\n",
    "    pre, rec, f1, sup, C = evaluate(model, G, tensor_data, val_mask, tensor_labels)\n",
    "    if (epoch+1)%50 == 0:\n",
    "        print(\"Epoch {:05d} | Train Loss {:.4f} | Val precision {:.4%} | Val recall {:.4%} | Val F1 {:.4%}\". format(epoch+1, loss.item(), pre, rec, f1))\n",
    "\n",
    "print()\n",
    "print('Test:')\n",
    "pre, rec, f1, sup, C = evaluate(model, G, tensor_data, test_mask, tensor_labels)\n",
    "confusion_matrix(C)\n",
    "print(\"Precision {:.4%} | Recall {:.4%} | F1 {:.4%}\". format(pre, rec, f1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Plot the loss\n",
    "fig, axs = plt.subplots(4,1,figsize=(6,8))\n",
    "ep = np.arange(1,n_epochs+1,1)\n",
    "axs[0].plot(ep[0:100], losses_tr[0:100], color='Goldenrod', linewidth=2)\n",
    "axs[0].plot(ep, np.zeros(ep.shape), color='black', linewidth=0.5)\n",
    "axs[0].set_xlim([0,100])\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].set_ylabel('Loss')\n",
    "\n",
    "axs[1].plot(ep[0:100], accuracies_tr[0:100], color='blue', linewidth=0.75)\n",
    "axs[1].plot(ep[0:100], accuracies_val[0:100], color='red', linewidth=2)\n",
    "axs[1].plot(ep, np.zeros(ep.shape), color='black', linewidth=0.5)\n",
    "axs[1].set_xlim([0,100])\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "\n",
    "axs[2].plot(ep[100:], losses_tr[100:], color='Goldenrod', linewidth=2)\n",
    "axs[2].plot(ep, np.zeros(ep.shape), color='black', linewidth=0.5)\n",
    "axs[2].set_xlim([100,n_epochs+1])\n",
    "axs[2].set_xlabel('epoch')\n",
    "axs[2].set_ylabel('Loss')\n",
    "\n",
    "axs[3].plot(ep[100:], accuracies_tr[100:], color='blue', linewidth=0.75)\n",
    "axs[3].plot(ep[100:], accuracies_val[100:], color='red', linewidth=2)\n",
    "axs[3].plot(ep, np.zeros(ep.shape), color='black', linewidth=0.5)\n",
    "axs[3].set_xlim([100,n_epochs+1])\n",
    "axs[3].set_xlabel('epoch')\n",
    "axs[3].set_ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of Graph CNN Optimisation : \n",
    "\n",
    "- APP Conv : with or without linear layer added generates a linearly decreasing loss and associated linearly decreasing accuracy with no good repartition of error between classes, difficulty to learn\n",
    "- GCNN without linear layers: more unstable results, no learning \n",
    "- GCNN with linear layers: linear layer at the beginning help stabilize and learn, a second linear layer in front doesn't create significant impact |no linear layer at the end no learning, second linear layer at the end reduces learning/less balanced errors, performance around 75%\n",
    "- Number of CNN layers: the addition of layers helps stabilize the learning accross epochs, when 4/5 layers less stables hence ~66% with more balanced, when 10/11 layers after 200 epochs very stales, errors not balanced at all\n",
    "- Add dropout, increase instability, when in a max -> strong acc, bad repartition, when in a min, the opposite => removed \n",
    "- Addition of a Avgpooling layer: no significant improvement on accuracy or error repartition \n",
    "- Hidden layer size for GCNN: if increase layer size increase creates instability but at a certain extend balances the errors in classes, around 60\n",
    "- Hidden layer size for final linear layer: 30, tradeoff with error-accuracy\n",
    "- Cross entropy + Soft Max give very unstable results over trials, converge to all 0 or all 1\n",
    "- NLL loss with log_sofmax gives very unstable results over epochs but reaches learning\n",
    "- Adding weights to loss function does generate improvement \n",
    "- BCE Loss not appropriate\n",
    "- Number of epochs, no need to go above 250, stabilisation around 200/250\n",
    "\n",
    "=> if stable , accuracy around 75%, very unbalanced errors\n",
    "=> if unstable (less layers, dropout ...), accuracy around 60%, more balanced errors \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA without feature selection not significant impact\n",
    "#pca = PCA(n_components=transformed_feat.shape[1])\n",
    "#transformed_feat.iloc[train_mask] = pca.fit_transform(transformed_feat.iloc[train_mask].to_numpy())\n",
    "#transformed_feat.iloc[val_mask] = pca.transform(transformed_feat.iloc[val_mask].to_numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
