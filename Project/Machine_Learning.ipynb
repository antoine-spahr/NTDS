{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HAS BEEN DONE**\n",
    "* Jakart vs Cosine vs Euclidian for features\n",
    "* Logistic regression simple et tuné + filtres\n",
    "* try different architecture -> testé combinaison de linear et GCNN / APPNP => GCNN avec une linear avant et deux linear apres = best\n",
    "\n",
    "\n",
    "**DONE 28.12.2019**\n",
    "\n",
    "* Trasformer some features because skewed OK\n",
    "* Use genre as feature as well OK \n",
    "* Grid search: learning rate, first_layer_size, hidden_size --> comparer en fonction du F1 sur le val set  OK\n",
    "* See/tune the impact of weight decay OK\n",
    "\n",
    "\n",
    "**TO DO**\n",
    "\n",
    "* Skip Connections ? \n",
    "* Try to compute a Random classifier, do it 20 times and see what is the F1 score in this case, or is the baseline we have enough\n",
    "* Recheck for standardization based on the F1 score\n",
    "* Explore different architectures?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, precision_score, precision_recall_fscore_support\n",
    "from sklearn.utils import resample\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import random\n",
    "\n",
    "import dgl.function as fn\n",
    "from dgl import DGLGraph\n",
    "import dgl.nn.pytorch as dgl_nn\n",
    "import dgl.transform as dgl_transform\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Data_path = 'Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the similarity matrix and generate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load similarity matrix\n",
    "file = open(Data_path + 'Adjacency_matrix_all025.pickle', 'rb')\n",
    "adj_mat =  pickle.load(file)\n",
    "\n",
    "# Generate graph\n",
    "G = DGLGraph(graph_data=adj_mat)\n",
    "G = dgl_transform.add_self_loop(G) # we are sure of doing this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "features_df = pd.read_csv(Data_path + 'features.csv',index_col=0).drop(columns = ['title'])\n",
    "\n",
    "# To consider also genres in the features comment this:\n",
    "#features_df = features_df[['budget', 'popularity', 'revenue', 'runtime', 'vote_average', 'vote_count']]\n",
    "\n",
    "\n",
    "# Labels\n",
    "labels_df = pd.read_csv(Data_path + 'labels.csv',index_col=0).drop(columns = ['title', ])\n",
    "IMDB_nom = labels_df['Nominations'].copy()\n",
    "IMDB_nom.loc[IMDB_nom > 0] = 1\n",
    "# Checking class imbalance\n",
    "IMDB_nom.value_counts() # 18.263 % of CLASS 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't want to transform the features don't run the following section and uncomment this:\n",
    "\n",
    "#transformed_features = features_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features engineering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# To visualize features before the transformation\n",
    "features_to_plot = features_df[['budget', 'popularity', 'revenue', 'runtime', 'vote_average', 'vote_count']]\n",
    "feat_names = ['budget', 'popularity', 'revenue', 'runtime', 'vote_average', 'vote_count']\n",
    "fig, axs = plt.subplots(3, 2,figsize=(15, 15))\n",
    "feat_mat = features_to_plot\n",
    "\n",
    "for name, ax in zip(feat_names, axs.reshape(-1)):\n",
    "    ax.hist(features_to_plot[name].values, bins=70,color='royalblue')\n",
    "    ax.set_title(name,size=15)\n",
    "    ax.set_ylim([0, 4810])\n",
    "    ax.set_xlabel('specific feat value',size=12)\n",
    "    ax.set_ylabel('number of samples',size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_transform(x):\n",
    "    trasformed_x = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        if x[i]>0:\n",
    "            trasformed_x[i] = np.log(x[i]) # could try also sqrt\n",
    "        else:\n",
    "            trasformed_x[i] = x[i]\n",
    "    return trasformed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only budget, popularity, revenue and vote_count are very skewed\n",
    "features_to_transform = features_df[['budget', 'popularity', 'revenue', 'vote_count']]\n",
    "feat_names = ['budget', 'popularity', 'revenue', 'vote_count']\n",
    "transformed_feat = features_df.copy()\n",
    "for name in feat_names:\n",
    "    transformed_feat[name] = feat_transform(features_to_transform[name])\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# To visualize features after the transformation\n",
    "trasformed_features_to_plot = transformed_feat[['budget', 'popularity', 'revenue', 'runtime', 'vote_average', 'vote_count']]\n",
    "feat_names = ['budget', 'popularity', 'revenue', 'runtime', 'vote_average', 'vote_count']\n",
    "fig, axs = plt.subplots(3, 2,figsize=(15, 15))\n",
    "feat_mat = features_to_plot\n",
    "\n",
    "for name, ax in zip(feat_names, axs.reshape(-1)):\n",
    "    ax.hist(trasformed_features_to_plot[name].values, bins=70,color='royalblue')\n",
    "    ax.set_title(name,size=15)\n",
    "    ax.set_ylim([0, 4810])\n",
    "    ax.set_xlabel('specific feat value',size=12)\n",
    "    ax.set_ylabel('number of samples',size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate masks and split train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss1 = StratifiedShuffleSplit(n_splits=1, train_size=0.8) #random_state=0\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, train_size=0.8) #random_state=0\n",
    "\n",
    "for prov_index, test_index in sss1.split(transformed_feat, IMDB_nom.values):\n",
    "    prov_mask = prov_index\n",
    "    test_mask = test_index\n",
    "\n",
    "for train_index, val_index in sss2.split(transformed_feat.iloc[prov_mask], IMDB_nom.values[prov_mask]):\n",
    "    train_mask = train_index\n",
    "    val_mask = val_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "tensor_data = torch.FloatTensor(transformed_feat.values)\n",
    "\n",
    "# labels\n",
    "tensor_labels = torch.LongTensor(IMDB_nom.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the data increases performances (73 -> 82), in terms of repartition bit more imbalanced\n",
    "# but still increase of performance for class 1 samples\n",
    "\n",
    "# To recheck this in terms of F1 score!!!!\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#transformed_feat.iloc[train_mask] = scaler.fit_transform(transformed_feat.iloc[train_mask].to_numpy())\n",
    "#transformed_feat.iloc[val_mask] = scaler.transform(transformed_feat.iloc[val_mask].to_numpy())\n",
    "#transformed_feat.iloc[test_mask] = scaler.transform(transformed_feat.iloc[test_mask].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C= 1,random_state = 0,solver = 'lbfgs').fit(transformed_feat.iloc[train_mask].to_numpy(),IMDB_nom.values[train_mask])\n",
    "train_pred = clf.predict(transformed_feat.iloc[train_mask].to_numpy())\n",
    "test_pred = clf.predict(transformed_feat.iloc[test_mask].to_numpy())\n",
    "\n",
    "tr_pre,tr_rec,tr_f1,tr_sup = precision_recall_fscore_support(train_pred,IMDB_nom.values[train_mask])\n",
    "print('Training set:')\n",
    "print('>>> Precision: {:0.4}'.format(tr_pre[1]))\n",
    "print('>>> Recall: {:0.4}'.format(tr_rec[1]))\n",
    "print('>>> F1: {:0.4}'.format(tr_f1[1]))\n",
    "print('>>> Support: {:}'.format(tr_sup[1]))\n",
    "print('')\n",
    "\n",
    "test_pre,test_rec,test_f1,test_sup = precision_recall_fscore_support(test_pred,IMDB_nom.values[test_mask])\n",
    "print('Test set:')\n",
    "print('>>> Precision: {:0.4}'.format(test_pre[1]))\n",
    "print('>>> Recall: {:0.4}'.format(test_rec[1]))\n",
    "print('>>> F1: {:0.4}'.format(test_f1[1]))\n",
    "print('>>> Support: {:}'.format(test_sup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix on validation classification\n",
    "#disp = sklearn.metrics.plot_confusion_matrix(clf, transformed_feat.iloc[val_mask],IMDB_nom.values[val_mask],cmap=plt.cm.Blues,display_labels = ['Not Nominated','Nominated'],normalize='true')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Logistic Regression & Graph Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix when classifier not available only confusion matrix\n",
    "def confusion_matrix(matrix):\n",
    "    figure = plt.figure()\n",
    "    axes = figure.add_subplot(111)\n",
    "    test = axes.matshow(matrix, cmap = plt.cm.get_cmap('Blues'));\n",
    "    axes.set_yticklabels(['','Not Nominated','Nominated'],style='italic')\n",
    "    axes.set_xticklabels(['','Not Nominated','Nominated'],style='italic')\n",
    "    axes.set_ylabel('True Label')\n",
    "    axes.set_xlabel('Predicted Label')\n",
    "    figure.colorbar(test)\n",
    "    for (j,i),label in np.ndenumerate(matrix):\n",
    "        axes.text(i,j,np.round(label,3),ha='center',va='center',color = 'grey')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplacianPolynomial(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats: int,\n",
    "                 out_feats: int,\n",
    "                 k: int,\n",
    "                 dropout_prob: float,\n",
    "                 norm=True):\n",
    "        super().__init__()\n",
    "        self._in_feats = in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self._k = k\n",
    "        self._norm = norm\n",
    "        # Contains the weights learned by the Laplacian polynomial\n",
    "        self.pol_weights = nn.Parameter(torch.Tensor(self._k + 1))\n",
    "        # Contains the weights learned by the logistic regression (without bias)\n",
    "        self.logr_weights = nn.Parameter(torch.Tensor(in_feats, out_feats))\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        torch.manual_seed(0)\n",
    "        torch.nn.init.xavier_uniform_(self.logr_weights, gain=0.01)\n",
    "        torch.nn.init.normal_(self.pol_weights, mean=0.0, std=1e-3)\n",
    "\n",
    "    def forward(self, graph, feat):\n",
    "        r\"\"\"Compute graph convolution.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        * Input shape: :math:`(N, *, \\text{in_feats})` where * means any number of additional\n",
    "          dimensions, :math:`N` is the number of nodes.\n",
    "        * Output shape: :math:`(N, *, \\text{out_feats})` where all but the last dimension are\n",
    "          the same shape as the input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph (DGLGraph) : The graph.\n",
    "        feat (torch.Tensor): The input feature\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (torch.Tensor) The output feature\n",
    "        \"\"\"\n",
    "        feat = self.dropout(feat)\n",
    "        graph = graph.local_var()\n",
    "        \n",
    "        # D^(-1/2)\n",
    "        norm = torch.pow(graph.in_degrees().float().clamp(min=1), -0.5)\n",
    "        shp = norm.shape + (1,) * (feat.dim() - 1)\n",
    "        norm = torch.reshape(norm, shp)\n",
    "\n",
    "        # mult W first to reduce the feature size for aggregation.\n",
    "        feat = torch.matmul(feat, self.logr_weights) # X*Teta\n",
    "\n",
    "        result = self.pol_weights[0] * feat.clone() # a0*L^0*X*Teta <-- fisrt polynomial weight a0 * L^0 * x\n",
    "\n",
    "        for i in range(1, self._k + 1): # get the next polynomial coefficient (a1*L^1, a2*L^2, ..... ak*L^k) \n",
    "            old_feat = feat.clone()\n",
    "            if self._norm:\n",
    "                feat = feat * norm\n",
    "            graph.ndata['h'] = feat\n",
    "            # Feat is not modified in place\n",
    "            graph.update_all(fn.copy_src(src='h', out='m'),\n",
    "                             fn.sum(msg='m', out='h')) # update all nodes with msg function copy_src (get data from source node) and reduce function sum\n",
    "            if self._norm:\n",
    "                graph.ndata['h'] = graph.ndata['h'] * norm\n",
    "\n",
    "            feat = old_feat - graph.ndata['h']\n",
    "            result += self.pol_weights[i] * feat\n",
    "\n",
    "        return result\n",
    "\n",
    "    def extra_repr(self):\n",
    "        \"\"\"Set the extra representation of the module,\n",
    "        which will come into effect when printing the model.\n",
    "        \"\"\"\n",
    "        summary = 'in={_in_feats}, out={_out_feats}'\n",
    "        summary += ', normalization={_norm}'\n",
    "        return summary.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, g, features, labels, train_mask, loss_fcn, optimizer):\n",
    "    \"\"\" \n",
    "    DESCRIPTION : Train and update model classification performances with training set\n",
    "    INPUT:\n",
    "        |--- model: [] classification model to train\n",
    "        |--- g: [DGLgraph] DeepGraphLearning graph object\n",
    "        |--- features: [FloatTensor] 2D tensor containing samples' features\n",
    "        |--- labels: [LongTensor] 1D tensor containing samples' labels (0-1)\n",
    "        |--- train_mask: [np.array] indices of training set\n",
    "        |--- loss_fcn: pytorch loss function chosen for model training\n",
    "        |--- optimizer: pytorch model optimizer \n",
    "    OUTPUT:\n",
    "        |--- loss: [float] value of loss function for the model at current state\n",
    "    \"\"\"\n",
    "    model.train()  \n",
    "    \n",
    "    pred = model(g, features)[train_mask] # prediction\n",
    "    loss = loss_fcn(pred, labels[train_mask])\n",
    "    optimizer.zero_grad()    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    #_, indices = torch.max(pred, dim=1)\n",
    "    #correct = torch.sum(indices == labels[train_mask])\n",
    "    #acc = correct.item() * 1.0 / len(labels[train_mask]) #not the best metric\n",
    "    \n",
    "    #C = sklearn.metrics.confusion_matrix(tensor_labels[train_mask], indices.numpy(), labels=[0,1], sample_weight=None, normalize='true')\n",
    "\n",
    "    #return loss, acc, C\n",
    "    return loss\n",
    "    \n",
    "def evaluate(model, g, features, mask, labels):\n",
    "    \"\"\" \n",
    "    DESCRIPTION : Evaluate model classification performance on validation set \n",
    "    INPUT:\n",
    "        |--- model: [] classification model to evaluate\n",
    "        |--- g: [DGLgraph] DeepGraphLearning graph object\n",
    "        |--- features: [FloatTensor] 2D tensor containing samples' features\n",
    "        |--- labels: [LongTensor] 1D tensor containing samples' labels (0-1)\n",
    "        |--- mask: [np.array] indices of validation set\n",
    "    OUTPUT:\n",
    "        |--- acc: [float] classification accuracy\n",
    "        |--- recall: [float] classification recall\n",
    "        |--- precision: [float] classification precision\n",
    "        |--- f1: [float] classification f1 score\n",
    "    \"\"\"\n",
    "    model.eval() \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(g, features)[mask]  # only compute the evaluation set\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(pred, dim=1)\n",
    "        \n",
    "        pre,rec,f1,sup = precision_recall_fscore_support(labels,indices.numpy())\n",
    "        #correct = torch.sum(indices == labels)\n",
    "        #acc = correct.item() * 1.0 / len(labels) #not the best metric\n",
    "        #f1 = f1_score(labels, indices)\n",
    "        #recall = recall_score(labels, indices)\n",
    "        #precision = precision_score(labels, indices)\n",
    "        \n",
    "        C = sklearn.metrics.confusion_matrix(labels, indices.numpy())\n",
    "        \n",
    "        return pre[1], rec[1], f1[1], sup[1], C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results of optimisation : \n",
    "\n",
    "Optimisation performed by looking at validation accuracy and distribution of errors across classes using confusion matrix\n",
    "- polynomial order : increase of the order tends to increase the instability of performances accross epochs, no strong impact on filter final shape -> tradeoff complexity/stability at 3\n",
    "- learning rate: small shift towards very unbalanced error, higher learning rate enabled to get a better trade-off between accuracy and distribution of error -> 0.2\n",
    "- number of epochs : event of strong instabilities across trials whatever parameters; strong instabilities allows better balance of errors but weaker accuracies; around 1500 period of stable learning\n",
    "- dropout : increase generates instabilities, tradeoff between accuracy and distribution of errors at 0.\n",
    "=> Final filter is basically always the same sort of shape as shown below"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pol_order = 3  # seems like a good trade off between performance and instability, increasing does give more\n",
    "lr = 0.3 # allows to get a good tradeoff between error distribution and accuracy\n",
    "weight_decay = 5e-6\n",
    "n_epochs = 500 \n",
    "p_dropout = 0.32  # 0.32 very good balancing, 75%, 0.3 balacing a bit less, 77%\n",
    "n_classes = 2\n",
    "in_feats=tensor_data.shape[1]\n",
    "\n",
    "true_ratio = 1074/4802 # <-- fraction of Nominations\n",
    "weights_loss = torch.FloatTensor([true_ratio, 1-true_ratio]) # to rebalance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model = LaplacianPolynomial(in_feats, n_classes, pol_order, p_dropout)\n",
    "\n",
    "loss_fcn = torch.nn.CrossEntropyLoss(weight=weights_loss)\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=lr,\n",
    "                             weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "accuracies_val = []\n",
    "accuracies_tr = []\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    loss = train(model, G, tensor_data, tensor_labels, train_mask, loss_fcn, optimizer)\n",
    "    losses.append(loss)\n",
    "    #accuracies_tr.append(acc)\n",
    "\n",
    "    pre, rec, f1, sup, C = evaluate(model, G, tensor_data, val_mask, tensor_labels)\n",
    "    #accuracies_val.append(acc)\n",
    "    if (epoch+1)%50 == 0:\n",
    "        print(\"Epoch {:05d} | Train Loss {:.4f} | Val precision {:.4%} | Val recall {:.4%} | Val F1 {:.4%}\". format(epoch+1, loss.item(), pre, rec, f1))\n",
    "\n",
    "print()\n",
    "print('Test:')\n",
    "pre, rec, f1, sup, C = evaluate(model, G, tensor_data, test_mask, tensor_labels)\n",
    "confusion_matrix(C)\n",
    "print(\"Precision {:.4%} | Recall {:.4%} | F1 {:.4%}\". format(pre, rec, f1))\n",
    "\n",
    "# instabilite au file des epochs -> more balanced, less performant"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Plot the loss\n",
    "fig, axs = plt.subplots(4,1,figsize=(6,8))\n",
    "ep = np.arange(1,n_epochs+1,1)\n",
    "axs[0].plot(ep[0:100], losses[0:100], color='Goldenrod', linewidth=2)\n",
    "axs[0].plot(ep, np.zeros(ep.shape), color='black', linewidth=0.5)\n",
    "axs[0].set_xlim([0,100])\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].set_ylabel('Loss')\n",
    "\n",
    "axs[1].plot(ep[0:100], accuracies_val[0:100], color='blue', linewidth=0.75)\n",
    "axs[1].plot(ep[0:100], accuracies_tr[0:100], color='red', linewidth=0.75)\n",
    "axs[1].plot(ep, np.zeros(ep.shape), color='black', linewidth=0.5)\n",
    "axs[1].set_xlim([0,100])\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].set_ylabel('Validation Accuracy')\n",
    "\n",
    "axs[2].plot(ep[100:], losses[100:], color='Goldenrod', linewidth=2)\n",
    "axs[2].plot(ep, np.zeros(ep.shape), color='black', linewidth=0.5)\n",
    "axs[2].set_xlim([100,n_epochs+1])\n",
    "axs[2].set_xlabel('epoch')\n",
    "axs[2].set_ylabel('Loss')\n",
    "\n",
    "axs[3].plot(ep[100:], accuracies_val[100:], color='blue', linewidth=0.75)\n",
    "axs[1].plot(ep[0:100], accuracies_tr[0:100], color='red', linewidth=0.75)\n",
    "axs[3].plot(ep, np.zeros(ep.shape), color='black', linewidth=0.5)\n",
    "axs[3].set_xlim([100,n_epochs+1])\n",
    "axs[3].set_xlabel('epoch')\n",
    "axs[3].set_ylabel('Validation Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning of Laplacian Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 500\n",
    "\n",
    "# To tune\n",
    "learning_rate = [1e-2,1e-3,1e-4,1e-5]\n",
    "pol_order = [2,3,4] \n",
    "p_dropout = [0.2,0.3,0.4] \n",
    "weight_decay = [0,5e-5,5e-6] # by default = 0\n",
    "\n",
    "\n",
    "n_classes = 2\n",
    "in_feats=tensor_data.shape[1]\n",
    "\n",
    "true_ratio = 1074/4802 # <-- fraction of Nominations\n",
    "weights_loss = torch.FloatTensor([true_ratio, 1-true_ratio]) # to rebalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_LP(learning_rate, pol_order,p_dropout,weight_decay):\n",
    "    \n",
    "\n",
    "    performances = torch.zeros(len(learning_rate),len(pol_order),len(p_dropout),len(weight_decay))\n",
    "    for l, lr_ in enumerate(learning_rate):\n",
    "        for p, p_order in enumerate(pol_order):\n",
    "            for d, dropout in enumerate(p_dropout):\n",
    "                for w, weight in enumerate(weight_decay):\n",
    "                        \n",
    "                    model = LaplacianPolynomial(in_feats, n_classes, p_order, dropout)\n",
    "\n",
    "                    loss_fcn = torch.nn.CrossEntropyLoss(weight=weights_loss)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(),lr=lr_, weight_decay = weight)\n",
    "                    losses_tr = []\n",
    "                        \n",
    "                    for epoch in range(n_epochs):\n",
    "                        loss = train(model, G, tensor_data, tensor_labels, train_mask, loss_fcn, optimizer)\n",
    "                        losses_tr.append(loss.item())\n",
    "                        pre, rec, f1, sup, C = evaluate(model, G, tensor_data, val_mask, tensor_labels) \n",
    "                        performances[l,p,d,w] = f1\n",
    "                        if (epoch+1)%200 == 0:\n",
    "                            print(\"Epoch {:05d} | Train Loss {:.4f} | Val precision {:.4%} | Val recall {:.4%} | Val F1 {:.4%}\". format(epoch+1, loss.item(), pre, rec, f1))\n",
    "                            \n",
    "\n",
    "        best_performance = torch.max(performances)\n",
    "        best_idx = (performances == best_performance).nonzero();\n",
    "            \n",
    "        best_lr = learning_rate[best_idx[0,0]]\n",
    "        best_p_order = pol_order[best_idx[0,1]]\n",
    "        best_dropout = p_dropout[best_idx[0,2]]\n",
    "        best_weight = weight_decay[best_idx[0,3]]\n",
    "                \n",
    "        results = [best_performance, best_lr,  best_p_order, best_dropout,best_weight]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_LP = grid_search_LP(learning_rate, pol_order,p_dropout,weight_decay)\n",
    "print(results_LP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_graph_filter_response(coeff: np.array, lam: np.ndarray):\n",
    "    \"\"\" \n",
    "        DESCRIPTION : Compute response of filtering using a polynomial filter \n",
    "        INPUT:\n",
    "            |--- coeff: [np.array] coeffiicients of polynomial filter\n",
    "            |--- lam: [np.ndarray] eigenvalues \n",
    "        OUTPUT:\n",
    "            |--- response: [np.ndarray] response[i] is the spectral response at frequency lam[i]\n",
    "    \"\"\"\n",
    "    V = np.vander(lam,coeff.shape[0],increasing=True)\n",
    "    response = V@coeff\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_decomposition(laplacian: np.ndarray):\n",
    "    \"\"\" \n",
    "        DESCRIPTION : Compute spectral decomposition of a graph using the graph Laplacian\n",
    "        INPUT:\n",
    "            |--- laplacian: [np.ndarray] graph laplacian \n",
    "        OUTPUT:\n",
    "            |--- lamb: [np.ndarray] containing graph eigenvalues\n",
    "            |--- U: [np.ndarray] containing corresponding graph eigenvectors\n",
    "    \"\"\"\n",
    "    # compute the eigenvalues and eigenvectors\n",
    "    if np.allclose(laplacian, laplacian.T, 1e-12):\n",
    "        lamb, U = np.linalg.eigh(laplacian)\n",
    "    else:\n",
    "        lamb, U = np.linalg.eig(laplacian)\n",
    "        #sort them\n",
    "        idx = np.argsort(lamb, axis=0)\n",
    "        lamb = lamb[idx]\n",
    "        U = U[:,idx]\n",
    "    \n",
    "    return lamb, U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_laplacian(adjacency: np.ndarray, normalize: bool):\n",
    "    \"\"\" \n",
    "        DESCRIPTION : Compute spectral decomposition of a graph using the graph Laplacian\n",
    "        INPUT:\n",
    "            |--- adjacency: [np.ndarray] adjacency matrix of the graph\n",
    "            |--- normalize: [bool] if normalize laplacian or not\n",
    "        OUTPUT:\n",
    "            |--- L: [n x n ndarray] combinatorial or symmetric normalized Laplacian. of the graph \n",
    "    \"\"\"\n",
    "    # degrees\n",
    "    I = np.identity(adjacency.shape[0])\n",
    "    degree = np.sum(adjacency, axis=1)\n",
    "    # Compute laplacian\n",
    "    D = I.copy()\n",
    "    np.fill_diagonal(D, degree)\n",
    "    L = D - adjacency\n",
    "    # normalized if requested \n",
    "    if normalize:\n",
    "        D12 = np.where(D > 0, np.power(D, -0.5, where=D>0), 0)\n",
    "        L = D12 @ L @ D12\n",
    "        \n",
    "    return L"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "L_norm = compute_laplacian(adjacency =adj_mat, normalize = True)\n",
    "lamb_, _ = spectral_decomposition(laplacian = L_norm)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.semilogy(lamb_,np.abs(polynomial_graph_filter_response(model.pol_weights.detach().numpy(), lamb_)))\n",
    "ax.set_xlabel('$\\lambda$')\n",
    "ax.set_ylabel('Spectral response (db)')\n",
    "ax.set_title('Spectral Response of Graph Filter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Graph Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model: Combine GraphConv layers first then two fully connected layers --> seems less stable over epochs\n",
    "class Linear_GNN(nn.Module):\n",
    "    def __init__(self, in_feats: int, out_feats: int, first_layer_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self._in_feats = in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self._first_layer_size = first_layer_size\n",
    "        self._hidden_size = hidden_size\n",
    "        \n",
    "        # Layers --> as much GraphConv as diameter --> reach everywhere\n",
    "        layer_size = 128\n",
    "        self.linear = nn.Linear(self._in_feats, self._first_layer_size)\n",
    "        self.gcn1 = dgl_nn.conv.GraphConv(self._first_layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn2 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn3 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn4 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn5 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn6 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn7 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn8 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn9 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn10 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.gcn11 = dgl_nn.conv.GraphConv(layer_size, layer_size, activation=F.relu)\n",
    "        self.linear1 = nn.Linear(layer_size, self._hidden_size)\n",
    "        self.linear2 = nn.Linear(self._hidden_size, self._out_feats)\n",
    "        \n",
    "    def forward(self, graph, feat):\n",
    "        h = F.relu(self.linear(feat))\n",
    "        h = self.gcn1(graph, h)\n",
    "        h = self.gcn2(graph, h)\n",
    "        h = self.gcn3(graph, h)\n",
    "        h = self.gcn4(graph, h)\n",
    "        h = self.gcn5(graph, h)\n",
    "        h = self.gcn6(graph, h)\n",
    "        h = self.gcn7(graph, h)\n",
    "        h = self.gcn8(graph, h)\n",
    "        h = self.gcn9(graph, h)\n",
    "        h = self.gcn10(graph, h)\n",
    "        h = self.gcn11(graph, h)\n",
    "        h = self.linear1(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.linear2(h)\n",
    "        h = F.log_softmax(h, dim=1)\n",
    "        return h \n",
    "\n",
    "# Model : Only GraphConv layers --> seems more stable\n",
    "class Pure_GNN(nn.Module):\n",
    "    def __init__(self, in_feats: int, out_feats: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self._in_feats = in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self._hidden_size = hidden_size\n",
    "        \n",
    "        # Layers\n",
    "        self.gcn1 = dgl_nn.conv.GraphConv(self._in_feats, 32, activation=F.relu)\n",
    "        self.gcn2 = dgl_nn.conv.GraphConv(32, 32, activation=F.relu)\n",
    "        self.gcn3 = dgl_nn.conv.GraphConv(32, 32, activation=F.relu)\n",
    "        self.gcn4 = dgl_nn.conv.GraphConv(32, 32, activation=F.relu)\n",
    "        self.gcn5 = dgl_nn.conv.GraphConv(32, 32, activation=F.relu)\n",
    "        self.gcn6 = dgl_nn.conv.GraphConv(32, 32, activation=F.relu)\n",
    "        self.gcn7 = dgl_nn.conv.GraphConv(32, 32, activation=F.relu)\n",
    "        self.gcn8 = dgl_nn.conv.GraphConv(32, 32, activation=F.relu)\n",
    "        self.gcn9 = dgl_nn.conv.GraphConv(32, 32, activation=F.relu)\n",
    "        self.gcn10 = dgl_nn.conv.GraphConv(32, 32, activation=F.relu)\n",
    "        self.gcn11 = dgl_nn.conv.GraphConv(32,  self._out_feats, activation=None)\n",
    "        \n",
    "    def forward(self, graph, feat):\n",
    "        h = self.gcn1(graph, feat)\n",
    "        h = self.gcn2(graph, h)\n",
    "        h = self.gcn3(graph, h)\n",
    "        h = self.gcn4(graph, h)\n",
    "        h = self.gcn5(graph, h)\n",
    "        h = self.gcn6(graph, h)\n",
    "        h = self.gcn7(graph, h)\n",
    "        h = self.gcn8(graph, h)\n",
    "        h = self.gcn9(graph, h)\n",
    "        h = self.gcn10(graph, h)\n",
    "        h = self.gcn11(graph, h)\n",
    "        h = F.log_softmax(h, dim=1)\n",
    "        return h \n",
    "\n",
    "# model : Use and APPNP layer with k=7 (the network diameter) followed by 2 fully connected linears. \n",
    "class Simple_APPNP(nn.Module):\n",
    "    def __init__(self, in_feats: int, out_feats: int, hidden_size: int, k: int):\n",
    "        super().__init__()\n",
    "        self._k = k\n",
    "        self._in_feats = in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self._hidden_size = hidden_size\n",
    "        \n",
    "        # Layers\n",
    "        self.appnpconv1 = dgl_nn.conv.APPNPConv(self._k, 0.1, 0) #alpha teleport proba = 0.1 (cf paper)\n",
    "        self.linear1 = nn.Linear(self._hidden_size, self._hidden_size)\n",
    "        self.linear2 = nn.Linear(self._hidden_size, self._out_feats)\n",
    "        \n",
    "    def forward(self, graph, feat):\n",
    "        h = self.appnpconv1(graph, feat)\n",
    "        h = self.linear1(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.linear2(h)\n",
    "        h = F.log_softmax(h, dim=1)\n",
    "        return h \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check network diameter --> take a lot of time to run => diameter 11\n",
    "import networkx as nx\n",
    "Gnx = nx.from_numpy_array(adj_mat)\n",
    "G_large = max(nx.connected_component_subgraphs(Gnx), key=len)\n",
    "d = nx.diameter(G_large) \n",
    "print(f'diameter : {d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed values\n",
    "in_feats = tensor_data.shape[1]\n",
    "out_feats = 2\n",
    "\n",
    "k = 11 # number of hop (how far to look) usually best to use the network diameter (according to paper)\n",
    "\n",
    "# Not relevant parameters\n",
    "n_epochs = 500\n",
    "\n",
    "# To tune in the grid search\n",
    "learning_rate = [1e-3,1e-4,1e-5]\n",
    "first_layer_size = [16,32,64]\n",
    "hidden_size = [256,512]\n",
    "weight_decay = [0,5e-5,5e-6] # by default = 0\n",
    "\n",
    "\n",
    "#lr2 = 2e-5\n",
    "#lr3 = 8e-6\n",
    "\n",
    "#p_dropout = 0 # for now not doing it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_NN(learning_rate, first_layer_size,hidden_size,weight_decay):\n",
    "    \n",
    "    performances = torch.zeros(len(learning_rate),len(first_layer_size),len(hidden_size),len(weight_decay))\n",
    "    for l, lr_ in enumerate(learning_rate):\n",
    "        for f, f_layer in enumerate(first_layer_size):\n",
    "            for h, hidden in enumerate(hidden_size):\n",
    "                for w, weight in enumerate(weight_decay):\n",
    "                        \n",
    "                    model = Linear_GNN(in_feats, out_feats, f_layer, hidden)\n",
    "\n",
    "                    loss_fcn = torch.nn.CrossEntropyLoss(weight=weights_loss)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(),lr=lr_, weight_decay=weight)\n",
    "                    losses_tr = []\n",
    "                        \n",
    "                    for epoch in range(n_epochs):\n",
    "                        loss = train(model, G, tensor_data, tensor_labels, train_mask, loss_fcn, optimizer)\n",
    "                        losses_tr.append(loss.item())\n",
    "                        pre, rec, f1, sup, C = evaluate(model, G, tensor_data, val_mask, tensor_labels) \n",
    "                        performances[l,f,h,w] = f1\n",
    "                        if (epoch+1)%200 == 0:\n",
    "                                print(\"Epoch {:05d} | Train Loss {:.4f} | Val precision {:.4%} | Val recall {:.4%} | Val F1 {:.4%}\". format(epoch+1, loss.item(), pre, rec, f1))\n",
    "                            \n",
    "\n",
    "        best_performance = torch.max(performances)\n",
    "        best_idx = (performances == best_performance).nonzero();\n",
    "            \n",
    "        best_lr = learning_rate[best_idx[0,0]]\n",
    "        best_first_layer = first_layer_size[best_idx[0,1]]\n",
    "        best_hidden_layer = hidden_size[best_idx[0,2]]\n",
    "        best_weight = weight_decay[best_idx[0,3]]\n",
    "                \n",
    "        results = [best_performance, best_lr, best_first_layer, best_hidden_layer,best_weight]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_NN = grid_search_NN(learning_rate, first_layer_size,hidden_size,weight_decay)\n",
    "print(results_NN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print()\n",
    "#print('Test:')\n",
    "#pre, rec, f1, sup, C = evaluate(model, G, tensor_data, test_mask, tensor_labels)\n",
    "#confusion_matrix(C)\n",
    "#print(\"Precision {:.4%} | Recall {:.4%} | F1 {:.4%}\". format(pre, rec, f1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model = Linear_GNN(in_feats, out_feats, first_layer_size, hidden_size)\n",
    "\n",
    "#model = Simple_APPNP(in_feats, out_feats, 6, k)\n",
    "\n",
    "loss_fcn = torch.nn.CrossEntropyLoss(weight=weights_loss)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr) #weight_decay=weight_decay)\n",
    "losses_tr = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # adapt learning rate\n",
    "    #if epoch == 100: \n",
    "        #for param_group in optimizer.param_groups:\n",
    "            #param_group['lr'] = lr2\n",
    "    #elif epoch == 300: \n",
    "        #for param_group in optimizer.param_groups:\n",
    "            #param_group['lr'] = lr3\n",
    "    \n",
    "    loss = train(model, G, tensor_data, tensor_labels, train_mask, loss_fcn, optimizer)\n",
    "    losses_tr.append(loss.item())\n",
    "    pre, rec, f1, sup, C = evaluate(model, G, tensor_data, val_mask, tensor_labels)\n",
    "    if (epoch+1)%50 == 0:\n",
    "        print(\"Epoch {:05d} | Train Loss {:.4f} | Val precision {:.4%} | Val recall {:.4%} | Val F1 {:.4%}\". format(epoch+1, loss.item(), pre, rec, f1))\n",
    "\n",
    "print()\n",
    "print('Test:')\n",
    "pre, rec, f1, sup, C = evaluate(model, G, tensor_data, test_mask, tensor_labels)\n",
    "confusion_matrix(C)\n",
    "print(\"Precision {:.4%} | Recall {:.4%} | F1 {:.4%}\". format(pre, rec, f1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Plot the loss\n",
    "fig, axs = plt.subplots(4,1,figsize=(6,8))\n",
    "ep = np.arange(1,n_epochs+1,1)\n",
    "axs[0].plot(ep[0:100], losses_tr[0:100], color='Goldenrod', linewidth=2)\n",
    "axs[0].plot(ep, np.zeros(ep.shape), color='black', linewidth=0.5)\n",
    "axs[0].set_xlim([0,100])\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].set_ylabel('Loss')\n",
    "\n",
    "axs[1].plot(ep[0:100], accuracies_tr[0:100], color='blue', linewidth=0.75)\n",
    "axs[1].plot(ep[0:100], accuracies_val[0:100], color='red', linewidth=2)\n",
    "axs[1].plot(ep, np.zeros(ep.shape), color='black', linewidth=0.5)\n",
    "axs[1].set_xlim([0,100])\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "\n",
    "axs[2].plot(ep[100:], losses_tr[100:], color='Goldenrod', linewidth=2)\n",
    "axs[2].plot(ep, np.zeros(ep.shape), color='black', linewidth=0.5)\n",
    "axs[2].set_xlim([100,n_epochs+1])\n",
    "axs[2].set_xlabel('epoch')\n",
    "axs[2].set_ylabel('Loss')\n",
    "\n",
    "axs[3].plot(ep[100:], accuracies_tr[100:], color='blue', linewidth=0.75)\n",
    "axs[3].plot(ep[100:], accuracies_val[100:], color='red', linewidth=2)\n",
    "axs[3].plot(ep, np.zeros(ep.shape), color='black', linewidth=0.5)\n",
    "axs[3].set_xlim([100,n_epochs+1])\n",
    "axs[3].set_xlabel('epoch')\n",
    "axs[3].set_ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of Graph CNN Optimisation : \n",
    "\n",
    "- APP Conv : with or without linear layer added generates a linearly decreasing loss and associated linearly decreasing accuracy with no good repartition of error between classes, difficulty to learn\n",
    "- GCNN without linear layers: more unstable results, no learning \n",
    "- GCNN with linear layers: linear layer at the beginning help stabilize and learn, a second linear layer in front doesn't create significant impact |no linear layer at the end no learning, second linear layer at the end reduces learning/less balanced errors, performance around 75%\n",
    "- Number of CNN layers: the addition of layers helps stabilize the learning accross epochs, when 4/5 layers less stables hence ~66% with more balanced, when 10/11 layers after 200 epochs very stales, errors not balanced at all\n",
    "- Add dropout, increase instability, when in a max -> strong acc, bad repartition, when in a min, the opposite => removed \n",
    "- Addition of a Avgpooling layer: no significant improvement on accuracy or error repartition \n",
    "- Hidden layer size for GCNN: if increase layer size increase creates instability but at a certain extend balances the errors in classes, around 60\n",
    "- Hidden layer size for final linear layer: 30, tradeoff with error-accuracy\n",
    "- Cross entropy + Soft Max give very unstable results over trials, converge to all 0 or all 1\n",
    "- NLL loss with log_sofmax gives very unstable results over epochs but reaches learning\n",
    "- Adding weights to loss function does generate improvement \n",
    "- BCE Loss not appropriate\n",
    "- Number of epochs, no need to go above 250, stabilisation around 200/250\n",
    "\n",
    "=> if stable , accuracy around 75%, very unbalanced errors\n",
    "=> if unstable (less layers, dropout ...), accuracy around 60%, more balanced errors \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not using it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA without feature selection not significant impact\n",
    "#pca = PCA(n_components=transformed_feat.shape[1])\n",
    "#transformed_feat.iloc[train_mask] = pca.fit_transform(transformed_feat.iloc[train_mask].to_numpy())\n",
    "#transformed_feat.iloc[val_mask] = pca.transform(ftransformed_feat.iloc[val_mask].to_numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
