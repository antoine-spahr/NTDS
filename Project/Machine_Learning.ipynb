{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import random\n",
    "\n",
    "import dgl.function as fn\n",
    "from dgl import DGLGraph\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the similarity matrices and generate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading keywords similarity matrix with shape (4802, 4802)\n",
      ">>> Loading genre similarity matrix with shape (4802, 4802)\n",
      ">>> Loading crew similarity matrix with shape (4802, 4802)\n",
      ">>> Loading cast similarity matrix with shape (4802, 4802)\n"
     ]
    }
   ],
   "source": [
    "Data_path = 'Data/'\n",
    "\n",
    "sim_mat = {}\n",
    "names = ['keywords', 'genre', 'crew', 'cast']\n",
    "for name in names:\n",
    "    with open(Data_path+'csim_'+name, 'rb') as src:\n",
    "        sim_mat[name] = pickle.load(src)\n",
    "        print(f'>>> Loading {name} similarity matrix with shape {sim_mat[name].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative 1** Combine the 4 similarity matrices equitably and pruned those with a similarity below 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple way\n",
    "adj_mat = sim_mat['keywords'].copy()\n",
    "for name in names[1:]:\n",
    "    adj_mat = adj_mat.add(sim_mat[name], fill_value=0)\n",
    "adj_mat = adj_mat/4    \n",
    "\n",
    "threshold = 0.25\n",
    "adj_mat[adj_mat < threshold] = 0\n",
    "\n",
    "# Generate graph\n",
    "G = DGLGraph(graph_data=adj_mat.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['budget','popularity','revenue','runtime','vote_average','vote_count','Nominations_GoldenGlobes']\n",
    "\n",
    "IMDB_path = Data_path + 'nodes_attributes.csv'\n",
    "IMDB = pd.read_csv(IMDB_path)\n",
    "IMDB.drop(columns = ['Unnamed: 0','id'], inplace=True)\n",
    "IMDB.set_index('title',inplace=True)\n",
    "\n",
    "# features\n",
    "IMDB_feat = IMDB[features]\n",
    "tensor_feat = torch.FloatTensor(IMDB_feat.values)\n",
    "\n",
    "# labels\n",
    "IMDB_nom = IMDB['Nominations_Oscars'].copy()\n",
    "IMDB_nom.loc[IMDB_nom > 0] = 1\n",
    "tensor_nom = torch.LongTensor(IMDB_nom.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, train_size=0.8, random_state=0)\n",
    "\n",
    "for prov_index, test_index in sss.split(tensor_feat, tensor_nom):\n",
    "    prov_mask = prov_index\n",
    "    test_mask = test_index\n",
    "\n",
    "for train_index, val_index in sss.split(tensor_feat[prov_mask], tensor_nom[prov_mask]):\n",
    "    train_mask = train_index\n",
    "    val_mask = val_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplacianPolynomial(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats: int,\n",
    "                 out_feats: int,\n",
    "                 k: int,\n",
    "                 dropout_prob: float,\n",
    "                 norm=True):\n",
    "        super().__init__()\n",
    "        self._in_feats = in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self._k = k\n",
    "        self._norm = norm\n",
    "        # Contains the weights learned by the Laplacian polynomial\n",
    "        self.pol_weights = nn.Parameter(torch.Tensor(self._k + 1))\n",
    "        # Contains the weights learned by the logistic regression (without bias)\n",
    "        self.logr_weights = nn.Parameter(torch.Tensor(in_feats, out_feats))\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        torch.manual_seed(0)\n",
    "        torch.nn.init.xavier_uniform_(self.logr_weights, gain=0.01)\n",
    "        torch.nn.init.normal_(self.pol_weights, mean=0.0, std=1e-3)\n",
    "\n",
    "    def forward(self, graph, feat):\n",
    "        r\"\"\"Compute graph convolution.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        * Input shape: :math:`(N, *, \\text{in_feats})` where * means any number of additional\n",
    "          dimensions, :math:`N` is the number of nodes.\n",
    "        * Output shape: :math:`(N, *, \\text{out_feats})` where all but the last dimension are\n",
    "          the same shape as the input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph (DGLGraph) : The graph.\n",
    "        feat (torch.Tensor): The input feature\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (torch.Tensor) The output feature\n",
    "        \"\"\"\n",
    "        feat = self.dropout(feat)\n",
    "        graph = graph.local_var()\n",
    "        \n",
    "        # D^(-1/2)\n",
    "        norm = torch.pow(graph.in_degrees().float().clamp(min=1), -0.5)\n",
    "        shp = norm.shape + (1,) * (feat.dim() - 1)\n",
    "        norm = torch.reshape(norm, shp)\n",
    "        \n",
    "        # mult W first to reduce the feature size for aggregation.\n",
    "        feat = torch.matmul(feat, self.logr_weights) # X*Teta\n",
    "\n",
    "        result = self.pol_weights[0] * feat.clone() # a0*L^0*X*Teta <-- fisrt polynomial weight a0 * L^0 * x\n",
    "\n",
    "        for i in range(1, self._k + 1): # get the next polynomial coefficient (a1*L^1, a2*L^2, ..... ak*L^k) \n",
    "            old_feat = feat.clone()\n",
    "            if self._norm:\n",
    "                feat = feat * norm\n",
    "            graph.ndata['h'] = feat\n",
    "            # Feat is not modified in place\n",
    "            graph.update_all(fn.copy_src(src='h', out='m'),\n",
    "                             fn.sum(msg='m', out='h')) # update all nodes with msg function copy_src (get data from source node) and reduce function sum\n",
    "            if self._norm:\n",
    "                graph.ndata['h'] = graph.ndata['h'] * norm\n",
    "\n",
    "            feat = old_feat - graph.ndata['h']\n",
    "            result += self.pol_weights[i] * feat\n",
    "\n",
    "        return result\n",
    "\n",
    "    def extra_repr(self):\n",
    "        \"\"\"Set the extra representation of the module,\n",
    "        which will come into effect when printing the model.\n",
    "        \"\"\"\n",
    "        summary = 'in={_in_feats}, out={_out_feats}'\n",
    "        summary += ', normalization={_norm}'\n",
    "        return summary.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have are model ready we just need to create a function that performs one step of our training loop, and another one that evaluates our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, g, features, labels, loss_fcn, train_mask, optimizer):\n",
    "    model.train()  # Activate dropout\n",
    "    \n",
    "    logits = model(g, features) # prediction\n",
    "    loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()  # Deactivate dropout\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)[mask]  # only compute the evaluation set\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        acc = correct.item() * 1.0 / len(labels)\n",
    "        f1 = f1_score(labels, indices)\n",
    "        #acc = torch.sum((logits.round() == labels).diagonal()).item() * 1.0 / len(labels)\n",
    "        return f1, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_feats = len(features)\n",
    "output = 2\n",
    "pol_order = 3\n",
    "lr = 0.02\n",
    "weight_decay = 5e-6\n",
    "n_epochs = 1000\n",
    "p_dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train the classifier end to end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ntds_2019/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/anaconda3/envs/ntds_2019/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00001 | Time(s) nan | Train Loss 395.2351 | Val f1 35.0993% | Val Accuracy 49.0247%\n",
      "Epoch 00051 | Time(s) 0.0065 | Train Loss 240.5186 | Val f1 31.4530% | Val Accuracy 47.8544%\n",
      "Epoch 00101 | Time(s) 0.0070 | Train Loss 3.4966 | Val f1 25.6983% | Val Accuracy 65.4096%\n",
      "Epoch 00151 | Time(s) 0.0067 | Train Loss 0.7440 | Val f1 25.5319% | Val Accuracy 59.0377%\n",
      "Epoch 00201 | Time(s) 0.0065 | Train Loss 0.6160 | Val f1 9.5238% | Val Accuracy 77.7633%\n",
      "Epoch 00251 | Time(s) 0.0065 | Train Loss 0.6144 | Val f1 15.8416% | Val Accuracy 77.8934%\n",
      "Epoch 00301 | Time(s) 0.0064 | Train Loss 0.6328 | Val f1 1.2048% | Val Accuracy 78.6736%\n",
      "Epoch 00351 | Time(s) 0.0063 | Train Loss 0.6147 | Val f1 10.2273% | Val Accuracy 79.4538%\n",
      "Epoch 00401 | Time(s) 0.0062 | Train Loss 0.6182 | Val f1 2.3529% | Val Accuracy 78.4135%\n",
      "Epoch 00451 | Time(s) 0.0061 | Train Loss 0.6281 | Val f1 20.8333% | Val Accuracy 75.2926%\n",
      "Epoch 00501 | Time(s) 0.0061 | Train Loss 0.6161 | Val f1 20.1754% | Val Accuracy 76.3329%\n",
      "Epoch 00551 | Time(s) 0.0061 | Train Loss 0.6163 | Val f1 21.9512% | Val Accuracy 75.0325%\n",
      "Epoch 00601 | Time(s) 0.0061 | Train Loss 0.6161 | Val f1 20.4255% | Val Accuracy 75.6827%\n",
      "Epoch 00651 | Time(s) 0.0060 | Train Loss 0.6159 | Val f1 24.3902% | Val Accuracy 75.8127%\n",
      "Epoch 00701 | Time(s) 0.0061 | Train Loss 0.6169 | Val f1 21.7054% | Val Accuracy 73.7321%\n",
      "Epoch 00751 | Time(s) 0.0061 | Train Loss 0.6196 | Val f1 25.4980% | Val Accuracy 75.6827%\n",
      "Epoch 00801 | Time(s) 0.0061 | Train Loss 0.6191 | Val f1 20.6349% | Val Accuracy 73.9922%\n",
      "Epoch 00851 | Time(s) 0.0060 | Train Loss 0.6119 | Val f1 24.9027% | Val Accuracy 74.9025%\n",
      "Epoch 00901 | Time(s) 0.0060 | Train Loss 0.6166 | Val f1 21.3439% | Val Accuracy 74.1222%\n",
      "Epoch 00951 | Time(s) 0.0061 | Train Loss 0.6096 | Val f1 19.9203% | Val Accuracy 73.8622%\n",
      "\n",
      "Test Accuracy 73.2570%\n"
     ]
    }
   ],
   "source": [
    "model = LaplacianPolynomial(in_feats, output, pol_order, p_dropout)\n",
    "\n",
    "loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=lr,\n",
    "                             weight_decay=weight_decay)\n",
    "\n",
    "dur = []\n",
    "for epoch in range(n_epochs):\n",
    "    if epoch >= 3:\n",
    "        t0 = time.time()\n",
    "    loss = train(model, G, tensor_feat, tensor_nom, loss_fcn, train_mask, optimizer)\n",
    "    if epoch >= 3:\n",
    "        dur.append(time.time() - t0)\n",
    "        \n",
    "    f1, acc = evaluate(model, G, tensor_feat, tensor_nom, val_mask)\n",
    "    \n",
    "    if epoch%50 == 0:\n",
    "        print(\"Epoch {:05d} | Time(s) {:.4f} | Train Loss {:.4f} | Val f1 {:.4%} | Val Accuracy {:.4%}\". format(\n",
    "                epoch+1, np.mean(dur), loss.item(), f1, acc))\n",
    "\n",
    "print()\n",
    "f1, acc = evaluate(model, G, tensor_feat, tensor_nom, test_mask)\n",
    "print(\"Test Accuracy {:.4%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(G, tensor_feat)[val_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
